[{"content":"前言 瓦片缓存组件是绝大部分瓦片服务所应该有的模块之一，而Geo Atlas同样实现了其瓦片缓存模块。本文用于描述Geo Atlas中的Cache模块的设计与实现过程。\n什么是GAC？ GAC，全称是Geo Atlas Cache，是Geo Atlas类库中的Cache模块，用于提供矢量瓦片的缓存功能。GAC源自GWC（GeoWebCache），是在GWC的基础上进行了适应性的调整而来。目前支持基于内存、文件系统两种缓存方式，且此两种缓存方式可任意组合。对于瓦片缓存处理策略，目前支持Seed、Reseed、Truncate三种，与GWC保持一致。\nGAC的基本理念 GWC中声明并强调TileLayer的概念，并基于此抽象，用以适配数据来源与瓦片存储（缓存）。也就如同GeoServer中，一个图层如果需要拥有缓存能力，那么还需要创建一个TileLayer；也就是说，一个拥有缓存的图层，将会同时持有两个Layer，一个是 Map Layer（FeatureType），另一个是TileLayer。 TileLayer中进行瓦片存储相关内容的配置，可以随意配置存储容器对象（文件系统、对象存储、数据库等）。\n而目前Geo Atlas Cache的实现则将大大简化这一操作，没有TileLayer对象，全局共用同一个存储容器对象，无需繁琐的存储配置，通过自动装配快速启用缓存，我认为这是中小项目中所需要的。\n目前对于瓦片缓存（Tile Cache）的清理，也就是同GWC中提供的Seed、Reseed、Truncate一般。Truncate只需持有BlobStore的句柄即可完成，但Seed和Reseed则需前往数据的源端获取瓦片，进而才可完成操作，也就是需要持有获取源端瓦片Generate的句柄才可。或许这就是为什么GWC中提出TileLayer的原因之一也未可知 😮。 不过，我却由此认为GWC的边界不清晰，我认为缓存就做缓存的事情就可以了，应当把Layer、TileMatrixSet（GridSet）和Cache分开。但如此，若想要支持Seed和Reseed这两种给策略的话，至少需要提供一个拓展点才可。我在此将其命名为TileSource，是为Cache与Source（源端瓦片）之间的适配组件。其实，这不也是一种等同TileLayer的存在，但我并不通过Layer来进行关联控制，也没有TileLayer的概念，缓存就是缓存。\nGAC的设计与实现 因为GeoServer沉重的历史包袱以及大而全的臃肿，所以有了Geo Atlas项目。GAC也将延续此理念，去除Cache特定于Layer的概念，无需为图层单独配置缓存，因为绝大部分情况下，都是使用相同的缓存配置。同时，他应该是可以被快速集成，且易于配置的。为了提升缓存组件的易用性、兼容性及稳定性，至少应该提供两种不同的缓存存储对象，且其中一种应该是基于内存的，另外一种是支持持久化的。当数据量很少时，可以关闭缓存或仅开启内存缓存；当数据量较大时，可以仅开启持久化缓存或同时开启内存缓存。内存缓存与可持久化缓存可自由搭配，任意组合。当两者全开启时，可形成两级缓存，此时需注意两级缓存间数据的同步。\nGAC的需求与设计 接下来，再次确定一下GAC的需求：\n支持矢量瓦片缓存 提供两类缓存存储对象，其中一种应该是基于内存的，另外一种是支持持久化的，且两者可任意组合，同时开启可形成二级缓存结构 提供Seed、Reseed、Truncate此三种瓦片缓存清理策略 全局共用同一个缓存存储对象，提供快速集成能力 其中，二级缓存是此前没有接触过的内容。结合自我臆想，给出了如下设想 🫣：\n此二级缓存，可自行确定组合方式，并非需要两者同时开启。难点：状态同步（数据一致性）\n一级缓存（基于内存）： Local Mem Cache + LRU Guava Caffeine 二级缓存（可持久化）：Outer Cache Redis File System Database GeoPackage 每一个Level一个gpkg文件，提升并发读写能力（支持配置为所有Level用一个gpkg，但是不推荐） PostGIS 不过，在经过一番调研之后，还是决定抄GeoWebCache的作业 😧。一是确实有一定的难度，二是目前时间有限，GAP中的我早已瑟瑟发抖 🙄。最重要的是，GeoWebCache中的MemoryBlobStore已经实现了上述二级缓存的需求呀 🫡，如此操作可直接覆盖掉前三个需求。而此时二级缓存的实现确定为：\n基于内存的缓存（Guava） 基于文件系统的缓存 需要特别注意的是，此MemoryBlobStore二级缓存是可拓展架构，后续可自行拓展不同的Provider。\n💡 对于Seed与Reseed的处理，则与GAC的理念中所述一致，通过TileSource对外提供拓展。也就是默认情况下，Cache模块只提供Seed与Reseed的声明，无法提供具体实现（无法直接与Source进行链接）。\n那么此刻只剩下最后一个需求了，其主旨围绕快速配置、易用。在此基于GeoWebCache中的DefaultStorageBroker 类进行缓存存储对象的代理，其符合GWC中缓存存储对象设计架构，也为后续提供了更多的拓展点，同时将其暴露给全局，即全局共用的缓存存储对象。对于快速集成能力，此处将结合Spring Boot的AutoConfiguration特性，为GAC提供自动装配能力。与此同时，将缓存存储对象的可设置属性通过配置的方式暴露出来，可直接在application.yml或application.properties中进行配置。具体可配置内容如下所示：\ngeo-atlas.cache.enabled=false（是否启用缓存，默认为false） geo-atlas.cache.inner-caching-enabled=false（是否启用内存缓存，默认为false）\ngeo-atlas.cache.inner.storage.provider=guava（可选值：guava，暂不支持，保留）\ngeo-atlas.cache.inner.storage.memory-limit=16（内存大小限制，单位MB，默认16）\ngeo-atlas.cache.inner.storage.concurrency-level=4（缓存并发级别的默认值，默认为4）\ngeo-atlas.cache.inner.storage.eviction-policy=null（缓存驱逐政策，即缓存淘汰算法，可选值：NULL、LRU、LFU、EXPIRE_AFTER_WRITE、EXPIRE_AFTER_ACCESS，默认值为NULL）\n💡 LRU、LFU暂时不支持\ngeo-atlas.cache.inner.storage.eviction-time=2*60（缓存驱逐时间的默认值，单位：秒，默认：2 minutes）\ngeo-atlas.cache.persistence-enabled=false（是否启用持久化缓存，默认为false） geo-atlas.cache.persistence.storage.provider=file-system（持久化缓存存储策略对象，可选值：file-system、geopackage，现只支持file-system） geo-atlas.cache.persistence.storage.base-directory（即最终瓦片持久化的目录基础路径，默认读取：java.io.tmpdir，可拓展） geo-atlas.cache.persistence.storage.file-system.path-generator-type=default（即瓦片存储路径算法，可选值：default、tms、xyz，默认为default）\ngeo-atlas.cache.persistence.storage.fs.block-size=4096（The default block size is 4096 bytes.）\nThis setting determines how the tile cache calculates disk usage. The value for this setting should be equivalent to the disk block size of the storage medium where the cache is located. The default block size is 4096 bytes.（此设置确定切片缓存如何计算磁盘使用情况。 该设置的值应等于缓存所在存储介质的磁盘块大小。 默认块大小为 4096 字节。）\n💡 目前该设置应没有什么用，毕竟又没做磁盘使用情况的统计。 🤨\nGAC的实现 既然决定抄GeoWebCache的作业，那主要内容就是对其Cache部分内容进行移植，同时将Geo Atlas的开源协议变更为LGPL（MIT → LGPL）。当所有内容都被自我吸收转换后，就可以自由控制协议了，比如再转为MIT或Apache。\nBlobStore 这里可以先看一下BlobStore的现有结构设计，BlobStore是缓存存储对象的顶层接口，MemoryBlobStore、FileBlobStore、NullBlobStore等均是BlobStore的具体实现。这里可以注意到MemoryBlobStore中有一个store属性（类型为BlobStore）和一个cacheProvider属性（类型为CacheProvider），这也就是前面提到的二级缓存实现。\n展开MemoryBlobStore，可以从其构造函数发现，默认情况下只提供了基于Guava的内存缓存，包装的BlobStore是一个空实现，且在此处注释，告知其默认行为和提示的操作。也就是说，包装的BlobStore实例对象可以是BlobStore接口的任意实现，我在此将使用基于文件系统的实现（FileBlobStore）进行填充。\n最终，将二级缓存构建的整体逻辑结合可配置性封装于一个注册类中，在此将其命名为：StorageBrokerRegister，此即为全局共用的缓存存储对象，主要代码如下所示：\npublic class StorageBrokerRegister implements EnvironmentAware, ImportBeanDefinitionRegistrar { private static final Logger log = LoggerFactory.getLogger(StorageBrokerRegister.class); private static final String FILE_SYSTEM_PROVIDER = \u0026#34;file-system\u0026#34;; private static final String GEO_PACKAGE = \u0026#34;geo-package\u0026#34;; private Environment environment; private DefaultStorageFinder storageFinder = new DefaultStorageFinder(null); @Override public void setEnvironment(Environment environment) { this.environment = environment; } @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { // FIXME: 2024/5/10 其实目前TransientCache并没有什么使用场景，不过暂时先留下 // TransientCache 早于MemoryStore出现, 前者应该是2012年, 后者应该是2014年 TransientCache transientCache = new TransientCache(100, 1024, 2000); BlobStore blobStore = new NullBlobStore(); Boolean cacheEnabled = getApplicationValue(GeoAtlasCacheEnvKeys.getCacheEnabled(), Boolean.class, Boolean.FALSE); if (cacheEnabled) { Boolean innerCachingEnabled = getApplicationValue(GeoAtlasCacheEnvKeys.getInnerCachingEnabled(), Boolean.class, Boolean.FALSE); Boolean persistenceEnabled = getApplicationValue(GeoAtlasCacheEnvKeys.getPersistenceEnabled(), Boolean.class, Boolean.FALSE); if (innerCachingEnabled) { MemoryBlobStore memoryBlobStore = buildInnerCache(); if (persistenceEnabled) { memoryBlobStore.setStore(buildPersistenceCache()); } blobStore = memoryBlobStore; }else if (persistenceEnabled) { blobStore = buildPersistenceCache(); }else { log.warn(\u0026#34;The cache configuration can be turned on, but the memory cache and persistent cache are turned off and cannot be further configured.\u0026#34; + \u0026#34; Please check your configuration items.\u0026#34;); log.warn(\u0026#34;This blobStore configuration fails and will enter a no-cache state.\u0026#34;); } } BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(DefaultStorageBroker.class); builder.addConstructorArgValue(blobStore).addConstructorArgValue(transientCache); AbstractBeanDefinition beanDefinition = builder.getBeanDefinition(); beanDefinition.setSynthetic(true); registry.registerBeanDefinition(\u0026#34;storageBroker\u0026#34;, beanDefinition); log.info(\u0026#34;The BlobStore registration is complete\u0026#34;); } ... } Seed、Reseed\u0026amp;Truncate 对于Truncate来说，直接进行全面移植即可。由于此处移除了TileLayer概念，所以Seed与Reseed无法全面移植处理，而是仅移植其最终实现部分，对于这两者与源端瓦片的链接则通过TileSource接口来实现。即默认情况下，GAC无法提供完整的Seed与Reseed功能，需要接入端实现TileSource中的seedTile方法，GAC中仅提供一个TileSource的空实现。\npublic interface TileSource { /** * 即默认不使用MetaTiles */ int[] META_TILING_FACTORS = {1,1}; /** * The size of a metatile in tiles. * * @return the {x,y} metatiling factors */ default int[] getMetaTilingFactors() { return META_TILING_FACTORS; } void seedTile(ConveyorTile tile, boolean tryCache) throws GeoAtlasCacheException, IOException; } public abstract class AbstractTileSource implements TileSource{ private final Logger log = LoggerFactory.getLogger(getClass()); protected static final ThreadLocal\u0026lt;ByteArrayResource\u0026gt; TILE_BUFFER = new ThreadLocal\u0026lt;\u0026gt;(); public void seedTile(ConveyorTile tile, boolean tryCache) throws GeoAtlasCacheException, IOException { // Cache模块无法自主执行seed\u0026amp;reseed操作，需要集成方自行实现 if (log.isWarnEnabled()){ log.warn(\u0026#34;The Cache module cannot perform seed \u0026amp; reseed operations autonomously, and the integrator needs to implement it by itself.\u0026#34;); } } protected void transferTile(TileObject tile, ConveyorTile tileProto, long requestTime, boolean persistent) throws GeoAtlasCacheException { ByteArrayResource resource = this.getTileBuffer(TILE_BUFFER); // copy resource tileProto.setBlob(resource); try { writeTileToStream(tile, resource); tile.setCreated(requestTime); if (persistent){ tileProto.getStorageBroker().put(tile); } tileProto.getStorageObject().setCreated(tile.getCreated()); } catch (StorageException var18) { throw new GeoAtlasCacheException(var18); } catch (IOException e) { log.error(\u0026#34;Unable to write image tile to ByteArrayOutputStream\u0026#34;, e); } } protected ByteArrayResource getTileBuffer(ThreadLocal\u0026lt;ByteArrayResource\u0026gt; tl) { ByteArrayResource buffer = (ByteArrayResource) tl.get(); if (buffer == null) { buffer = new ByteArrayResource(16 * 1024); tl.set(buffer); } buffer.truncate(); return buffer; } public boolean writeTileToStream(final TileObject raw, Resource target) throws IOException { try (OutputStream outStream = target.getOutputStream()) { IOUtils.copy(raw.getBlob().getInputStream(), outStream); } return true; } } // GAC中TileSource的默认实现 public class DefaultTileSource extends AbstractTileSource { } 💡 这里需要注意的是，GWC中有一个Metatiles技术，是一个针对地图瓦片（PNG|JPEG）的优化技术。但是在GWC现有的架构组织上，Metatiles是无法支持矢量瓦片技术的，也就是会还原为1x1的组合。所以我在此处直接将Metatiles默认设置为1x1的组合，同时暂时去除了瓦片转存部分中Metatiles拆分逻辑。\nTileMatrixSubset拓展 先讲一下当指定BBox后，Truncate、Seed、Reseed的大体执行逻辑。GAC对于瓦片清理策略的执行与GWC保持高度一致，同样支持指定BBox进行瓦片清理，BBox用于限定可清理范围。在前文中对于TileMatrixSet和TileMatrixSubset做过解释，前者用于定义瓦片矩阵集，后者用于在瓦片矩阵集中描述数据的实际范围，同时限定瓦片可请求的范围，此两者分别对应GWC中的GridSet及GridSubset。在GWC携带BBox的瓦片清理策略执行过程中，会获取对应TileLayer的GridSubset，并与给定的BBox计算相交范围，而后就可以精确的确定哪些瓦片是需要被清理的了。对于GAC来说，并没有如TileLayer的概念，也无法与源端或图层（FeatureLayer）进行直接链接，所以是无法获取到图层所持有的TileMatrixSubset对象（如果有的话）。\n当然，并不是说没有TileMatrixSubset对象就无法进行瓦片缓存的清理了。就拿我们实际应用中大部分场景来说，一般都会使用基于Web墨卡托或CGCS2000经纬度等间隔直投的瓦片矩阵集作为TileMatrixSet，用数据的实际范围构建TileMatrixSubset。此时可认为前者为全球范围，后者为局部范围。但无论与全球范围还是局部范围进行相交计算，其最终计算结果是一致的，都是可以正常工作的。只不过直觉告诉我，使用局部范围计算应该会更快一些 也未可知啊 🤔（我没有数据支持，纯属瞎咧咧 😛），所以继续留下TileMatrixSubset的拓展点。\n也就是说，默认情况下，GAC是无法获取到图层的TileMatrixSubset对象的，那么会使用TileMatrixSet对应的范围进行TileMatrixSubset的构建。可能你会问，既然可以在全局获取到TileMatrixSet对象，那么为什么就不能如此处理TileMatrixSubset对象呢？这是因为TileMatrixSet是强制需要有的，而且Geo Atlas中的TileMatrixSet并不与图层绑定，声明即支持；而TileMatrixSubset是可选的，且是特定于图层的，所以无法如此设置。\npublic abstract class AbstractTileSeedService implements TileSeedService{ private TileBreeder breeder; public AbstractTileSeedService(TileBreeder breeder) { this.breeder = breeder; } public void doSeed(SeedRequest request) throws GeoAtlasCacheException { TileMatrixSet matrixSet = TileMatrixSetContext.getTileMatrixSet(request.getMatrixSetId()); if (matrixSet == null) { throw new GeoAtlasCacheException(\u0026#34;TileMatrixSet not found\u0026#34;); } breeder.seed(request, getSubset(request, matrixSet)); } /** * 必须实现获取subset的方法 * @param request * @param matrixSet * @return */ protected abstract TileMatrixSubset getSubset(SeedRequest request, TileMatrixSet matrixSet) throws GeoAtlasCacheException; } public class DefaultTileSeedService extends AbstractTileSeedService{ public DefaultTileSeedService(TileBreeder breeder) { super(breeder); } @Override protected TileMatrixSubset getSubset(SeedRequest request, TileMatrixSet matrixSet) throws GeoAtlasCacheException { // 由于目前并不存在如Tile Layer的概念, 所以在Cache模块里面无法获取到FeatureLayer的TileMatrixSubset, 所以这里直接返回null, // 那么将会使用 request中给定的matrixSetId 来创建 TileMatrixSubset(也就是是一个范围与TileMatrixSet相同的TileMatrixSubset) // 继承方可以自行实现该接口, 从而注入FeatureLayer的TileMatrixSubset return null; } } 快速集成 此处基于Spring Boot的AutoConfiguration特性对外提供快速集成能力，也就是说GAC是具备自动装配功能的。这里将需要被自动装配的内容封装到一块，命名为GeoAtlasCacheAutoConfiguration，并将其添加到spring.factories配置文件中。\n@ConditionalOnProperty(value = \u0026#34;geo-atlas.cache.enabled\u0026#34;, havingValue = \u0026#34;true\u0026#34;, matchIfMissing = true) @Import(StorageBrokerRegister.class) public class GeoAtlasCacheAutoConfiguration { @Bean @ConditionalOnMissingBean(value = SeederThreadPoolExecutor.class) public SeederThreadPoolExecutor seederThreadPoolExecutor() { return new SeederThreadPoolExecutor(16, 32); } @Bean @ConditionalOnMissingBean(value = TileSource.class) public TileSource tileSource() { return new DefaultTileSource(); } @Bean @ConditionalOnMissingBean(value = TileBreeder.class) public TileBreeder tileBreeder(StorageBroker storageBroker, TileSource tileSource, SeederThreadPoolExecutor stpe) { return new TileBreeder(tileSource, storageBroker, stpe); } @Bean @ConditionalOnMissingBean(value = TileSeedService.class) public TileSeedService tileSeedService(TileBreeder tileBreeder){ return new DefaultTileSeedService(tileBreeder); } @Bean @ConditionalOnMissingBean(value = TileSeedEndpoint.class) public TileSeedEndpoint tileSeedEndpoint(TileSeedService tileSeedService) { return new TileSeedEndpoint(tileSeedService); } } 💡 需要特别注意的是，此处所述的自动装配，仅仅只能完成缓存模块的装配，并暴露出全局唯一的BlobStore代理对象：StorageBroker，但是并没有完成与图层或源端的链接，需要接入方自行处理。对于GAC的接入示例，可参考附录部分。\n小结 GAC，即Geo Atlas Cache，自此已全部完成。通过GAC开发，对于GWC（GeoWebCache）的理解又多了一些🫣。\n事务的产生必有其原因与其使命，即要解决的问题，同时还会受产生阶段的环境影响。GAC之所以出现是因为需要构建Geo Atlas的生态，同时希望可以保持缓存模块的纯粹性和易用性，且可以快速进行集成，以便投入项目开发中。受制于个人经历与眼界的影响，GAC、Geo Atlas并没有达成其理想的状态，虽不影响正常的使用。（这似乎是一个悖论，人是复杂的，或许是冥冥之中有人告诉我，让我有了此段描述也未可知啊 😑）\n我愿集众家之所长，塑于Geo Atlas。期待你的进步，加油！\n💡 ps：更多实现情况，请参见GAC源码：Geo Atlas Cache@Github\n参考 Java二级高速缓存架构设计_缓存_元年技术洞察_InfoQ写作社区\n多级缓存 架构设计 - 疯狂创客圈 - 博客园\n微服务架构中的多级缓存设计还有人不懂？\n彻底弄懂浏览器缓存策略\nMBTiles 与 SMTiles 格式的地图瓦片\nBlobStores — GeoServer 2.26.x User Manual\nGeoWebCache Metatiles\n附录 GAC的集成示例 我在ogc-api模块中集成了tile-cache（即GAC），下面对本次集成做简要描述：\n第一要素，依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.geo-atlas.component\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tile-cache\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${revision}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 实现源端（Data Source）于GAC的链接 \u0026amp; 实现TileSource拓展\n@Component public class DefaultTileGenerator extends AbstractTileSource implements GeoAtlasTileGenerator { private final FeatureSourceHelper featureSourceHelper; private String lockProvider; private transient LockProvider lockProviderInstance; private static final Logger log = LoggerFactory.getLogger(DefaultTileGenerator.class); public DefaultTileGenerator(FeatureSourceHelper featureSourceHelper) { this.featureSourceHelper = featureSourceHelper; } public ConveyorTile generator(ConveyorTile tile) throws IOException, GeoAtlasCacheException, OutsideCoverageException { // FIXME: 2024/5/10 可以再次检查 MimeType, 在应用层面控制MimeType的支持 // checkMimeType(tile); TileMatrixSet tileMatrixSet = TileMatrixSetContext.getTileMatrixSet(tile.getGridSetId()); if (tileMatrixSet == null) { throw new IllegalArgumentException(\u0026#34;TileMatrixSet not found by identifier: \u0026#34; + tile.getGridSetId()); } // FIXME: 2024/5/10 需要在metadata中给featureLayer指定coverage, 可以选择从数据源读取或者是自行设定(更推荐自行设定, 性能友好且更通用) final TileMatrixSubset gridSubset = tile.getGridSubset(); if (gridSubset == null) { throw new IllegalArgumentException(\u0026#34;Requested gridset not found: \u0026#34; + tile.getRequest().getSchema()); } final long[] gridLoc = tile.getTileIndex(); checkNotNull(gridLoc); // Final preflight check, throws OutsideCoverageException if necessary gridSubset.checkCoverage(gridLoc); // FIXME: 2024/5/10 暂时不对meta tiles做支持 int metaX; int metaY; if (tile.getMimeType().supportsTiling()) { metaX = getMetaTilingFactors()[0]; metaY = getMetaTilingFactors()[1]; } else { metaX = metaY = 1; } ConveyorTile tileResponse; if (tile.getStorageBroker() == null) { tileResponse = getNonCachedTile(tile); }else { tileResponse = getTileResponse(tile, true, true, metaX, metaY); } // sendTileRequestedEvent(returnTile); return tileResponse; } public ConveyorTile getNonCachedTile(ConveyorTile tile) throws GeoAtlasCacheException { try { return getTileResponse(tile, false, false, 1, 1); } catch (IOException e) { throw new GeoAtlasCacheException(e); } } @Override public void seedTile(ConveyorTile tile, boolean tryCache) throws GeoAtlasCacheException, IOException { // Ignore a seed call on a tile that\u0026#39;s outside the cached grid levels range // 忽略缓存网格级别范围之外的图块上的种子调用 final TileMatrixSubset subset = tile.getGridSubset(); final int zLevel = (int) tile.getTileIndex()[2]; if (!subset.shouldCacheAtZoom(zLevel)) { if (log.isDebugEnabled()) { log.debug( \u0026#34;Ignoring seed call on tile \u0026#34; + tile + \u0026#34; as it\u0026#39;s outside the cacheable zoom level range\u0026#34;); } return; } int metaX = getMetaTilingFactors()[0]; int metaY = getMetaTilingFactors()[1]; if (!tile.getMimeType().supportsTiling()) { metaX = metaY = 1; } getTileResponse(tile, tryCache, true, metaX, metaY); } protected ConveyorTile getTileResponse( ConveyorTile tile, final boolean tryCache, final boolean persistent, final int metaX, final int metaY) throws GeoAtlasCacheException, IOException { if (tryCache \u0026amp;\u0026amp; tryCacheFetch(tile)) { return finalizeTile(tile); } TileObject target = null; TileRequest request = tile.getRequest(); LockProvider.Lock lock = null; try { /* ****************** Acquire lock ******************* */ lock = getLockProvider().getLock(buildLockKey(tile)); // got the lock on the tile, try again if (tryCache \u0026amp;\u0026amp; tryCacheFetch(tile)) { log.debug(\u0026#34;--\u0026gt; {} returns cache hit for {}\u0026#34;, Thread.currentThread().getName(), Arrays.toString(tile.getTileIndex())); } else { log.debug(\u0026#34;--\u0026gt; {} submitting getTile request for tile matrix location on {}\u0026#34;, Thread.currentThread().getName(), Arrays.toString(tile.getTileIndex())); long requestTime = System.currentTimeMillis(); try { FeatureSourceConveyor wrapper = featureSourceHelper.getFeatureSource(request.getNamespace(), request.getLayer()); Pyramid pyramid = findPyramid(request, wrapper.getRules()); target = pyramid.getTile(request, wrapper.getFeatureSource(), wrapper.getCrs()); // 此处便完成了瓦片数据对象到瓦片缓存的转换 transferTile(target, tile, requestTime, persistent); }catch (Exception e) { Throwables.throwIfInstanceOf(e, GeoAtlasCacheException.class); throw new GeoAtlasCacheException(\u0026#34;Problem communicating with GeoAtlas TileAPI\u0026#34;, e); } } } finally { if (lock != null) { lock.release(); } } return finalizeTile(tile); } ... } 实现TileMatrixSubset拓展\n@Component @ConditionalOnBean(TileBreeder.class) public class SimpleTileSeedService extends AbstractTileSeedService { private final FeatureTileMatrixSubsetContext subsetContext; private final FeatureSourceHelper featureSourceHelper; public SimpleTileSeedService(TileBreeder breeder, FeatureTileMatrixSubsetContext subsetContext, FeatureSourceHelper featureSourceHelper) { super(breeder); this.subsetContext = subsetContext; this.featureSourceHelper = featureSourceHelper; } @Override protected TileMatrixSubset getSubset(SeedRequest request, TileMatrixSet matrixSet) throws GeoAtlasCacheException { NamespaceInfo namespaceInfo = featureSourceHelper.getNamespaceInfo(request.getNamespace()); if (namespaceInfo == null) { throw new GeoAtlasCacheException(\u0026#34;namespace not found\u0026#34;); } FeatureLayerInfo featureLayerInfo = featureSourceHelper.getFeatureLayerInfo(request.getNamespace(), request.getLayerName(), namespaceInfo); if (featureLayerInfo == null){ throw new GeoAtlasCacheException(\u0026#34;layer not found\u0026#34;); } return subsetContext.getTileMatrixSubset(featureLayerInfo, matrixSet); } } 缓存配置（此配置来自于：geoatlas-tile-instance），并未展示出全部配置内容\ngeo-atlas: cache: enabled: true inner-caching-enabled: true persistence-enabled: true # 默认是file-system inner: storage: eviction-policy: EXPIRE_AFTER_ACCESS memory-limit: 256 persistence: storage: base-directory: ${PERSISTENCE_CACHE_DIR:/tmp/gac} ","permalink":"https://fuyi-atlas.github.io/posts/gis/what-is-gac/","summary":"前言 瓦片缓存组件是绝大部分瓦片服务所应该有的模块之一，而Geo Atlas同样实现了其瓦片缓存模块。本文用于描述Geo Atlas中的Cache模块的设计与实现过程。\n什么是GAC？ GAC，全称是Geo Atlas Cache，是Geo Atlas类库中的Cache模块，用于提供矢量瓦片的缓存功能。GAC源自GWC（GeoWebCache），是在GWC的基础上进行了适应性的调整而来。目前支持基于内存、文件系统两种缓存方式，且此两种缓存方式可任意组合。对于瓦片缓存处理策略，目前支持Seed、Reseed、Truncate三种，与GWC保持一致。\nGAC的基本理念 GWC中声明并强调TileLayer的概念，并基于此抽象，用以适配数据来源与瓦片存储（缓存）。也就如同GeoServer中，一个图层如果需要拥有缓存能力，那么还需要创建一个TileLayer；也就是说，一个拥有缓存的图层，将会同时持有两个Layer，一个是 Map Layer（FeatureType），另一个是TileLayer。 TileLayer中进行瓦片存储相关内容的配置，可以随意配置存储容器对象（文件系统、对象存储、数据库等）。\n而目前Geo Atlas Cache的实现则将大大简化这一操作，没有TileLayer对象，全局共用同一个存储容器对象，无需繁琐的存储配置，通过自动装配快速启用缓存，我认为这是中小项目中所需要的。\n目前对于瓦片缓存（Tile Cache）的清理，也就是同GWC中提供的Seed、Reseed、Truncate一般。Truncate只需持有BlobStore的句柄即可完成，但Seed和Reseed则需前往数据的源端获取瓦片，进而才可完成操作，也就是需要持有获取源端瓦片Generate的句柄才可。或许这就是为什么GWC中提出TileLayer的原因之一也未可知 😮。 不过，我却由此认为GWC的边界不清晰，我认为缓存就做缓存的事情就可以了，应当把Layer、TileMatrixSet（GridSet）和Cache分开。但如此，若想要支持Seed和Reseed这两种给策略的话，至少需要提供一个拓展点才可。我在此将其命名为TileSource，是为Cache与Source（源端瓦片）之间的适配组件。其实，这不也是一种等同TileLayer的存在，但我并不通过Layer来进行关联控制，也没有TileLayer的概念，缓存就是缓存。\nGAC的设计与实现 因为GeoServer沉重的历史包袱以及大而全的臃肿，所以有了Geo Atlas项目。GAC也将延续此理念，去除Cache特定于Layer的概念，无需为图层单独配置缓存，因为绝大部分情况下，都是使用相同的缓存配置。同时，他应该是可以被快速集成，且易于配置的。为了提升缓存组件的易用性、兼容性及稳定性，至少应该提供两种不同的缓存存储对象，且其中一种应该是基于内存的，另外一种是支持持久化的。当数据量很少时，可以关闭缓存或仅开启内存缓存；当数据量较大时，可以仅开启持久化缓存或同时开启内存缓存。内存缓存与可持久化缓存可自由搭配，任意组合。当两者全开启时，可形成两级缓存，此时需注意两级缓存间数据的同步。\nGAC的需求与设计 接下来，再次确定一下GAC的需求：\n支持矢量瓦片缓存 提供两类缓存存储对象，其中一种应该是基于内存的，另外一种是支持持久化的，且两者可任意组合，同时开启可形成二级缓存结构 提供Seed、Reseed、Truncate此三种瓦片缓存清理策略 全局共用同一个缓存存储对象，提供快速集成能力 其中，二级缓存是此前没有接触过的内容。结合自我臆想，给出了如下设想 🫣：\n此二级缓存，可自行确定组合方式，并非需要两者同时开启。难点：状态同步（数据一致性）\n一级缓存（基于内存）： Local Mem Cache + LRU Guava Caffeine 二级缓存（可持久化）：Outer Cache Redis File System Database GeoPackage 每一个Level一个gpkg文件，提升并发读写能力（支持配置为所有Level用一个gpkg，但是不推荐） PostGIS 不过，在经过一番调研之后，还是决定抄GeoWebCache的作业 😧。一是确实有一定的难度，二是目前时间有限，GAP中的我早已瑟瑟发抖 🙄。最重要的是，GeoWebCache中的MemoryBlobStore已经实现了上述二级缓存的需求呀 🫡，如此操作可直接覆盖掉前三个需求。而此时二级缓存的实现确定为：\n基于内存的缓存（Guava） 基于文件系统的缓存 需要特别注意的是，此MemoryBlobStore二级缓存是可拓展架构，后续可自行拓展不同的Provider。\n💡 对于Seed与Reseed的处理，则与GAC的理念中所述一致，通过TileSource对外提供拓展。也就是默认情况下，Cache模块只提供Seed与Reseed的声明，无法提供具体实现（无法直接与Source进行链接）。\n那么此刻只剩下最后一个需求了，其主旨围绕快速配置、易用。在此基于GeoWebCache中的DefaultStorageBroker 类进行缓存存储对象的代理，其符合GWC中缓存存储对象设计架构，也为后续提供了更多的拓展点，同时将其暴露给全局，即全局共用的缓存存储对象。对于快速集成能力，此处将结合Spring Boot的AutoConfiguration特性，为GAC提供自动装配能力。与此同时，将缓存存储对象的可设置属性通过配置的方式暴露出来，可直接在application.yml或application.properties中进行配置。具体可配置内容如下所示：\ngeo-atlas.cache.enabled=false（是否启用缓存，默认为false） geo-atlas.cache.inner-caching-enabled=false（是否启用内存缓存，默认为false）\ngeo-atlas.cache.inner.storage.provider=guava（可选值：guava，暂不支持，保留）\ngeo-atlas.cache.inner.storage.memory-limit=16（内存大小限制，单位MB，默认16）\ngeo-atlas.cache.inner.storage.concurrency-level=4（缓存并发级别的默认值，默认为4）\ngeo-atlas.cache.inner.storage.eviction-policy=null（缓存驱逐政策，即缓存淘汰算法，可选值：NULL、LRU、LFU、EXPIRE_AFTER_WRITE、EXPIRE_AFTER_ACCESS，默认值为NULL）\n💡 LRU、LFU暂时不支持","title":"Geo Atlas Cache，一个精简的GWC组件"},{"content":"前言 书接上回，本章节为下篇：TileMatrixSet实现及相关计算原理探讨。\n本章节将以TileMatrixSet模型的典型实现，即GeoWebCache中的Gridset作为开端，对OGC中TileMatrxSet模型进行印证。而后对TileMatrixSet相关计算原理进行探讨，并以CGCS2000切片方案为例进行验证。另附带说明TileMatrixSet在Geo Atlas中的实现及应用 🤨。\nGridset \u0026amp; TileMatrixSet 这里就不再对GeoWebCache做介绍了，直接切入主题。GeoWebCache中的Gridset正是对应着TileMatrixSet模型，我们先来看一下GeoWebCache对于Gridset的相关介绍：\nGridsets and Gridsubsets Gridsets 和 Gridsubsets 是指 GeoWebCache 所服务的图层的空间参考系统（the spatial reference system）。从本质上来说，正如 Tiles 中所介绍的，GeoWebCache 与参考系统无关。当 GeoWebCache 向 WMS 发出请求时，它使用 Gridsets 和 Gridsubsets 信息将其内部切片索引转换为 WMS 可以理解的空间请求。\n💡 说实话，有时候我觉得老外的啰嗦话挺多的，其实这里就是表达GeoWebCache就是使用 Gridsets 和 Gridsubsets 将瓦片坐标转换为瓦片对应的空间范围的。从其实现来看，此处所述的内部切片索引正是对应着瓦片坐标系。\n下面分别对 Gridset 和 Gridsubset 的构成进行描述，此处引用原文：\nA gridset is a global definition (i.e. not layer-specific) specifying:\n全局定义即对应着通用的切片方案，所以不是特定于层（图层）的。\nA spatial reference system (EPSG code)\n对应着投影坐标系。\nA bounding box describing the extent, typically the maximum extent for the above reference system\n描述范围的边界框，通常是上述参考系统的最大范围（也就是说，也可以是你自行定义）\nOne of either a list of scale denominators, resolutions, or zoom levels\n比例分母、分辨率或缩放级别列表之一。（对应着尺度分级）\nThe tile dimensions in pixels (constant for all zoom levels)\n以像素为单位的瓦片尺寸（对于所有缩放级别均保持不变）。\n💡 这也是我在前文说的，目前大部分情况下，瓦片的尺寸基本都使用像素作为单位进行换算。同时，OGC TileMatrixSet中提出的是每个维度（尺度层级）下瓦片大小要一致，但是不同维度的瓦片大小可以不一致。不过在GeoWebCache实现中，要求全维度都使用相同的瓦片大小。当然，这也符合实际中绝大部分的使用情况。\n(Optional) Pixel size (to calculate scales). The default is 0.28mm/pixel, corresponding to 90.71428571428572 DPI.\n（可选）像素大小（用于计算比例）。默认为0.28mm/像素，对应90.71428571428572 DPI。\n💡 这里的Pixel Size对应着Cell Size，其以米为单位。 而0.28mm的来源，前文也曾提到：通常，设备的真实像素尺寸是未知的，0.28 毫米是 2005 年普通显示器的实际像素尺寸。即使当前的显示设备采用小得多的像素尺寸，该值仍被用作参考。 注：自 20 世纪 80 年代以来，Microsoft Windows 操作系统已将其默认的标准显示每英寸像素 (PPI) 设置为 96。该值的结果约为每像素 0.264 mm。该值与本标准中采用的实际 0.28mm 的相似性可能会造成一些混乱。\n对于90.71428571428572数值来源，可继续往下看，切片原理部分会做解释。\nA gridsubset is a layer-specific definition specifying:\ngridsubset 是特定于图层的，这是因为其代表着数据的真实范围。（对应着TileMatrixSet Limits）\n💡 从GeoServer的实现中，图层的BBox是在发布时从数据源处获取的，该图层数据的BBox即为gridsubset的边界范围\nThe gridset for the layer\n其实，Gridset 和 Gridsubset 通常是成对出现的，若有子集则必有原集，但Gridset是可以单独存在的。这两是在同一投影坐标系下，相同尺度分级下、不同BBox（BBox的关系是包含与被包含的关系）的TileMatrixSet实现，只不过 Gridsubset 基本作为辅助元素。\n(Optional) The bounding box for that layer (which must be a subset of the extent of the gridSet)\n（可选）该层的边界框（必须是gridSet范围的子集）\n💡 可选就很灵性，也就是说也可以取默认，也就是取投影坐标系的范围。\n(Optional) A list of zoom levels (which must be a subset of what is defined in the gridSet)\n（可选）缩放级别列表（必须是 gridSet 中定义的子集）\n💡 此处描述的不是可以自定义尺度分级，而是在 GridSet定义的基础上的限定。比如GridSet中定义了0-23级，那么subset只能是0-23的子集，可以限定，但是不可以再行定义。\nBy default, this information is set in geowebcache.xml.\n💡 从上文的描述中，并没有提及切片原点相关的内容。从其实现可知，其默认使用左下角作为切片原点进行实现的，这也是OGC的提倡。那为什么不在此处做出明示呢？估计是遵从约定大于配置原则 也未可知啊！（裘千丈警告😂）\nFrom gridsets to tiles 以下是解释在特定缩放级别下的网格集和子网格集的构建过程，并确定正在请求（可以请求）哪些瓦片。（这句翻译烫嘴🙄）\n对于上述介绍来说，我们现阶段只需要留意以下几个地方就好，像比例和范围计算可暂不用完全理解。\n对于Gridset，给定的边界不一定是Gridset中各格网域的最终边界，应该还需要根据规则进行计算，给定的边界不一定可以正好适合，所以会存在拓展的情况。Gridsubset同理。这也与上文中关于TileMatrixSet域部分对应：每个瓦片矩阵集都有一个可选的近似边界框（即BBox，之所以近似应是因为一般取是最小外接矩形MBR →Minimum Bounding Rectangle，并非完全一致。Maybe🤔），但每个瓦片矩阵都有一个精确的边界框，该边界框是从其他参数间接推导出来的。由于单元格对齐方式不同，每个比例下的瓦片矩阵边界框通常会略有不同（比如4490经纬度投影的0级和1级的边界框就不同）。\n上述内容提到，默认是将边界框对齐到左下角。也就是说，默认情况下，是使用左下角作为切片原点，但是可以通过进行设置是否与右上角对其。\n注：此为GeoWebCache的默认配置（左下角）。 切片原点不同，那么瓦片坐标也不同，但是可以进行相互转换。\nGridsubset就是限制，只有处于Gridsubset范围内的切片，才允许被访问\n💡 对于上述内容中给出的Gridset的BBox与Gridsubset的BBox的数值可不用过多理会，只要明白图示表达的意思就行。\nGridset 实现 这里只贴GridSet和Grid的属性部分代码\nGridSet public class GridSet implements Info { private String name; private SRS srs; // 瓦片宽度，一般为256，像素单位 private int tileWidth; // 瓦片高度，一般为256，像素单位 private int tileHeight; /** * Whether the y-coordinate of {@link #tileOrigin()} is at the top (true) or at the bottom * (false) */ protected boolean yBaseToggle = false; /** * By default the coordinates are {x,y}, this flag reverses the output for WMTS getcapabilities */ private boolean yCoordinateFirst = false; private boolean scaleWarning = false; private double metersPerUnit; private double pixelSize; // 限定的格网边界 private BoundingBox originalExtent; private Grid[] gridLevels; private String description; /** * {@code true} if the resolutions are preserved and the scaleDenominators calculated, {@code * false} if the resolutions are calculated based on the sacale denominators. */ private boolean resolutionsPreserved; } Grid public class Grid implements Serializable, Cloneable { private static final long serialVersionUID = 1L; // 矩阵宽度，即横向的瓦片数量 private long numTilesWide; // 矩阵高度，即纵向的瓦片数量 private long numTilesHigh; // 分辨率 private double resolution; // 比例分母 private double scaleDenom; private String name; } 对比OGC TileMatrixSet UML Model可以发现，GridSet并非完全遵循OGC的标准实现，可以在其基础上进行了一定的修改。当然，这和我目前看的TileMatrixSet标准的版本也有关系（比如说：cornerOfOrigin）。\nGridSet中要求所有层级的瓦片尺寸大小不变，所以将Grid中的tileWidth、tileHeight、pixelSize提升到GridSet中定义。 命名基本不一致 部分简化，又做了一定拓展。比如CRS 变更为SRS，但是兼容别名访问（3857，900913）；比如同时支持比例分母和分辨率来进行尺度分级定义，两者可以相互转换。 💡 其实，我也觉得标准给得相对复杂了些。在各自的场景下可以进行简化，甚至部分内容固定化。\n切片原理 术语 DPI（英语：Dots Per Inch，每英寸点数）：是一个量度单位，用于点阵数字图像，意思是指每一英寸长度中，取样或可显示或输出点的数目。如：打印机输出可达300DPI的分辨率，表示打印机可以每一平方英寸的面积中可以输出300x300＝90000个输出点。\n打印机所设置之分辨率的DPI值越高，印出的图像会越精细。打印机通常可以调校分辨率。例如撞针打印机，分辨率通常是60至90 DPI。喷墨打印机则可达300~720 DPI。激光打印机则有600~1200 DPI。\n一般显示器为96 DPI，印刷所需位图的DPI数则视印刷网线数（lpi）而定。一般150线印刷质量需要350 DPI的位图。而这里的D（dot）就是像素（pixel）。\nPPI：每英寸像素（英语：Pixels Per Inch，缩写：PPI），又被称为像素密度，是一个表示打印图像或显示器单位长度上像素数量的指数 (并非单位面积的像素量)。一般用来计量电脑显示器，电视机和手持电子设备屏幕的精细程度。通常情况下，每英寸像素值越高，屏幕能显示的图像也越精细。5PPI表示每英寸有5个像素，500PPI表示每英寸有500个像素。\n屏幕分辨率：也叫做像素分辨率，常用屏幕上每英寸长度内包含的像素数量来表达，即，Pixel per Inch，因此简写为 PPI（或 DPI），一般地图的默认屏幕分辨率是96。有的地方也用像素大小（pixel size）来描述屏幕的可分辨率，如 WMTS 标准中的0.28mm（标准化渲染像素大小）。\n💡 前文中对于0.28mm的由来做了描述：0.28 毫米是 2005 年普通显示器的实际像素尺寸。其对应的DPI为：25.4/0.28=90.7142857142857\nps：自 20 世纪 80 年代以来，Microsoft Windows 操作系统已将其默认的标准显示每英寸像素 (PPI) 设置为 96。该值（即像素大小）的结果约为每像素 0.264 mm，该值的来源：25.4/96=0.26458333333333334毫米，即约为0.264 mm\n1 英寸 = 2.54 厘米（cm）= 25.4 毫米（mm）\n💡 PPI和DPI都是描述分辨率（屏幕分辨率、空间分辨率是其中的分类）的单位。不过PPI和DPI经常都会出现混用现象。但是他们所用的领域也存在区别。从技术角度说，“像素（pixel）”只存在于电脑显示领域，而“点（dot）”只出现于打印或印刷领域。所以，私以为，除地图打印的场景外，为了在屏幕渲染的地图分辨率应该使用PPI而不是DPI，毕竟此处强调的是屏幕分辨率。🤔\nResolution（地图分辨率）：在 GIS 领域所提到的地图分辨率（Resolution），也称地面分辨率（Ground Resolution）或空间分辨率（Spatial Resolution），表示屏幕上一个像素（pixel）所代表的实际地面距离，也就是一个像素代表多少地图单位（地图单位/像素），地图单位取决于数据所使用的空间参考（是米还是度）。\n💡 即指代地图在图像中每一像素下，表示的地理空间距离（distance per pixel）。可以是每像素多少米（平面），也可以是每像素多少度（当前主要表示经纬度直投的方式下的度量）\n在此处，是一个具体的概念与单位的结合体。本质上就是分辨率，与PPI、DPI并无太大区别，只不过前两者以英寸为单位，以单位内像素数量为度量；此处的dpp以像素为单位，以单位内地理空间距离为度量。都是用于分辨率的表达。\n是不是可以写作：Spatial distance per pixel（sdpp）、Distance per pixel（dpp）、Groud distance per pixel（gdpp） 也未可知🙄\nScale（比例尺）：地图比例尺（scale）是指地图上距离与地面实际距离的比例（也就是地图上1厘米表示实际的地面距离，厘米作为单位）。例如当实际距离为1,000米，而在地图上的距离为1厘米时，则称这一地区的比例尺是1:100,000。一般将比例尺为1:1至1:600000的地图称为大比例尺地图，1:600000至1:2,000,000的地图称为中比例尺地图，1:2000000至1:∞的地图称为小比例尺地图。在计算地图比例尺的时候，通常用到地图分辨率和屏幕分辨率这两个参数。\n比例尺是通过地面分辨率（Resolution）和屏幕分辨率（PPI）由如下公式来定义的：\nResolution：即地图分辨率、地面分辨率，是指一个像素(pixel)所代表的实际地面距离(米，此处需要将单位转换为米进行计算) PPI：屏幕分辨率度量单位，是指屏幕上每英寸长度内包含的像素数量，目前基本上都是96 0.0254(m/inch)是指米与英寸的单位转换 可以用这个公式，对比例尺(scale)和分辨率(resolution)进行换算。\n其中，WMTS 1.0.0标准中没有规定屏幕分辨率（pixel/inch），而是用像元大小（0.28mm=0.00028m）来界定的，二者的换算关系是：\nppi = 1 inch/(pixelSize(m)/0.0254) 所以，对于 WMTS 1.0.0接口（也是GeoWebCache的默认状态），PPI为：\n1 inch/(0.00028m/0.0254(m/inch))=0.0254/0.00028=90.71428571428572≈90.714 💡 如果遇到使用的空间参考单位为度，比如使用经纬度直投时，那么就需要将度转换为米进行计算。该值与OGC中的metersPerUnit对应，计算方式为：赤道处周长/360.0。CGCS2000|WGS84的转换参数为：6378137.0 * 2.0 * Math.PI / 360.0=111319.49079327358 米/度\n此段内容参考自超图的比例尺与分辨率一文，且与地理信息公共服务平台电子地图数据规范中数据中显示比例尺计算公式进行比较，确定一致。\n格网划分 准确说，应该是规则格网划分。也就是在特定的比例（尺度下）对给定的范围，按照指定的瓦片规格进行规则划分，形成平铺网格，即瓦片矩阵（TileMatrix），通常以矩阵的宽和高来表示。而瓦片矩阵集（TileMatrixSet）是由一组以不同比例定义的切片矩阵组成，也就是说，需要先有一组比例集合（尺度分级集合）。\nps: 当然也可以自行从给定的范围进行计算。\n在此，可以直接使用地理信息公共服务平台电子地图数据规范中地图分级或者是天地图的分级，就类似于OGC中提到的Well-Known Scale Set。\n欲要继续进行格网划分，在确定比例分级（尺度分级）的基础上，还需要确定如下几个参数：\ncrs：坐标参考系统，即格网所使用的空间参考 extent：格网范围，通常是所用参考系统的最大范围 PPI：一般默认使用96，不过OGC使用90.71428571428572 Tile Size：一般默认使用256 x 256，像素为单位 CornerOfOrigin：原点角，通常为左上角，TMS惯用的则是左下角 目前国内常用的大范围瓦片矩阵集（通用瓦片矩阵集）有两种，分别是基于Web墨卡托投影、和基于CGCS2000大地坐标系等间隔直投（经纬度直投）。这里，我们先看一下基于Web墨卡托投影的瓦片矩阵集的格网划分过程（也就是TileMatrixSet的构建过程），参数则取上述的默认值或通用值。\n基于Web墨卡托投影的瓦片矩阵集的格网划分 墨卡托投影，是等角正轴圆柱投影，由荷兰地图学家墨卡托(G.Mercator)于1569年创立。假想一个与地轴方向一致的圆柱切或割于地球，按等角条件，将经纬网（地球椭球面上的经纬线）投影于圆柱面上，并沿圆柱母线切开展为平面后，即得本投影。从其定义可知，墨卡托投影具有两种形态，分别是切圆柱投影及割圆柱投影，其中，最早也是最常用的是切圆柱投影（像我们平时所述的墨卡托投影基本上都是指代等角正轴切圆柱投影）。\n💡 墨卡托投影（等角正轴切圆柱投影）以赤道作为标准纬线，以首子午线（本初子午线）为中央经线，其交点为坐标原点。也就是说，投影展开的母线对应椭球面的180°经线（又称为逆子午线或对向子午线（antimeridian），是本初子午线向东或向西180度的经线，既为东经180°，又为西经180°）\nWeb 墨卡托投影是墨卡托投影（等角正轴切圆柱投影）的一种变体，同时也是 web 地图和在线服务的事实标准。对于小比例尺地图，它与标准的墨卡托用的公式一样。但Web墨卡托在所有比例尺下都使用球面公式，但大比例尺的墨卡托地图通常使用投影的椭球面形式。这种差异在全球比例尺下是察觉不到的，但会导致局部地区的地图稍微偏离同一比例尺的真正的椭球面墨卡托地图。 虽然Web墨卡托的公式是墨卡托的球面形式，但地理坐标必须得以WGS 84椭球面基准获得。此差异会导致投影略微不符合正形投影。\n💡 Web墨卡托投影在技术实现上为了简化计算，假设地球是一个球体，而非严格的椭球体。这使得它既不是严格的椭球面投影，也不是严格的球面投影，EPSG的定义说这个投影“使用椭球坐标系的球面演化”。它是建立在地球表面的WGS84椭球面模型定义的地理坐标上的，但在投影的时候却仿佛坐标是定义在球面上的。\nWeb墨卡托投影的范围：[-20037508.3427892, -20037508.3427892, 20037508.3427892, 20037508.3427892]，经投影后所得投影面的投影坐标系如下图所示：\n💡 Web墨卡托投影应是基于墨卡托（等角正轴切圆柱投影）的变体，其同样以赤道为标准纬线，本初子午线为中央经线，两者交点为坐标原点，向东和向北为正。\n此时需要再次放出TileMatrixSet的示意图了，因为要开始进行格网划分，并构建瓦片矩阵集了，有图好理解些。\n再次确定一下参数值：\ncrs：3857|900913 extent：[-20037508.3427892, -20037508.3427892, 20037508.3427892, 20037508.3427892] PPI：96 Tile Size：256 x 256 CornerOfOrigin：左上角 Scale Set：天地图，因上述地图分级缺少矩阵数量，所以改用天地图，不过比例都是一致的。 如上图所示，天地图的分级是从1级开始的，1级以2x2的分布共计4张瓦片。从此处看其缺少0级。我们可以补充一下，并可进行核对。目前地图比例分级通常都采用2的倍率进行切片，也就是说当前级比例是下一级的2倍。第0级用一张瓦片（256x256）表示全部范围，那么在此可以计算：\n// 0级的分辨率为（Extent Width / tile size） (20037508.3427892 * 2)/256=156543.03392804062 // 0级比例尺为（根据上述提供的比例尺计算公式进行计算） 1: (156543.03392804062 * 96/0.0254)=1: 5.916587109091299E8 // 核对一下1级的分辨率和比例尺 156543.03392804062/2=78271.51696402031 5.916587109091299E8/2=2.958293554545649E8 这里可以发现我们算出来的1级的比例尺的最后两位小数与天地图的有点差别（其实使用上应没有什么问题了），原因是因为我们使用的范围其实是相对不精确的。接下来，我们使用WGS84椭球体的赤道圈作为范围来计算（避免小数位保留问题导致计算不一致问题）\n// 0级的分辨率为 (20037508.3427892 * 2)/256=156543.03392804097 // 0级比例尺为 1: (((6378137.0 * 2.0 * Math.PI)/256) * 96/0.0254)=1: 5.916587109091312E8 // 核对一下1级的分辨率和比例尺 156543.03392804097/2=78271.51696402048 5.916587109091312E8/2=2.958293554545656E8 这一次发现，1级的比例尺与天地图的完全一致了。之所以从0级开始说明，这是因为当没有给定比例分级的时候，可以自行基于给定的范围和2的倍率的方式计算分级。当给定分级，则可直接基于分级做后续计算。其中重点，就是计算每一个分级下，矩阵的宽度和高度（以此来作为矩阵描述）。我们在此基于天地图的比例尺分级进行1-5级的瓦片矩阵计算，并进行对比验证（分辨率的计算方式由比例尺计算公式反推）。\n// Extent Width: (6378137.0 * 2.0 * Math.PI)=4.007501668557849E7 // 1级比例尺：2.958293554545656E8 // 1级矩阵 2x2 resolution=(2.958293554545656E8*0.0254)/96=78271.51696402048 // 范围的宽 / 瓦片对应的宽 matrixWidth=4.007501668557849E7/(256*78271.51696402048)=2 // 范围的高(Web墨卡托投影平面是正方形, 所以宽高一致) / 瓦片对应的高 matrixHigh=4.007501668557849E7/(256*78271.51696402048)=2 // 2级比例尺：1.479146777272828E8 // 2级矩阵 4x4 resolution=(1.479146777272828E8*0.0254)/96=39135.75848201024 // 范围的宽 / 瓦片对应的宽 matrixWidth=4.007501668557849E7/(256*39135.75848201024)=4 // 范围的高 / 瓦片对应的高 matrixHigh=4.007501668557849E7/(256*39135.75848201024)=4 // 3级比例尺：7.39573388636414E7 // 3级矩阵 8x8 resolution=(7.39573388636414E7*0.0254)/96=19567.87924100512 // 范围的宽 / 瓦片对应的宽 matrixWidth=4.007501668557849E7/(256*19567.87924100512)=8 // 范围的高 / 瓦片对应的高 matrixHigh=4.007501668557849E7/(256*19567.87924100512)=8 // 4级比例尺：3.69786694318207E7 // 4级矩阵 16x16 resolution=(3.69786694318207E7*0.0254)/96=9783.93962050256 // 范围的宽 / 瓦片对应的宽 matrixWidth=4.007501668557849E7/(256*9783.93962050256)=16 // 范围的高 / 瓦片对应的高 matrixHigh=4.007501668557849E7/(256*9783.93962050256)=16 // 5级比例尺：1.848933471591035E7 // 5级矩阵 32x32 resolution=(1.848933471591035E7*0.0254)/96=4891.96981025128 // 范围的宽 / 瓦片对应的宽 matrixWidth=4.007501668557849E7/(256*4891.96981025128)=32 // 范围的高 / 瓦片对应的高 matrixHigh=4.007501668557849E7/(256*4891.96981025128)=32 经过再次对比，可确定此计算与天地图以及电子地图规范中的地图分级数据一致。且已经完成格网划分动作，是的，其实就是这么简单。确定好参数后，直接进行矩阵构建（格网划分）就好了（矩阵宽，高计算）。\n如果你需要从范围去计算比例分级的话，需要注意的，本次给定的范围是比较特殊的，是相当规整的，因为Web墨卡托的投影面结果是一个正方形的平面区域。所以对于方形瓦片来说，其在正方形的区域下横纵方向的分辨率是一致的。如果给定的区域是长方形或者其他的形状，则可能需要进行一定的范围调整，以保证方形瓦片在横纵方向上的分辨率是统一的，同时矩阵在横纵方向上数量是整数（整除的）。\n基于CGCS2000经纬度投影的瓦片矩阵集的格网划分 经纬度等间隔直投：英文叫法是Platte Carre projection，是等距矩形投影（Equirectangular projection）基准点纬度取0°（赤道）时的特殊情况。它的特点是相同的经纬度间隔在屏幕上的间距相等，没有复杂的坐标变换。我们可简单的理解为，在笛卡尔坐标系中，将赤道作为X轴，子午线（本初子午线）作为Y轴，然后把本来应该在南北两极相交的经线一根一根屡直了，成为了互相平行的经线，而每条纬线的长度也在这个过程中都变为与赤道等长。\n再次确定一下参数值：\ncrs：4490 extent：[-180.0 -90.0, 180.0 90.0] PPI：96 Tile Size：256 x 256 CornerOfOrigin：左上角 Scale Set：天地图。 在这里我们直接进行1-5级的瓦片矩阵计算验证，由于当前是投影使用经纬度单位，需要需要将默认的米转换为度：\n// EPSG_4490_TO_METERS: 6378137.0 * 2.0 * Math.PI / 360.0=111319.49079327358 // 1级比例尺：2.958293554545656E8 // 1级矩阵 2x1 resolution=(2.958293554545656E8*0.0254/111319.49079327358)/96=0.703125 // 范围的宽 / 瓦片对应的宽 matrixWidth=360/(256*0.703125)=2 // 范围的高(Web墨卡托投影平面是正方形, 所以宽高一致) / 瓦片对应的高 matrixHigh=180/(256*0.703125)=1 // 2级比例尺：1.479146777272828E8 // 2级矩阵 4x2 resolution=(1.479146777272828E8*0.0254/111319.49079327358)/96=0.3515625 // 范围的宽 / 瓦片对应的宽 matrixWidth=360/(256*0.3515625)=4 // 范围的高 / 瓦片对应的高 matrixHigh=180/(256*0.3515625)=2 // 3级比例尺：7.39573388636414E7 // 3级矩阵 8x4 resolution=(7.39573388636414E7*0.0254/111319.49079327358)/96=0.17578125 // 范围的宽 / 瓦片对应的宽 matrixWidth=360/(256*0.17578125)=8 // 范围的高 / 瓦片对应的高 matrixHigh=180/(256*0.17578125)=4 // 4级比例尺：3.69786694318207E7 // 4级矩阵 16x8 resolution=(3.69786694318207E7*0.0254/111319.49079327358)/96=0.087890625 // 范围的宽 / 瓦片对应的宽 matrixWidth=360/(256*0.087890625)=16 // 范围的高 / 瓦片对应的高 matrixHigh=180/(256*0.087890625)=8 // 5级比例尺：1.848933471591035E7 // 5级矩阵 32x16 resolution=(1.848933471591035E7*0.0254/111319.49079327358)/96=0.0439453125 // 范围的宽 / 瓦片对应的宽 matrixWidth=360/(256*0.0439453125)=32 // 范围的高 / 瓦片对应的高 matrixHigh=180/(256*0.0439453125)=16 CGCS2000经纬度等间隔直投后所得投影平面就是一个长方形，宽为360°，高为180°。假如此时要以范围进行比例分级计算的话，以256x256瓦片来计算单张瓦片覆盖完整区域的情况（即0级，矩阵1x1的情况）会发现横纵方向上的分辨率不一致，且横向恰好是纵向的2倍。在GeoWebCache中，其默认以小边（即分辨率较低的一边）为主，通过一定规则对范围进行调整（即寻找一个同时适合横纵方向的分辨率，而后基于此计算新的范围）。对于[-180,-90,180,90]这个范围，最终直接以纵向的分辨率（0.703125）为主进行范围调整，因横向分辨率恰好是纵向的2倍，所以最终范围没有发生变化。\nresX=360/256=1.40625 resY=180/256=0.703125 当然，GeoWebCache中不仅只有这一种构建TileMatrixSet的策略，以范围进行构建的更适合没有提供比例分级的情况，其它的还有给定基比例分母（分辨率或比例尺）的方式，或是给定完整的比例分级集合。\n💡 再回顾一下上节附录中提到的4326瓦片矩阵集其0集是2x1，在这个TileMatrixSet中，标识符为“-1”的TileMatrix只有一个图块，有128行留空。因此，许多实施者不想提供这一级别（包括 INSPIRE 技术指南），而是更喜欢从仅用 2 个瓦片（一个用于负经度，一个用于正经度）表示世界的 TileMatrix 开始。\n所以GeoWebCache以范围进行TileMatrixSet构建的程序中，其以小边为主必然会导致如CGCS2000（或WGS84）经纬度直投的方式不存在有单张瓦片覆盖全部区域的层级。\n此段内容较为粗糙，主要以说明其计算原理为主。如果你对此部分内容感兴趣，可以去看一下GeoWebCache中GridSetFactory类的实现，或者是去看Geo Atlas中TileMatrixSetFactory类的实现（我加了部分中文注释 ☺️）\n关于像素大小（Pixel Size）的影响 对于地图分辨率（Resolution）和比例尺（Scale）来说，其实像素大小（pixel size）影响的只有比例尺。所以即使是使用不同的像素大小构建的瓦片矩阵集，只要其地图分辨率是一致的，那么瓦片就可正确的叠加在一起。如下图所示，左侧是我在GeoServer（0.28mm）中配置的CGCS2000经纬度投影的瓦片矩阵集（Geo Atlas中4490同理进行配置），右下则为天地图（0.26458mm）的CGCS2000经纬度投影的瓦片矩阵集。\n坐标转换（tile coordinates ↔ crs） 此处所提坐标转换即为描述如何将瓦片坐标系与其对应的空间坐标系进行相互转换。对于大地坐标系或投影坐标系肯定是熟悉的，这里对瓦片坐标系再次进行说明下。\n上图均为切片原点为左上角的瓦片矩阵集的瓦片坐标系，如果切片原点在左下角，那么tileCol 轴不变，而tileRow轴由下指向上，与当前图示的tileRow轴方向相反。瓦片坐标从0值开始，切片原点处的瓦片坐标为(0, 0)。\n对于两种坐标的转换，其实上文附录部分也有做简要说明，不过仅仅只是伪代码，如今我们就直接看代码吧。\n💡 注：以下所展示内容均指代切片原点在左上角的情况。\nTile Coordinates → Crs BBox 瓦片坐标 tileRow，对应Y tileCol，对应X tileZoom，对应Z 瓦片坐标系转其对应的空间坐标系（即瓦片占据的空间范围）的逻辑其实很好理解。正如格网划分是需要基于某个投影坐标系下进行，划分的同时也是将投影空间转换为瓦片矩阵空间，现在只需要反向推算回去即可。也就是，需要计算出瓦片的左下角和右上角对应的投影坐标。\n需要提供的参数：\n切片原点方位以及投影坐标：如下表示的切片原点是左上角 每个层次的分辨率以及瓦片宽高 public BoundingBox boundsFromIndex(long[] tileIndex) { final int tileZ = (int) tileIndex[2]; TileMatrix matrix = getMatrix(tileZ); long tileX = tileIndex[0]; long tileY; // 计算瓦片在横向上占据了多少地图单位，比如256在当前层级表示多少米或度 double width = matrix.getResolution() * (double) matrix.getTileWidth(); double height = matrix.getResolution() * (double) matrix.getTileHeight(); double[] tileOrigin = this.tileOrigin(); BoundingBox tileBounds; // 此处假定传递的瓦片坐标属于google体系的瓦片坐标系，即原点在左上角 if (CornerOfOrigin.TOP_LEFT.equals(this.cornerOfOrigin)) { tileY = tileIndex[1]; // (minx, miny, maxx, maxy) tileBounds = new BoundingBox(tileOrigin[0] + width * (double) tileX, tileOrigin[1] - height * (double) (tileY + 1L), tileOrigin[0] + width * (double) (tileX + 1L), tileOrigin[1] - height * (double) tileY); } else { throw new IllegalArgumentException(\u0026#34;Unsupported corner of origin: \u0026#34; + this.cornerOfOrigin); } return tileBounds; } 💡 值得注意的是，我在此表示BBox用的是左下角和右上角的方式。此种思考方式与切片原点在左上角的瓦片坐标系有些不太统一，总之有点别扭。或许当切片原点在左上角时，使用左上角与右下角表示BBox对于思考更友好些 也未可知啊 🙄\n这难道就是OGC初始提倡使用左下角作为切片原点的理由吗？一方面左下角沿横纵方向都是逐渐增大，更符合人们的思考；另一方面，对于BBox的描述多使用左下角和右上角？🤨\nCrs BBox → Tile Coordinates 上文附录提到的BBOX转瓦片坐标系，是获取到一系列的瓦片坐标系。也就是给定BBox，然后获取到对用的瓦片坐标范围。但是附录中并未提到关于比例层级的内容，不过逻辑是所有层级都是共用的。\n那么在此基于GeoWebCache的实现，提供两类BBox → Tile Coordinates的转换方法。一类是获取与给定范围最接近的瓦片坐标系，另一类则是上文附录提到的一系列的瓦片坐标系。\nClosestIndex 此方法为获取与给定范围最接近的瓦片坐标系，其仅仅适用于将一个正确的瓦片的BBox转换为瓦片坐标，不适用于任意的BBox（即随意构造的BBox）。\n// 首先是找到与给定的BBox最匹配的TileMatrix的索引，它遍历所有层级的瓦片矩阵， // 计算每个层级的分辨率误差，并选择误差最小的层级。同时，它还检查分辨率的一致性， // 确保与之前的最佳分辨率相差不超过10%。 protected long[] closestIndex(BoundingBox tileBounds) throws MatrixMismatchException { double wRes = tileBounds.getWidth() / getMatrix(0).getTileWidth(); // 保存最小的分辨率误差，初始值为最大双精度值（无限大） double bestError = Double.MAX_VALUE; // 保存最佳的瓦片矩阵层级索引，初始值为-1 int bestLevel = -1; // 保存最佳的分辨率，初始值为-1.0 double bestResolution = -1.0; for (int i = 0; i \u0026lt; getNumLevels(); i++) { TileMatrix grid = getMatrix(i); double error = Math.abs(wRes - grid.getResolution()); if (error \u0026lt; bestError) { bestError = error; bestResolution = grid.getResolution(); bestLevel = i; } else { break; } } if (Math.abs(wRes - bestResolution) \u0026gt; (0.1 * wRes)) { throw new ResolutionMismatchException(wRes, bestResolution); } return closestIndex(bestLevel, tileBounds); } protected long[] closestIndex(int level, BoundingBox tileBounds) throws MatrixAlignmentMismatchException { TileMatrix grid = getMatrix(level); double width = grid.getResolution() * grid.getTileWidth(); double height = grid.getResolution() * grid.getTileHeight(); double x = (tileBounds.getMinX() - tileOrigin()[0]) / width; double y = (tileOrigin()[1] - tileBounds.getMaxY()) / height; long posX = Math.round(x); long posY = Math.round(y); if (Math.abs(x - posX) \u0026gt; 0.1 || Math.abs(y - posY) \u0026gt; 0.1) { throw new MatrixAlignmentMismatchException(x, posX, y, posY); } long[] ret = {posX, posY, level}; return ret; } ClosestRectangle 此方法对应上文附录提到的将给定BBox转换为对应的一系列的瓦片坐标系。其同样是先找到与给定的BBox最匹配的TileMatrix的索引，而后计算对应索引层级下BBox对应的瓦片坐标范围。该方法适用于任意范围。\npublic long[] closestRectangle(BoundingBox rectangleBounds) { double rectWidth = rectangleBounds.getWidth(); double rectHeight = rectangleBounds.getHeight(); double bestError = Double.MAX_VALUE; int bestLevel = -1; // Now we loop over the resolutions until for (int i = 0; i \u0026lt; getNumLevels(); i++) { TileMatrix grid = getMatrix(i); double countX = rectWidth / (grid.getResolution() * grid.getTileWidth()); double countY = rectHeight / (grid.getResolution() * grid.getTileHeight()); double error = Math.abs(countX - Math.round(countX)) + Math.abs(countY - Math.round(countY)); if (error \u0026lt; bestError) { bestError = error; bestLevel = i; } else if (error \u0026gt;= bestError) { break; } } return closestRectangle(bestLevel, rectangleBounds); } protected long[] closestRectangle(int level, BoundingBox rectangeBounds) { TileMatrix grid = getMatrix(level); double width = grid.getResolution() * grid.getTileWidth(); double height = grid.getResolution() * grid.getTileHeight(); long minX = (long) Math.floor((rectangeBounds.getMinX() - tileOrigin()[0]) / width); long minY = (long) Math.floor((tileOrigin()[1] - rectangeBounds.getMaxY()) / height); long maxX = (long) Math.ceil(((rectangeBounds.getMaxX() - tileOrigin()[0]) / width)); long maxY = (long) Math.ceil(((tileOrigin()[1] - rectangeBounds.getMinY()) / height)); // We substract one, since that\u0026#39;s the tile at that position long[] ret = {minX, minY, maxX - 1, maxY - 1, level}; return ret; } TileMatrixSet in Geo Atlas 其实在Geo Atlas，用于构建矢量切片服务的Java基础库一文中也曾提到，Pyramid、IO(in Library)、Tile Cache模块均来自GeoWebCache，是对其进行了拆解和少量变更。TileMatrixSet就是来自于GeoWebCache的GridSet，在此处对其进行了少量修改：\n切片原点修改为左上角（GeoWebCache默认还是使用左下角作为切片原点，不过也支持配置为左上角）\n将构造GridSet的SRS变更为的CRS（不再割裂）\n默认提供3857\u0026amp;4490（其实3857就是900913，只不过GeoTools原生并没有提供900913的坐标系声明；此处4490是遵循天地图CGCS2000经纬度等间隔直投的尺度分级构建，包含0级）\nTileMatrixSet WORLD_EPSG_3857 = TileMatrixSetFactory.createTileMatrixSet( projectedName, CRS.decode(\u0026#34;EPSG:3857\u0026#34;, true), BoundingBox.WORLD3857, CornerOfOrigin.TOP_LEFT, TileMatrixSetFactory.DEFAULT_LEVELS, null, TileMatrixSetFactory.DEFAULT_PIXEL_SIZE_METER, 256, 256, false); TileMatrixSet WORLD_EPSG_4490 = TileMatrixSetFactory.createTileMatrixSet( projectedName, CRS.decode(\u0026#34;EPSG:4490\u0026#34;, true), BoundingBox.WORLD4490, CornerOfOrigin.TOP_LEFT, 1.40625d, // 天地图经纬度投影0级的分辨率 TileMatrixSetFactory.DEFAULT_LEVELS, TileMatrixSetFactory.EPSG_4326_TO_METERS, TileMatrixSetFactory.CGCS2000_PIXEL_SIZE_METER, 256, 256, false); 命名变更，将GridSet变更为TileMatrixSet，Grid变更为TileMatrix。理由是：更符合瓦片矩阵的命名，同时OGC中也是如此命名的。\n小结 下篇尝试着对TileMatrixSet的相关计算原理进行探讨说明，文中先后描述了TileMatrixSet在GeoWebCache及Geo Atlas中的实现，同时结合天地图提供的两种投影（Web墨卡托以及CGCS2000经纬度等间隔直投）下的瓦片矩阵集进行计算验证。整体看来，感觉还是有些混乱，计算原理部分的说明不够清晰，未达成心中想要的标准（布局合理、前后连贯、逻辑分明、层层递进、朗朗上口，也未可知啊 😒），再接再厉吧！ 🙄\n个人理解始终有限，如有存在偏颇的方法，还希望大家多多指教 😬🤝！\n参考 电子地图数据规范 再谈TileMatrixSet，二维瓦片金字塔结构的标准定义（上） Gridsets and Gridsubsets 比例尺与分辨率 国内主要地图瓦片坐标系定义及计算原理 切片地图元信息配置说明 Web墨卡托投影 ESRI-墨卡托投影 百度百科-墨卡托投影 未经投影的地理坐标系如何显示为平面地图 ","permalink":"https://fuyi-atlas.github.io/posts/gis/what-is-tilematrixset-calculation-principle/","summary":"前言 书接上回，本章节为下篇：TileMatrixSet实现及相关计算原理探讨。\n本章节将以TileMatrixSet模型的典型实现，即GeoWebCache中的Gridset作为开端，对OGC中TileMatrxSet模型进行印证。而后对TileMatrixSet相关计算原理进行探讨，并以CGCS2000切片方案为例进行验证。另附带说明TileMatrixSet在Geo Atlas中的实现及应用 🤨。\nGridset \u0026amp; TileMatrixSet 这里就不再对GeoWebCache做介绍了，直接切入主题。GeoWebCache中的Gridset正是对应着TileMatrixSet模型，我们先来看一下GeoWebCache对于Gridset的相关介绍：\nGridsets and Gridsubsets Gridsets 和 Gridsubsets 是指 GeoWebCache 所服务的图层的空间参考系统（the spatial reference system）。从本质上来说，正如 Tiles 中所介绍的，GeoWebCache 与参考系统无关。当 GeoWebCache 向 WMS 发出请求时，它使用 Gridsets 和 Gridsubsets 信息将其内部切片索引转换为 WMS 可以理解的空间请求。\n💡 说实话，有时候我觉得老外的啰嗦话挺多的，其实这里就是表达GeoWebCache就是使用 Gridsets 和 Gridsubsets 将瓦片坐标转换为瓦片对应的空间范围的。从其实现来看，此处所述的内部切片索引正是对应着瓦片坐标系。\n下面分别对 Gridset 和 Gridsubset 的构成进行描述，此处引用原文：\nA gridset is a global definition (i.e. not layer-specific) specifying:\n全局定义即对应着通用的切片方案，所以不是特定于层（图层）的。\nA spatial reference system (EPSG code)\n对应着投影坐标系。\nA bounding box describing the extent, typically the maximum extent for the above reference system","title":"再谈TileMatrixSet，二维瓦片金字塔结构的标准定义（下）"},{"content":"前言 其实，在此前矢量金字塔技术研究一文中已经大致提及了瓦片金字塔与TileMatrixSet的关系，为什么还要在这里再次说明呢？主要是前文更侧重于矢量金字塔的概念定义，对于TileMatrixSet的描述过少，所以才会再次对TileMatrixSet进行说明。本文将基于 OGC Two Dimensional Tile Matrix Set 标准对TileMatrixSet进行研究探讨，将侧重于说明其基本构成要素的定义以及整体的技术实现。而GeoWebCache中的GridSet作为典型实现，本文将会对其进行再次说明，以印证OGC 关于TileMatrixSet的定义。由于内容较多，本文将分为下上两个篇章进行描述。上篇是对TileMatrixSet以及其构成要素的基本概念进行描述，而下篇则是对GeoWebCache中的GridSet实现进行研究，同时结合我国CGCS2000切片方案对TileMatrixSet格网划分计算原理进行探讨，另附带说明Geo Atlas中关于此模块的实现细节。\n本章节为上篇：TileMatrixSet及其构成要素的基本概念。\n什么是TileMatrixSet？ TileMatrixSet，即Tile Matrix Set，源自 OGC Two Dimensional Tile Matrix Set(目前已更新到v2)。该标准定义了瓦片矩阵集的规则和要求，作为一种基于一组规则网格对空间进行索引的方式，这些规则网格为坐标参考系统中有限比例列表定义了一个域（瓦片矩阵）。\n💡 The OGC Two Dimensional Tile Matrix Set and Tile Set Metadata Standard defines the rules and requirements for a tile matrix set as a way to index space based on a set of regular grids defining a domain (tile matrix) for a limited list of scales in a Coordinate Reference System.\n在瓦片矩阵集中，每个瓦片矩阵被划分为规则的瓦片。在瓦片矩阵集合中，瓦片可以由瓦片列、瓦片行和瓦片矩阵标识符（即矩阵集标识与层标识）来唯一地标识。\n💡 In a tile matrix set, each tile matrix is divided into regular tiles. In a tile matrix set, a tile can be univocally identified by a tile column, a tile row, and a tile matrix identifier.\n再来看看Tiles API中的描述：该标准定义了用于指定瓦片矩阵集和描述瓦片集的逻辑模型和编码。一个瓦片矩阵集代表一种切片方案，它使得应用程序能够基于坐标参考系统 (CRS) 中为多个尺度（比例分级）定义的一组规则网格来分区和索引空间。\n💡 That Standard defines logical models and encodings for specifying tile matrix sets and describing tile sets. A tile matrix set is a tiling scheme that enables an application to partition and index space based on a set of regular grids defined for multiple scales in a Coordinate Reference System (CRS).\n从上面我们可以获取到如下几个信息：\n它是用于对空间进行分区和索引的一种方式方法 基于某个坐标参考系统，并基于该坐标参考系统持有的一组有限比例列表（尺度分级） 表现为一组规则格网，且该组格网与有限尺度分级列表一一对应 基于此，可以确定TileMatrixSet正是对应着瓦片金字塔结构。TileMatrixSet基于的坐标系对应着瓦片金字塔的瓦片投影坐标系，格网表达对应着瓦片组织结构与瓦片坐标系，有限尺度分级对应瓦片金字塔尺度分层。从瓦片金字塔模型来说，TileMatrixSet基于多尺度分级实现金字塔的多尺度分层表达，并基于此为每一个尺度定义了一个边界，而后基于一定规则进行格网划分进而形成一个域（瓦片矩阵），此为分块。\n对于分层以及分层分块的差异可参见如下两图：\n术语 Cell（单元） minimum geometrical spaces delimited by the grid lines of a regular grid.\n由规则网格的网格线界定的最小几何空间。\nNote 1 : in 2D spaces, cells are often referred as pixels.\n在二维空间中，单元通常称为像素。\nNote 2 : In this standard, the term pixel is reserved to the individual elements of a visualization device. Tiles are composed by regular grid cells that can be made partially coincident with the pixels of a visualization device for display purposes.\n在本标准中，术语“像素”专指可视化设备的单个元素。瓦片由规则网格单元组成，这些网格单元可以部分与可视化设备的像素重合，以用于显示目的。\nRaster Tile（栅格切片） tile that contains information in a gridded form. Commonly the values of the grid represent colors of each cell in the grid for immediate pictorial representation on visualization devices, but can also be coverage subsets.\n包含网格形式信息的切片。通常，网格的值表示网格中每个单元格的颜色，以便在可视化设备上立即进行图形表示，但也可以是覆盖子集。\n💡 Coverage：表示空间上变化的现象（可以是离散的或连续的）。覆盖可以是多种形式的数据，例如栅格、点云、多边形等。用于描述和存储空间现象的变化，可以是温度、降水量、高程等。例如，数字高程模型（DEM）是一种覆盖，表示地形的高程。可以以多种格式存储，包括 NetCDF、GeoTIFF、HDF 等。\nCoverage Subsets：这指的是从更大的覆盖数据集中提取特定部分。例如，你可能拥有一个全球温度数据集，但只需要覆盖欧洲的子集。子集提取使你可以提取并处理你需要的那部分数据，通常基于空间或时间标准。\n栅格是Coverage的一种特定形式，表示为一个网格，每个网格单元（像素）都有一个值。栅格数据通常用于表示连续的空间现象。栅格数据广泛用于遥感图像、土地覆盖分类、气象数据等。例如，卫星图像是一种常见的栅格数据，每个像素表示该位置的反射率值。栅格数据通常以图像格式存储，例如 GeoTIFF、JPEG、PNG 等，也可以以网格格式存储，例如 ASCII Grid、ESRI Grid 等。\nps: 本段内容来自ChatGPT\nspace partitioning（空间分区） process of dividing a geometric space (usually a Euclidean space) into two or more disjoint subsets (see also partition of a set). Space partitioning divides a space into non-overlapping regions. Any point in the space can then be identified to lie in exactly one of the regions.\n将几何空间（通常是欧几里得空间）划分为两个或多个不相交子集的过程（另请参见集合的划分）。空间分区将空间划分为不重叠的区域。然后可以识别空间中的任何点恰好位于其中一个区域\ntessellation（细分曲面） partitioning of a space into a set of conterminous subspaces having the same dimension as the space being partitioned.\n将空间划分为一组与被划分空间具有相同维度的连续子空间。\nNote 1 : A tessellation composed of congruent regular polygons or polyhedra is a regular tessellation. One composed of regular, but non-congruent polygons or polyhedra is a semiregular tessellation. Otherwise the tessellation is irregular.\n由全等的正多边形或多面体组成的镶嵌是正则镶嵌。由规则但不全等的多边形或多面体组成的一种是半规则镶嵌。否则镶嵌是不规则的。\n💡 tessellation：将二维区域划分为多边形块，或将三维区域划分为多面体块，这样就不会存在图形重叠和任何间隙。（来自ESRI的GIS字典）\nTile（瓦片） In the context of a 2D tile matrix, a tile is one of the rectangular regions of space, which can be uniquely identified by an integer row and column, making up the tile matrix.\n在 2D 瓦片矩阵的上下文中，瓦片是空间的矩形区域之一，可以通过构成瓦片矩阵的整数行和列来唯一标识。\nIn the context of a geospatial data tile set, a tile contains data for such a partition of space as part of an overall set of tiles for that tiled geospatial data.\n在地理空间数据瓦片集的上下文中，瓦片包含这样的空间分区的数据，作为该瓦片地理空间数据的整个瓦片组的一部分\nNote 2 : Tiles are useful to efficiently request, transfer, cache, display, store and process geospatial data for a specific resolution and area of interest, providing deterministic performance and scalability for arbitrarily large datasets.\n瓦片可有效地请求、传输、缓存、显示、存储和处理特定分辨率和感兴趣区域的地理空间数据，为任意大的数据集提供确定性的性能和可扩展性。\nNote 3: Tiles can contain a variety of data types, such as grid-based pictorial representations (map tiles), coverage subsets (coverage tiles), or feature-based representations (vector tiles).\n瓦片可以包含多种数据类型，例如基于网格的瓦片表示（栅格地图瓦片）、覆盖子集（覆盖瓦片）或基于特征的表示（矢量瓦片）。\nTiling Schema（切片方案） scheme that defines how space is partitioned into individual tiles, potentially featuring multiple levels of detail (each tiling at a different granularity to reflect a different resolution or scale).\n定义如何将空间划分为各个瓦片的方案，可能具有多个细节层次（每个层次瓦片的粒度不同，以反映不同的分辨率或比例）\nA tiling scheme defines the spatial reference system and the geometric properties of each tile defined by the scheme. Those properties include which space each tile occupies, i.e. its extent, as well as a tile coordinate origin if a particular corner of origin convention is established.\n切片方案定义了空间参考系统以及该方案定义的每个切片的几何属性。这些属性（几何属性）包括每个切片占据的空间，即其范围，以及切片坐标原点（如果建立了特定的原点约定角）。\n💡 ps：难道不应该强制要求，必须进行切片坐标原点的指定吗？无论是左上角还是左下角还是其他🤔\nTile Matrix（瓦片矩阵） tiling grid in a given 2D coordinate reference system, associated to a specific scale and partitioning space into regular conterminous tiles, each of which being assigned a unique identifier.\n在给定的二维坐标参考系统中平铺网格，与特定比例相关联，并将空间划分为规则的连续瓦片，每块瓦片都被分配一个唯一的标识符。\nNote 1 to entry: Each tile of a tile matrix is uniquely identifiable by a row and a column integer indices. The number of rows is referred to as the matrix height, while the maximum number of columns is referred to as the matrix width (the number of columns can vary for different rows in variable width tile matrices).\n瓦片矩阵的每块瓦片都可以通过行和列整数索引来唯一标识。行数称为矩阵高度，而最大列数称为矩阵宽度（可变宽度瓦片矩阵中的不同行的列数可能不同）。\n💡 瓦片矩阵，对应的是瓦片金字塔结构中的金字塔分层，是在特定比例（尺度）下，基于某种规则进行格网划分后形成的一个域。\nTile Matrix Set（瓦片矩阵集） tiling scheme consisting of a set of tile matrices defined at different scales covering approximately the same area and having a common coordinate reference system.\n切片方案由一组以不同比例定义的切片矩阵组成，覆盖大致相同的区域并具有公共坐标参考系。\n💡 Tile Matrix Set即为切片方案（Tiling Schema）的一种具体实现，故在此可将其称为切片方案。这也对应着OGC Tiles API中所述：一个瓦片矩阵集代表一种切片方案（A tile matrix set is a tiling scheme）\nTile Indexing Scheme（瓦片索引方案） scheme allowing to uniquely reference a tile in a tiling scheme by the use of a unique identifier (or set of identifiers), and reversely, which unique identifier (or unique set of identifiers) corresponds to a space satisfying the geometric properties of a specific tile.\n允许通过使用唯一标识符（或标识符集）来唯一引用切片方案中的瓦片的方案，反之亦然，该唯一标识符（或唯一标识符集）对应于满足特定瓦片的几何属性的空间。\n💡 瓦片索引方案对应瓦片坐标系，上面描述的是瓦片坐标系（唯一标识符或标识符集，如：x、y、z）与基于空间参考下瓦片占据的空间（即其范围，BBox）之间的对应关系，可以相互转换。\nTile Set（切片集） a set of tiles resulting from tiling data according to a particular tiling scheme.\n根据特定切片方案由切片数据生成的一组切片。\n💡 从使用上来说，可以是逻辑的（逻辑存在，但还未生成的），也可以是物理存在的。\nTile Set Metadata（切片集元数据） additional metadata beyond the common properties defining the tile set. Such metadata could be an abstract, the owner, the author, or other common metadata.\n除了定义切片集的公共属性之外的其他元数据。此类元数据可以是摘要、所有者、作者或其他常见元数据。\nmetadata describing common properties defining a tile set, layers and styles used to produce the tile set, the limits of the tile matrix with actual data and common metadata such as abstract, owner, author, etc.\n描述定义切片集的公共属性的元数据、用于生成切片集的图层和样式、具有实际数据的瓦片矩阵的限制以及公共元数据（例如摘要、所有者、作者等）。\n💡 总的来说，切片集元数据提供有关切片集的预期用途以及其中包含的原点、访问限制、切片方案、图层、要素属性以及公共元数据（例如摘要、所有者、作者等）信息\nVector Tile（矢量切片） tile that contains vector information that has been generalized (simplified) at the tile scale resolution and clipped by the tile boundaries.\n包含已按瓦片比例分辨率概括（简化）并由瓦片边界裁剪的矢量信息的切片。\n💡 实际情况下，如GeoServer所支持的矢量切片，其对于矢量数据的概括（简化）做的并不好。\nWell-Known Scale Set（通用比例集） well-known combination of a coordinate reference system and a set of scales that a tile matrix set declares support for.\n众所周知的坐标参考系统和瓦片矩阵集声明支持的一组比例的组合。\n💡 其实就是一些通用的，公知的比例尺分级方案，比如：基于Web墨卡托投影的比例尺分级。需要注意的是，它仅仅只有比例尺分级，只是TileMatrixSet的一部分，两个并不相等。\n构成要素的基本理念与模型定义 此处主要描述在二维空间中，TileMatrixSet的构成要素与逻辑模型定义。\n从TileMatrixSet的定义（切片方案由一组以不同比例定义的切片矩阵组成，覆盖大致相同的区域并具有公共坐标参考系）可知，Tile Matrix、尺度分级集合以及瓦片坐标系就是TileMatrixSet的主要构成要素。\nTile Matrix 可以为瓦片定义两个主要用例：存储和可视化。当瓦片在可视化设备中渲染时，空间以像素为单位进行量化，并以尺寸为特征，就会出现比例的概念。然后，前两个空间维度的CRS单位的瓦片大小和可视化设备像素的大小变得相关。两个空间维度与设备的像素轴对齐。\n在栅格瓦片中，定义了与瓦片矩阵重合但更密集的第二个规则网格（单元尺寸较小，但为该尺寸的精确约数）。这个新的更高分辨率网格的每个网格单元称为网格单元。网格单元是通过使用瓦片中渲染单元的数量（瓦片宽度和瓦片高度）将原始瓦片平均划分为网格单元来定义的。在常见的平铺 2D 可视化客户端中，网格单元的一部分与设备像素重合，并且网格的这一部分在设备中渲染：第二个网格在此被命名为外推设备网格。换句话说，瓦片在 CRS 的每个维度上被划分为多个单元，其方式创建的单元在可视化期间将变得与可视化设备的像素完全相同。\n💡 第二个规则网格（外推设备网格）是一个更高分辨率的网格，相比原始瓦片矩阵，它更密集。通过将原始瓦片划分为多个更小的网格单元（256x256 → 64x64），这些网格单元在可视化设备上与设备像素重合，从而在可视化过程中提供更高的分辨率和更清晰的显示效果。这种方法确保了地图或图像在不同设备上的渲染效果达到最佳。\n总感觉这是在要求瓦片的实现端需要满足不同设备的渲染兼容性。\nTile Matrix 的构成要素 原点：规则网格覆盖的边界框的二维空间中的原点和原点角（例如，整数坐标为0的左上角的CRS坐标）。这是切片集原点，定义整个切片集的空间原点参考点的位置。\n瓦片大小：CRS 每个维度的瓦片大小（以 CRS 单位表示）\n这里隐含的是每个维度（尺度层级）下瓦片大小要一致，但是不同维度的瓦片大小可以不一致。不过在实际使用中，基本上全维度都使用相同的瓦片大小。同时，瓦片的大小基本都使用像素作为单位进行换算。\n以瓦片单位表示的瓦片矩阵的大小（即瓦片数量），用于封闭（关闭，合围）瓦片空间的边界框。通常，前两个维度的大小称为矩阵宽度和矩阵高度。\n比例（表示为比例分母），可以基于给定的BBox进行计算\n由于服务无法预测客户端可视化设备的像素大小，因此在本标准中，比例分母是针对“标准化渲染像素大小”0.28 mm x 0.28 mm（毫米）定义的。该定义与 Web 地图服务 WMS 1.3.0 OGC 06-042 和后来被 WMTS 1.0 OGC 07-057r7 采用的符号编码 (SE) 实现规范 1.1.0 OGC 05-077r4 中使用的定义相同。通常，设备的真实像素尺寸是未知的，0.28 毫米是 2005 年普通显示器的实际像素尺寸。即使当前的显示设备采用小得多的像素尺寸，该值仍被用作参考。\n自 20 世纪 80 年代以来，Microsoft Windows 操作系统已将其默认的标准显示每英寸像素 (PPI) 设置为 96。该值的结果约为每像素 0.264 mm。该值与本标准中采用的实际 0.28mm 的相似性可能会造成一些混乱。\n现代显示设备（屏幕）的像素非常小，以至于操作系统允许定义大于 1 的呈现比例因子（例如 150%）。在这些情况下，设备像素的实际大小与操作系统使用的大小不同。\n通常，矩阵宽度是恒定的，并且在这种情况下，使用单个标准化渲染单元尺寸的两个维度的单个比例因子会导致单元在前两个维度中具有相同的尺寸。这通常称为方形像素。\n💡 保持矩阵宽度恒定，并在两个维度上使用单个标准化渲染单元尺寸的比例因子，可以确保像素在两个维度上具有相同的尺寸，即方形像素。方形像素保证了数据在空间分析中的一致性和准确性，避免了由于像素尺寸不一致而导致的图像变形和数据误差。这在 GIS、遥感和其他空间数据处理应用中是至关重要的。\n不过，WMS 可以允许非方形像素（尽管许多实现无法正确支持非方形像素）。\n瓦片空间 对于二维空间的情况，当给定瓦片矩阵的 CRS 坐标左上角点（tileMatrixMinX、tileMatrixMaxY）、瓦片矩阵的瓦片单位宽和高（matrixWidth、matrixHeight）、瓦片中的渲染单元格值（tileWidth、tileHeight）、将坐标参考系 (CRS) 单位转换为米的系数（metersPerUnit）和比例尺（1：scaleDenominator），瓦片矩阵边界框的右下角（tileMatrixMaxX、tileMatrixMinY）可以按如下方式计算：\n💡 metersPerUnit (crs)： 将坐标参考系统 (CRS) 单位转换为米的系数。也就是说，这个参数表示的是将给定的CRS中一个单位转换为米的系数。换句话说，也就是在指定的CRS中，一个单位表示多少米。目前常用的就两种投影，一是以米为单位的（即metersPerUnit为1）；其次是以度为单位的经纬度投影（metersPerUnit表示为1度代表多少米，即：360/赤道周长，不同CRS使用不同的椭球体，所以其赤道周长也会存在一定差异。）例如，WGS84 metersPerUnit (crs) 为 111319.4908 米/度。\n瓦片矩阵中的每个瓦片均由其瓦片列索引和瓦片行索引来标识，这些索引的 0,0 原点位于瓦片矩阵的一个角。当使用左上角时，tileCol向右增加，TileRow向底部增加，如图5所示（左下角也可以用作原点，使TileRow向顶部增加）。\n图块矩阵可以实现为文件夹中的一组图像文件（例如，PNG 或 JPEG），每个文件代表一个图块。\nTIFF 规范 v6 的第 6 节以与本标准中相同的方式定义了 2D 图块。切片矩阵中的所有切片都可以存储在单个 TIFF 文件中。 TIFF 文件仅包含一组共享公共单一比例的连续图块。\nTile Matrix Set 根据需要在客户端屏幕中表示的比例范围，单个瓦片矩阵（即单层次的矩阵）是不切实际的，并且可能会迫使软件在渲染之前花费太多时间来简化/概括数据集。（因为它仅仅是实现了数据横向上的分块，并没有其他的加成，这也是多尺度分级出现的原因）\n通常，会逐步定义多个瓦片矩阵，以覆盖应用程序所需的预期比例范围。瓦片矩阵集是由一组瓦片矩阵组成的切片方案，针对特定比例进行了优化（所谓优化即为概括|简化等处理，使得数据可以更好、更快的显示），并由瓦片矩阵标识符标识。每个瓦片矩阵集都有一个可选的近似边界框（即BBox，之所以近似应是因为一般取是最小外接矩形MBR →Minimum Bounding Rectangle，并非完全一致。Maybe🤔），但每个瓦片矩阵都有一个精确的边界框，该边界框是从其他参数间接推导出来的。由于单元格对齐方式不同，每个比例下的瓦片矩阵边界框通常会略有不同（比如4490经纬度投影的0级和1级的边界框就不同）。\n瓦片矩阵在瓦片矩阵集中具有唯一的字母数字标识符。一些基于瓦片的实现更喜欢使用缩放级别编号或细节级别指定 (LoD)，其优点是建议瓦片矩阵列表中的某种顺序。本标准不使用缩放级别概念，但为了在更喜欢数字缩放级别的实现中轻松采用本标准，附件 D 中定义的许多瓦片矩阵集使用数字作为瓦片矩阵标识符。在这种情况下，瓦片矩阵集中定义的瓦片矩阵列表中的索引顺序仍然可以在内部用作缩放级别排序。\n在一些其他标准中，瓦片矩阵集概念称为图像金字塔😯，如 OGC KML 2.2 OGC 07-147r2 标准的第 11.6 条中所示。 JPEG2000 (ISO/IEC 15444-1) 和 JPIP (ISO/IEC 15444-9) 也使用类似的空间划分，称为分辨率级别。然而，在这些情况下，金字塔是自定义的，从更详细的瓦片矩阵（使用方形瓦片）开始，并通过连续聚合前一个尺度的 4 个瓦片来构造下一个尺度的瓦片，依此类推（见图 6） ，并将前一个尺度的每 4 个连续值插值到下一个尺度的一个值中。该方法涉及更严格的结构，该结构具有与二的幂相关的比例以及与较低比例分母上的图块完美重叠的图块（四叉树结构，自底向上进行瓦片金字塔构建）。本文档中介绍的平铺矩阵集更加灵活，但 KML superoverlays 或基于 JPEG2000 的实现可以使用此标准以及一些额外的规则来描述其瓦片矩阵集。本文档描述了一些与附录 D 中的 2 的幂相关的比例集的瓦片矩阵集。【可视为通用规范与细分规范的区别，如图像金字塔的基础理论便是与瓦片矩阵集相同的】\n💡 注意：客户端和服务器在比较浮点数与公差时必须小心（必须使用双精度、16 位数字）。\nWell-known scale sets（通用比例集） 当在集成客户端中需要重叠和呈现不同瓦片矩阵集中编码的瓦片时，由于这些瓦片不具有公共比例分母集和相同的 CRS，将瓦片重新缩放或重新投影到视图的公共比例可能需要重新采样计算，从而导致视觉效果质量下降。\n防止此问题的推荐方法是使用相同的全局瓦片矩阵集。如果数据的地理范围仅覆盖瓦片矩阵集区域的一部分，则瓦片集元数据的瓦片矩阵限制元素（Tile Matrix Limits）可用于通知这些限制。\n如果无法使用相同的瓦片矩阵集，则使用公共 CRS 以及由尽可能多的层和服务共享的公共尺度集也可以是一种解决方案。由此，引入了众所周知的尺度集（WKSS → well-known scale set）的概念。【众口难调，不如统一】\n请注意，WKSS 仅定义了完整定义瓦片矩阵集所需内容的一小部分。 WKSS 是一项可选功能，它不能取代定义瓦片矩阵集及其瓦片矩阵的需要。如果服务共享并引用通用瓦片矩阵集定义（通用的切片方案，例如基于Web墨卡托投影的切片方案），则不再需要 WKSS 的最初目的。\n💡 从结果来看，Well-known scale sets更像是一个过程中的产物。现如今大家基本都会直接使用通用的切片方案，或者是自定义切片方案。\nTile based coordinates in a tile matrix set（瓦片坐标系） 基于瓦片的坐标中的瓦片可以通过其在瓦片矩阵维度中的瓦片位置和瓦片矩阵集中的瓦片矩阵标识符来引用。在二维空间中，瓦片由以下 3 个离散索引名称来标识：瓦片行、瓦片列和瓦片矩阵标识符。【也就是我们常说的瓦片坐标系】\n在栅格图块中，外推设备网格域集中的网格单元可以通过 CRS 中的一组浮点坐标以及通过不存在舍入问题的两种方式之一来标识，如下所示。\n网格单元由瓦片索引（通过其在瓦片矩阵维度中的瓦片位置和瓦片矩阵集中的瓦片矩阵标识符引用）和瓦片内的单元索引（i、j、\u0026hellip;）组成。在二维空间中，一个单元由 5 个离散索引标识，这些索引名为：瓦片行、瓦片列、瓦片矩阵标识符、i 和 j。这就是 GetFeatureInfo 在 WMTS 中的工作方式。这组坐标称为“瓦片坐标”。\n💡 在大部分情况下，瓦片行、瓦片列、瓦片矩阵标识符已经足够使用，i和j应是在需要精确获知点位坐标的时候才需要使用吧🤔\n通过瓦片矩阵的外推设备网格域集（从瓦片空间的左上角开始）和瓦片矩阵集中的瓦片矩阵标识符定义的网格单元位置。在二维空间中，网格单元由 3 个离散索引标识，分别为：i \u0026lsquo;、j \u0026rsquo; 和瓦片矩阵标识符。请注意，i \u0026rsquo; 和 j \u0026rsquo; 可以是非常大的整数，并且对于非常详细的比例，如果将瓦片矩阵存储为二进制数，则可能需要 64 位整数表示法。这组索引称为“瓦片矩阵坐标”。\n💡 基于此可以随意的进行二次网格划分，可以更好的适配不同设备分辨率。我没有去调研过，但相信如今不同设备分辨率下瓦片渲染效果应当是各地图渲染引擎给做好了适配兼容了吧。毕竟，现如今基本都是全端设备共用一个tile size（如：256x256）。当然，这和现今设备的分辨率都比较高应该也有很大的关系吧🤔\n💡 在目前瓦片服务中，基本上只需要给定瓦片行、瓦片列、瓦片矩阵标识即可，大部分时候所述的瓦片坐标系就是这三者组合。当然，瓦片坐标系各种各样，可以基于四叉树，可以是其他任意的法则，只不过瓦片行、瓦片列、瓦片矩阵标识的组合更加通用而已。\nVariable width tile matrices（可变宽矩阵） 到目前为止，假设所有瓦片行的矩阵宽度（matrixWidth）都是恒定的。这是不使地球扭曲太多的投影的常见用法。但是，当使用等距矩形板方形投影（参见附件 D 第 2 款，经纬度投影，如：WorldCRS84Quad）时，靠近两极的瓷砖的畸变会增加。在极端情况下，上方瓦片的上行（代表北极的那一行）包含一系列重复值，这些值代表空间中几乎相同的位置。对于下方瓦片的下排（代表南极的那一个）也可以这样说。当瓦片以平面投影表示时，这是无法避免的效果，但是当数据以虚拟地球表示时，失真会导致极点中出现冗余信息，需要客户端在渲染过程中消除这些冗余信息。失真补偿最好在服务器端完成。\n解决方案包括减少高纬度行中的瓦片数量（matrixWidth），并在纵向维度上生成具有压缩比例的瓦片（见图 8）。为了实现此解决方案，必须扩展瓦片模型以指定合并系数（c），通过聚合 c 个水平瓦片但保持瓦片宽度（和瓦片高度）来减少宽度方向上的瓦片数量。合并系数不适用于赤道附近，但适用于中高纬度地区（纬度越高，系数越大）。\n即使瓦片可以合并，也不会改变索引或瓦片矩阵集，它们将与未应用合并时相同。例如，如果 c 系数为 4，则第一个瓦片的 tileCol 将为 0，第二个瓦片的 tileCol 将为 4，第三个瓦片的 tileCol 将为 8，依此类推。换句话说，对于同一个示例，tileCol 0、1、2 和 3 指向同一个瓦片。\n注意：此解决方案是必要的，以便仍然能够根据瓦片索引在空间中定义一个矩形。（即不脱离TileMatrixSet架构）\n💡 可变宽矩阵，在此更多先做介绍。我目前所接触到的二维场景中，还没有见过可变宽矩阵的使用。当然，也可能是人家做了支持，但是我没有发现， 🫣\nTileMatrixSet Requirements Classes（模型定义） 💡 e：在某些需要高精度的情况下，需要使用相同CRS的精确实现，或者CRS是动态的（随时间略有变化）并且需要伴随一个epoch。对于此数据结构，Tile MatrixSet 由通用 CRS 名称定义。具体瓦片集使用的 CRS 实现和纪元在瓦片集元数据中指定。在大多数情况下，瓦片集共享相同的通用 CRS 重叠，但对于某些高精度应用程序和非常细粒度的比例，客户端可以执行运行时校正以根据该信息准确地覆盖瓦片集，或者拒绝重叠瓦片集具有相同的 CRS 实现或纪元。\nf：该元素并不是要覆盖 CRS 轴顺序，而是通过重复 CRS 定义中已包含的信息使其对开发人员可见。\n瓦片的像元大小（cell size）可以通过将scaleDenominator乘以0.28 x 10^(- 3)/metesPerUnit 来获得。如果 CRS 使用米作为水平尺寸的测量单位，则 metesPerUnit = 1 ；如果使用度数，则metersPerUnit = 2 * pi * a / 360（a是地球椭球体的最大半径；也称为赤道半径）。\nCell Size（像元大小）：即一个像元表示多少米，也可以看作为一个像元表达的地图单位。\n这里只贴我认为比较重要的信息，如果你想要查看更多请查看原文。\nTileSet Metadata 瓦片由tileMatrix id、tileRow 编号和tileCol 编号来标识。这三个元素仅在与 TileMatrixSet 描述相关联时才有意义，该描述包含将索引转换为已知 CRS 中的坐标所需的信息（以scaleDenominator、cellSize、pointOfOrigin 和cornerOfOrigin 的形式）。 TileSet Metadata 的主要目的是将 TileSet 与 TileMatrixSet 描述链接起来。此外，该模型还包含描述 TileSet 主要特征的元素，保留从 TileSet 到原始数据集合和样式的连接以及开始导航的推荐中心点。\n💡 其实，我认为这个和TileJson的目的一致，即给出TileSet的描述与限制。\nTileMatrixSet limits 想象一种情况，瓦片集覆盖瓦片矩阵集的整个边界框。现在，想象一下瓦片集范围需要扩展到每个瓦片矩阵的原点和原点之外。更改原点会更改任何瓦片行索引和瓦片列索引的位置。换句话说，在新的瓦片集中，覆盖与前一个瓦片集相同的边界框的瓦片接收不同的瓦片行索引和瓦片列索引。这会使客户端可能已存储的任何缓存切片失效，并且所有客户端副本都需要更新。为了克服这个问题，数据集可以选择使用更通用的 TileMatrixSet 来覆盖更大的区域（甚至整个地球，例如附录 D 中定义的区域之一）。事实上，定义未来可能被数据集覆盖的区域的 TileMatrixSet 可以轻松地重新用于许多其他数据集，并成为通用的 TileMatrixSet。\n为了通知客户端有关tileset中tile索引的有效范围，引入了TileMatrixSetLimits概念。 TileMatrixSetLimits 列表告知包含实际数据的每个 TileMatrix 的这些索引的最小和最大限制。这些限制之外的区域被视为空白区域，并且不被瓦片集覆盖。\n💡 其实这里就解释了GeoWebCache中GridSet和GridSubset的关系，同时这就对应了GeoServer中404的响应，GeoServer通过此法可以避免大量无效的请求打到数据库（缓存或数据源），也在一定程度上提高了系统整体的安全性。但是此法有个缺点：就是需要提前确定数据源的范围（BoundingBox，GeoServer会读取数据的MBR范围并缓存）。当然，我们可以手动的设置该范围，而不是每次都从数据库进行获取，或者是周期性的更新该范围。总而言之，我觉得利是大于弊的，可以通过一些其他的手段进行优化。\nTileSetMetadata requirements class 💡 🤔，其实我感觉有些过于复杂了。在当下的环境中，我认为可以结合TileJson规范实现，并进行必要的简化。\n小结 上篇主要是对 **OGC Two Dimensional Tile Matrix Set and Tile Set Metadata** 一文中重点内容的翻译，并附上部分注解。不过个人理解有限，如有存在偏颇的方法，还希望大家多指教 😬 🤝！\n💡 文中许多部分附上原文（英文），是担心翻译不一定能够表达原文的意思，所以保留原文。但如果全部都附上原文将会导致篇幅激增，所以仅在较为简短且相对重要的地方附上原文。\nps：附录也是重点内容，不要错过哦🙄\n附录 ANNEX D (INFORMATIVE) COMMON TILEMATRIXSET DEFINITIONS 本附件包括一些常用的 TileMatrixSet 定义。\nWebMercator [web墨卡托投影] 人们可以定义任意数量的缩放级别，并且不需要包括此处定义的所有缩放级别。这里，示出了25个缩放级别。 墨卡托投影越靠近两极，单元尺寸就会变形。此处提供的像元大小仅在靠近赤道的东西方向上有效。 EPSG CRS 代码 3857 是 Web Mercator 的官方代码。最初分配了一个非官方代码“900913”（GOOGLE 用数字拼写），有时仍在使用。 该瓦片矩阵集是大众市场中最常用的瓦片矩阵集：例如，Google 地图、Microsoft Bing 地图和 OpenStreetMap。然而，它长期以来一直受到批评，因为它是基于球形墨卡托而不是椭球体。 WebMercatorQuad 的使用应仅限于可视化。任何附加用途（包括距离测量、路由等）都需要首先使用墨卡托球面表达式将坐标转换为适当的 CRS。\n💡 其实目前很多人都注意到该问题了（且应该注意到），一般的做法是，使用3857进行可视化，使用4326（4490）进行分析计算。数据的存储上也是存两份（也可能只存一份），也就是3857和4326（4490）。这样的存储还有一个比较好的地方，也就是可视化不再需要动态投影。毕竟动态投影相对而言还是比较慢的，算是性能损耗点，此方式也是空间换时间了。\nps：无谓好坏，只有适合！ 😉\n💡 注：EPSG 数据库版本 8.9 关于 3857 的描述是：“使用椭球坐标的球面展开。相对于WGS 84 / World Mercator (CRS code 3395)，地图上可能会出现 0.7% 的比例误差和北向差异达 43 公里（相当于地面 21 公里）。”\n所以使用 Web 墨卡托可能生成错误的地理空间定位信息，对全球导航活动安全以及需要精确定位和导航的国防部、情报界和盟军合作伙伴系统、任务和操作构成不可接受的风险信息。”建议使用 WorldMercatorWGS84Quad（因为其基于椭球体墨卡托投影） 🫣\nWorldCRS84Quad/Variant 1: World CRS84 Quad (recommended) 此 TileMatrixSet 中的缩放级别标识符不对应于 WMTS 1.0 的附件 E.3 中的相同比例值。在这个TileMatrixSet中，标识符为“-1”的TileMatrix只有一个图块，有128行留空。因此，许多实施者不想提供这一级别（包括 INSPIRE 技术指南），而是更喜欢从仅用 2 个瓦片（一个用于负经度，一个用于正经度）表示世界的 TileMatrix 开始。\n此 TileMatrixSet 和 WorldMercatorWGS84Quad 和 WebMercatorQuad 的比例分母相同，但标识符被替换为 1（本例中为0）。这可能会产生混乱。【因为相对WorldMercatorWGS84Quad 和 WebMercatorQuad ，本例少了一层。】\n💡 上图中的id应该是0才对，从定义上0级是2x1。同时，相对WorldMercatorWGS84Quad、 WebMercatorQuad 以及我国的CGCS2000经纬度切片方案，本例少了一层（也就是0级是1x1的状态）。\nWorldCRS84Quad/Variant 2: World EPSG:4326 Quad 尽管第 6.2.1.1 条中有规定，一些实现者更喜欢使用 CRS http://www.opengis.net/def/crs/EPSG/0/4326 定义以前的 TileMatrixSet。 该定义与使用 http://www.opengis.net/def/crs/OGC/1.3/CRS84 定义的变体相同，只是 CRS 坐标（EPSG:4326）以纬度、经度顺序表示（CRS84是经度、纬度顺序的表示），仅影响 PointOfOrigin 和 BoundingBox 编码。 对于大多数实际目的，这两种变体是等效的，因为 TileMatrixSet 主要定义瓦片平铺结构以及每个瓦片矩阵的比例/分辨率，而不是每个平铺内的数据的存储方式。\n对于许多栅格和矢量切片格式，CRS84 和 EPSG:4326 是等效的，因为强制执行特定的轴顺序。 例如，API 的附加参数还可以通过将 CRS 指定为 CRS84 或 EPSG:4326 来覆盖默认轴顺序。\nps：其实这两种都是 基于WGS84经纬度投影的切片方案，唯一的差异就是轴顺序的问题。国内使用肯定是选择经度优先的使用，也就是经度、纬度顺序的表示。\nps：在GeoTools中可以如此使用以引入CRS84：CRS.decode(\u0026ldquo;CRS:84\u0026rdquo;); GEOGCS[\u0026ldquo;WGS84\u0026rdquo;,\nDATUM[\u0026ldquo;WGS84\u0026rdquo;,\nSPHEROID[\u0026ldquo;WGS84\u0026rdquo;, 6378137.0, 298.257223563]],\nPRIMEM[\u0026ldquo;Greenwich\u0026rdquo;, 0.0],\nUNIT[\u0026ldquo;degree\u0026rdquo;, 0.017453292519943295],\nAXIS[\u0026ldquo;Geodetic longitude\u0026rdquo;, EAST],\nAXIS[\u0026ldquo;Geodetic latitude\u0026rdquo;, NORTH],\nAUTHORITY[\u0026ldquo;Web Map Service CRS\u0026rdquo;,\u0026ldquo;84\u0026rdquo;]]\n当然，你也可以这样引入：CRS.decode(\u0026ldquo;EPSG:4326\u0026rdquo;, true); 可以指定轴顺序。或者是在全局指定轴顺序：System.setProperty(\u0026ldquo;org.geotools.referencing.forceXY\u0026rdquo;, \u0026ldquo;true\u0026rdquo;); 参考：GeoTools Axis Order\n更多信息请参考GeoTools，且应以官方所述为主。\nWorldMercatorWGS84Quad 此瓦片矩阵集看起来与前一个（Web Mercator Quad）类似，但该矩阵基于椭圆体墨卡托。请注意，此覆盖的最北纬度是 85.08405903（与 Web Mercator 不同）。\n💡 美国国防部 (DoD) 下属的国家地理空间情报局 (NGA) 测绘办公室提醒公众在所有关键任务活动中使用国防部批准的 1984 年世界大地测量系统 (WGS 84) 应用程序，并鼓励使用像这样的（WorldMercatorWGS84Quad）基于 WGS84 的切片矩阵集，并阻止使用基于 Web 墨卡托的 Web 墨卡托瓦片矩阵集，例如 WebMercatorQuad。\nNGA 测绘办公室建议使用通用缩放级别比例集，该比例集定义为纬度 +-31.0606963703645 处的真实像元大小，这意味着赤道处的比例缩小了 0.857385503731176。为方便起见，本标准建议在赤道处使用比例分母。\nANNEX I (INFORMATIVE) PSEUDOCODE 本信息附件提供了伪代码，说明如何获取覆盖边界框矩形的瓦片以及如何获取绑定瓦片的 CRS 坐标。（也就是瓦片坐标系与瓦片对应的空间范围CRS坐标的相互转换）\n从 BBOX 到瓦片索引 以下伪代码片段可用于将 CRS 坐标中的所需边界框 (bBoxMinX、bBoxMinY、bBoxMaxX、bBoxMaxY) 转换为一系列瓦片集索引。此伪代码使用与子条款 6.1.1 相同的符号。在此伪代码中，假设 bBoxMinX、bBoxMinY、bBoxMaxX、bBoxMaxY、tileMatrixMinX、tileMatrixMinY、tileMatrixMinY、tileMatrixMaxY、tileSpanX 和 tileSpanY 是浮点变量 (IEEE-754)，其精度问题源于表示的有限精度。这些精度问题可能会在典型的 floor() 向下舍入函数中被放大，该函数可能返回比预期值 ±1 的值。为了解决这个问题，此代码使用在 CRS 坐标精度不受影响的地方添加或减去一个小值 (epsilon)。\n要获取覆盖此边界框的所有瓦片，客户端将扫描从tileMinCol到tileMaxCol以及从tileMinRow到tileMaxRow（全部包含在内）。总共将获取 (tileMaxCol -tileMinCol + 1) x (tileMaxRow -tileMinRow + 1)。\n从瓦片索引到 BBOX 以下伪代码可用于将一对瓦片索引（tileCol、tileRow）转换为由瓦片左上角（leftX、upperY）定义的此瓦片的边界框（以 CRS 坐标表示）：\n参考 OGC Two Dimensional Tile Matrix Set and Tile Set Metadata Esri 支持 GIS 字典 矢量金字塔技术研究 ","permalink":"https://fuyi-atlas.github.io/posts/gis/what-is-tilematrixset/","summary":"前言 其实，在此前矢量金字塔技术研究一文中已经大致提及了瓦片金字塔与TileMatrixSet的关系，为什么还要在这里再次说明呢？主要是前文更侧重于矢量金字塔的概念定义，对于TileMatrixSet的描述过少，所以才会再次对TileMatrixSet进行说明。本文将基于 OGC Two Dimensional Tile Matrix Set 标准对TileMatrixSet进行研究探讨，将侧重于说明其基本构成要素的定义以及整体的技术实现。而GeoWebCache中的GridSet作为典型实现，本文将会对其进行再次说明，以印证OGC 关于TileMatrixSet的定义。由于内容较多，本文将分为下上两个篇章进行描述。上篇是对TileMatrixSet以及其构成要素的基本概念进行描述，而下篇则是对GeoWebCache中的GridSet实现进行研究，同时结合我国CGCS2000切片方案对TileMatrixSet格网划分计算原理进行探讨，另附带说明Geo Atlas中关于此模块的实现细节。\n本章节为上篇：TileMatrixSet及其构成要素的基本概念。\n什么是TileMatrixSet？ TileMatrixSet，即Tile Matrix Set，源自 OGC Two Dimensional Tile Matrix Set(目前已更新到v2)。该标准定义了瓦片矩阵集的规则和要求，作为一种基于一组规则网格对空间进行索引的方式，这些规则网格为坐标参考系统中有限比例列表定义了一个域（瓦片矩阵）。\n💡 The OGC Two Dimensional Tile Matrix Set and Tile Set Metadata Standard defines the rules and requirements for a tile matrix set as a way to index space based on a set of regular grids defining a domain (tile matrix) for a limited list of scales in a Coordinate Reference System.","title":"再谈TileMatrixSet，二维瓦片金字塔结构的标准定义（上）"},{"content":"Geo Atlas 是什么？ Geo Atlas，译为地理地图册(地理地图集)，就像小时候买到的纸质的地理地图册书本，里面填充着各式各样的地图。所以, 我也希望有那么一个东西，同样可以对外提供各种各样的地图以供使用。\n目前来说，他还只是一个基于Java的开发的，可用于快速构建矢量切片服务的基础库。\n本例中基于Geo Atlas实现了一个精简版本的矢量切片服务，从结果来看，可以将其看作为仅实现了矢量切片功能的GeoServer的精简提升版本。\n背景与动机 首先，我是一个主做服务端开发的GIS开发工程师，平时接触最多的就是管网地图服务发布。此前的工作中没有使用任何平台技术，比如：ArcGIS、超图等，而是使用开源技术。技术栈大体可以总结为：SpringBoot + GeoServer + Mapbox + 空间数据库。\n其次，目前在业务应用上，二维地图还是主流。而二维地图技术里面，属Mapbox Vector Tile的体验最好。所以，目前的技术路线是通过 GeoServer 来发布Vector Tile。\n然后，在 GeoServer 使用中，我发现了这个几个问题：\n受限于其开源协议（GPL 2.0）的约束，你无法通过修改部分源代码的方式，将其直接集成到系统内部。只能是单独部署，通过其提供的REST接口进行交互。基于此，部署方式同样受限。 GeoServer 是一个大而全的东西，同时他是一个单体项目。也就是说，哪怕我只需要提供MWTS+MVT服务，我也需要部署一个全功能的节点，无法按需使用（这里不得不提一下GeoServer Cloud项目，但是它提出得要求更多了，需要一套微服务的环境） 从公谨《WebGIS数据不切片或是时代必然》一文推论，GeoServer中提供的MVT技术（指数据切片过程）已经算是切片起源时代的产物了，而今已经跨过了矢量切片时代（数据还是预切），进入了动态矢量切片时代了 GeoServer中Vector Tile与GeoWebcache中的Tile Meta技术有冲突，瓦片清理存在BUG。可以理解为，GeoServer对于矢量瓦片的支持并不是很好。 不能很好的处理瓦片缓存与动态业务数据的矛盾，即使GeoWebcache提供缓存清理的策略。（Layer中Boundingbox范围问题） GeoServer完全是栅格金字塔技术的实现，不过是将栅格金字塔技术同时应用在栅格数据和矢量数据上，同时他并没有很好的针对不同层级对数据进行抽稀简化（其尝试从服务端配图SLD中获取数据分级规则，我没有测试过，但反对，理由参见: 关于矢量瓦片技术支持前端渲染带来的思考），那么就出现了一个pbf有近20M的情况。同时也回答了为什么现在大家都直接在数据库层面实现矢量瓦片（或者说是在数据源中），一是可以无视数据传递的时间损耗；二是可以直接做抽稀简化，这样出去的数据少了，传输速度自然也快了；三是数据库空间支持已经很成熟了；四是门槛高啊（护城河）\n最后，我也想对我目前的状态做一次总结。那么，Geo Atlas应该就是我最好的总结方式。因为它既可以丰富我的简历🫣，又可以帮助我绘制技能树，完成这一次的总结。\nps: 当然，还有当下信创的背景原因。就当，抛砖引玉了😧，哈哈哈😬\n特性 遵循 OGC Two Dimensional Tile Matrix Set and Tile Set Metadata Standard 2.0 [并不完全遵循] 尝试遵循 NEW OGC API 提供矢量切片能力 支持自定义数据属性分级规则 支持Google瓦片坐标系(原点在左上角, 默认即为Google瓦片坐标系) 支持3857(900913), 4490投影(即默认提供相应的TileMatrixSet) 支持自定义坐标系及自定义坐标转换行为(源数据坐标系) 支持自定义数据范围(OGC TileMatrixSet Limits，拒绝范围外请求) 提供全局统一的，可快速集成的瓦片缓存组件， 支持基于内存和文件系统的缓存 支持使用GeoPackage进行缓存 支持Seed, Reseed, Truncate三种瓦片缓存处理策略 提供Namespace, Datastore, FeatureLayer元数据管理模块，并提供一个可视化操作界面(Geo Atlas Dashboard) 提供栅格数据切片能力 提供地形数据切片能力 提供按需快速集成能力(将常用功能封装为各种stater) 截图 快速开始 以下说明旨在基于Docker技术快速搭建一个矢量切片服务示例。\nGeospatial Data Source(With some data) 💡 提供下载的是矢量数据，不是最终地图，与符号化后的地图再可视化表达上存在一定差异。用户利用此数据编制地图，应当严格执行《地图管理条例》有关规定；编制的地图如需向社会公开的，还应当依法履行地图审核程序。\n💡 数据仅供学习研究使用\nTiles API Service(Backend)\nGeo Atlas Dashboard(Frontend)\n请确保你已经安装好了Java, Maven, Docker以及Docker Compose。 我测试使用Wsl2(Windows11) + Docker Desktop(4.30.0) + Apache Maven 3.8.7 + Oracle jdk 11.0.20\n克隆代码\ngit clone --recursive https://github.com/geoatlas-cloud/geo-atlas.git cd geo-atlas/ 配置环境变量\ncp .env.production.local.template .env.production.local 然后手动修改配置文件, 将其中的配置项修改为你自己的配置，如:\nHOST_IP：宿主机IP POSTGRES_PASSWORD：PostgreSQL数据库初始化密码 JASYPT_ENCRYPTOR_PASSWORD：用于加密数据库账户信息的密钥 CACHE_ENABLED：是否开启缓存 NEXT_PUBLIC_BASE_MAP_TYPE：地图类型, osm|tianditu NEXT_PUBLIC_BASE_MAP_TILE_KEY：当使用天地图时需要填写key，4490经纬度投影默认使用天地图，如果需要进行4490经纬度投影预览还请填写天地图Key 执行构建脚本, 拉起服务\nchmod +x ./build2run.sh ./build2run.sh 等待服务启动完成后访问: http://localhost:11003, 而后按照GeoServer的使用习惯, 逐步创建\nnamespace datastore feature layer 可通过预览的方式检查瓦片服务是否正常 默认给出的数据为我国的境界与政区数据, 来自省市县数据CTAmap, 源自1：100万公众版基础地理信息数据（2021） 其实我也曾提取过境界与政区数据(全国1:100万基础地理信息数据-境界与政区提取), 不过与上述数据相比而言比较粗糙, 后由于时间关系没有进行细化, 所以没有使用\n在提供境界与政区数据的同时, 还支持切换为OSM China的数据。 OSM-China数据的处理过程大致为: 将源数据通过Osm2pgsql入库, 而后使用pg_dump制作转储文件, 并基于此转储文件制作PostGIS镜像, 在容器初始化的时候会自动恢复数据。 但是转储后的文件比较大, 导致镜像也比较大, 同时数据比较多导致恢复的时候比较慢。如果将其作为示例中的数据源的话, 那么三个服务全部启动完成耗时估计得有5分钟了, 所以并未将其作为默认得数据源。\n如果你想要使用OSM的数据测试, 可以将其作为额外的数据源进行连接, 这样就不会影响示例应用的初步体验了\n指南 点击图片跳转B站\n技术概览 基于GeoTools、GeoWebCache进行Geo Atlas构建，基于Spring Boot Framework对外提供快速集成能力。\nGeoTools提供矢量数据读取以及坐标转换能力 在此对GeoWebCache进行了拆分为两个部分：分别是金字塔(pyramid, 提供瓦片索引与瓦片生成能力)与瓦片缓存(tile-cache, 提供瓦片缓存能力) Mapbox Vector Tile Generator 由 java-vector-tile 提供支持\n系统架构概述 下图描述了系统的总体架构。\n此处更多的表达了内部结构与层次关系，无关部署\n组件概述 APP\nGeo Atlas Dashboard: 可视化操作界面 Tiles API Application: 提供矢量切片服务的应用程序(同时支持Dashboard) Boot\nTiles API Boot Stater: 对Tiles API的快速集成封装, 约定大于配置 Component\nMetadata Mgmt: 提供Namespace, Datastore, FeatureLayer元数据管理模块 Tile Cache: 提供瓦片缓存能力, 目前支持基于内存, 文件系统两种缓存方式, 可任意组合 OGC APIS: 提供OGC APIs, 目前仅支持Tiles API Library\nBase References: 基础依赖声明 Pyramid Model: 金字塔模型, 用于构建瓦片金字塔结构索引, 同时提供切片管道 Tile Generator: 瓦片生成器, 目前仅支持Mapbox Vector Tile Generator External Data Sources\nConfig Storage: 用于存储Namespace, Datastore, FeatureLayer等元数据 Geospatial Data Source: 用于存储矢量数据, 如PostGIS、SQLServer 构建 克隆代码\ngit clone --recursive https://github.com/geoatlas-cloud/geo-atlas.git cd geo-atlas/ 要构建应用程序，请从根项目目录运行以下命令, 或者使用IDEA的Maven插件\nmvn clean install -DskipTests 运行调试 配置环境变量, 默认使用dev环境(当然，你可以直接修改dev.yml文件, 而不是通过环境变量控制)\n自行用IDEA打开项目，然后运行Application类。\n支持 thread.zhou@gmail.com thread_zhou@126.com 声明与致谢 Geo Atlas现版本参照GeoServer应用模式构建，并参考了GeoServer以及GeoServer Cloud的实现方式 Pyramid、IO(in Library)、Tile Cache模块均来自GeoWebCache, 是对其进行了拆解和少量变更 矢量数据的读取与坐标转换使用GeoTools Mapbox Vector Tile Generator由 java-vector-tile 提供支持 ","permalink":"https://fuyi-atlas.github.io/posts/program/geo-atlas/001/","summary":"Geo Atlas 是什么？ Geo Atlas，译为地理地图册(地理地图集)，就像小时候买到的纸质的地理地图册书本，里面填充着各式各样的地图。所以, 我也希望有那么一个东西，同样可以对外提供各种各样的地图以供使用。\n目前来说，他还只是一个基于Java的开发的，可用于快速构建矢量切片服务的基础库。\n本例中基于Geo Atlas实现了一个精简版本的矢量切片服务，从结果来看，可以将其看作为仅实现了矢量切片功能的GeoServer的精简提升版本。\n背景与动机 首先，我是一个主做服务端开发的GIS开发工程师，平时接触最多的就是管网地图服务发布。此前的工作中没有使用任何平台技术，比如：ArcGIS、超图等，而是使用开源技术。技术栈大体可以总结为：SpringBoot + GeoServer + Mapbox + 空间数据库。\n其次，目前在业务应用上，二维地图还是主流。而二维地图技术里面，属Mapbox Vector Tile的体验最好。所以，目前的技术路线是通过 GeoServer 来发布Vector Tile。\n然后，在 GeoServer 使用中，我发现了这个几个问题：\n受限于其开源协议（GPL 2.0）的约束，你无法通过修改部分源代码的方式，将其直接集成到系统内部。只能是单独部署，通过其提供的REST接口进行交互。基于此，部署方式同样受限。 GeoServer 是一个大而全的东西，同时他是一个单体项目。也就是说，哪怕我只需要提供MWTS+MVT服务，我也需要部署一个全功能的节点，无法按需使用（这里不得不提一下GeoServer Cloud项目，但是它提出得要求更多了，需要一套微服务的环境） 从公谨《WebGIS数据不切片或是时代必然》一文推论，GeoServer中提供的MVT技术（指数据切片过程）已经算是切片起源时代的产物了，而今已经跨过了矢量切片时代（数据还是预切），进入了动态矢量切片时代了 GeoServer中Vector Tile与GeoWebcache中的Tile Meta技术有冲突，瓦片清理存在BUG。可以理解为，GeoServer对于矢量瓦片的支持并不是很好。 不能很好的处理瓦片缓存与动态业务数据的矛盾，即使GeoWebcache提供缓存清理的策略。（Layer中Boundingbox范围问题） GeoServer完全是栅格金字塔技术的实现，不过是将栅格金字塔技术同时应用在栅格数据和矢量数据上，同时他并没有很好的针对不同层级对数据进行抽稀简化（其尝试从服务端配图SLD中获取数据分级规则，我没有测试过，但反对，理由参见: 关于矢量瓦片技术支持前端渲染带来的思考），那么就出现了一个pbf有近20M的情况。同时也回答了为什么现在大家都直接在数据库层面实现矢量瓦片（或者说是在数据源中），一是可以无视数据传递的时间损耗；二是可以直接做抽稀简化，这样出去的数据少了，传输速度自然也快了；三是数据库空间支持已经很成熟了；四是门槛高啊（护城河）\n最后，我也想对我目前的状态做一次总结。那么，Geo Atlas应该就是我最好的总结方式。因为它既可以丰富我的简历🫣，又可以帮助我绘制技能树，完成这一次的总结。\nps: 当然，还有当下信创的背景原因。就当，抛砖引玉了😧，哈哈哈😬\n特性 遵循 OGC Two Dimensional Tile Matrix Set and Tile Set Metadata Standard 2.0 [并不完全遵循] 尝试遵循 NEW OGC API 提供矢量切片能力 支持自定义数据属性分级规则 支持Google瓦片坐标系(原点在左上角, 默认即为Google瓦片坐标系) 支持3857(900913), 4490投影(即默认提供相应的TileMatrixSet) 支持自定义坐标系及自定义坐标转换行为(源数据坐标系) 支持自定义数据范围(OGC TileMatrixSet Limits，拒绝范围外请求) 提供全局统一的，可快速集成的瓦片缓存组件， 支持基于内存和文件系统的缓存 支持使用GeoPackage进行缓存 支持Seed, Reseed, Truncate三种瓦片缓存处理策略 提供Namespace, Datastore, FeatureLayer元数据管理模块，并提供一个可视化操作界面(Geo Atlas Dashboard) 提供栅格数据切片能力 提供地形数据切片能力 提供按需快速集成能力(将常用功能封装为各种stater) 截图 快速开始 以下说明旨在基于Docker技术快速搭建一个矢量切片服务示例。","title":"Geo Atlas，用于构建矢量切片服务的Java基础库"},{"content":"前言 在图像切片时代，多层次模型依靠的是影响金字塔。得益于影像栅格数据分辨率的特点，基于影像金字塔可以较好的实现多分辨率模型。但是在矢量切片时代中，就无法直接从影像金字塔技术获利了，因为矢量数据不具有分辨率这个特性，而是采用矢量金字塔技术来实现多层次、多尺度模型。\n影像金字塔（分层） 影像金字塔技术通过影像重采样方法，建立一系列不同分辨率的影像图层，每个图层分割存储，并建立相应的空间索引机制，从而提高缩放浏览影像时的显示速度。如下图所示的影像金字塔，底部是影像的原始最高分辨率的表示，为512×512图像分辨率，越往上的影像的分辨率越小，分别为256×256，128×128，顶部是影像金字塔的最低分辨率的图像64×64，因此这个影像金字塔共有4层，即4个等级的分辨率。显然影像的图像分辨率越高，影像金字塔的等级越多。\n从给出的定义与图示来看，好像与我们目前使用的瓦片地图有一定的差别。是的，这是因为影像金字塔负责的内容仅仅是构建多分辨率层次，也就是每一层都是对应完整数据范围的一块数据，也就是我们常说的分层（栅格数据是均衡的，可以通过分辨率来作为尺度描述，所以多分辨率层级也就对应着多尺度层级）。\n瓦片金字塔（分块） 节选自《高性能影像数据瓦片化关键技术研究-刘世永-2016》\n要实现现如今使用的瓦片地图模型，还需要瓦片金字塔配合完成。\n瓦片金字塔模型是当前应用最广的多层次地图数据组织模型，通过瓦片金字塔模型，前端在进行放大和缩小操作时，可以有效地减少数据读取的空间查询时间。通过只加载可视区域范围内的瓦片，可以减少数据加载量，降低网络传输压力，提高前端的数据可视化速度。\n也就是说，瓦片金字塔实在影像金字塔的基础上，基于特定规格大小对每层影像进行切割。此操作也就是对应着我们常说的分块。\n数据原始分辨率并不是标准化的，分层结果即不够标准化，也不够细致。所以在此基础上再次分片，既是减少了数据量使得传输与加载效率提升，同时也是给出了标准，兼容性更好。\n瓦片金字塔的主要原理为：基于某个特定的地图投影坐标系（常规是Web墨卡托），将曲面的地球投影到二维平面，而后将该二维平面进行多尺度地划分，即相当于制作了多个不同分辨率层级的数字地图。各层级对应相应编码，层级越高地图所对应的分辨率越高；而后对每一层级的全球空间范围地图按照某种空间划分方法进行格网划分，划分成若干行和列的固定尺寸的正方形栅格图片，这些切分出来的规整的单个格网单元称为瓦片，各层级的划分方法都是相同的。\n瓦片划分方法需满足以下条件：\n每个层级下的所有瓦片可以无缝拼合成一张全球空间范围的世界地图 每个瓦片都有唯一编码，根据编码可以解算该瓦片对应的空间范围 在某一层级下给定一坐标点可以根据其空间坐标解算其所在瓦片的编号 每一层级瓦片对应一层金字塔分层，所有层级的瓦片便构成了整个瓦片金字塔模型。每一层中的瓦片划分方法一般采用均匀四分的划分方法，即以赤道和中央经线的交点为初始中心，不断地对地图进行四分，直到每个格网的大小为tilesize * tilesize为止，其中tilesize表示单个瓦片的边长。基于此种划分方法，第0层金字塔（金字塔顶层）用一个瓦片就能表示整张世界地图，第1层要用4^z个瓦片来表示整个世界地图，z为当前瓦片的金字塔层级。\n瓦片投影坐标系 瓦片金字塔模型中的投影坐标系可以有多种，目前最广泛采用的是Web Mercator投影，它是Mercator投影的一种变体。\n瓦片坐标系 所有瓦片的编码都是基于瓦片坐标系下进行的，瓦片坐标系的原点一般都在左上角或者左下角，TMS规范中是在左下角（GeoWebCache遵循该规范），但是现有的Google、Mapnik切片系统都是选用左上角作为原点，本文主要以原点在左上角的瓦片坐标系进行说明。 瓦片的编码方式如下图所示，层级用z表示，瓦片经线方向（指瓦片经度发生变化的方法，即东西向，东向为正）上编号为x，纬线方向（指瓦片维度发生变化的方向，即南北向，南向为正）上编号为y，因此每一个瓦片都可以通过一个三维元组（x,y,z）来唯一描述。\n总的来说，如今我们所说的影像（栅格）金字塔大多指代的是影像金字塔与瓦片金字塔的结合体或是瓦片金字塔（默认含有分层），而不是单独指代分层或者单独指代分块。\n矢量金字塔 影像金字塔是为栅格数据服务的，也是图像切片时代的核心产物。但是到了矢量切片时代后，由于矢量数据并不具备分辨率的特性，且矢量数据不同于栅格数据，它有着疏密不一致、分布不均与的特点，所以无法直接利用影像金字塔技术。不过瓦片金字塔是基于分层金字塔的基础上构建的，所以对数据类型并没有要求，在矢量切片中是可以直接复用的。总而言之，在矢量切片时代中，需要一个符合矢量数据特点的分层模型，作为矢量数据源与瓦片金字塔之间沟通的桥梁。同时考虑到应用上的兼容性，所以最终基于金字塔理论之上进行分层模型的定义，谓之：矢量金字塔。\n对于矢量数据的分层，将使用比例尺作为尺度描述，建立一系列不同比例尺的分层。不同于栅格金字塔的多分辨率层级，矢量数据金字塔没有分辨率的概念，但是不同层级之间的数据详尽程度也是不同的。随着比例尺的由小到大，矢量要素也变得越来越详细；而随着比例尺由大到小，矢量要素也将变得精简与概化，以符合人们的使用要求。\n总而言之，矢量金字塔的的目的就是解决在小比例尺下大数据量（或高密度区域）矢量数据聚集度高、要素重叠和显示速度慢的问题。（其实就是矢量数据的制图综合问题，矢量金字塔不过是其中的一个解而已）\n注：我们此处所谈矢量金字塔，只是矢量数据分层金字塔，只是分层。\n矢量分层 为了达成人们的使用要求，则需要对数据进行处理，以符合给定比例尺级别下保持相应的详尽程度。\n与栅格数据不同，矢量数据通常都具有空间特征与属性信息。空间特征体现在数据的空间坐标系、空间分布以及几何特征；属性信息则是数据实体相关的一些信息。那么矢量数据的空间特征与属性信息则可作为分层的依据，首要对数据的属性信息进行处理，而后再基于空间特征进行处理。\n属性分类分级 对于数据的属性特征处理主要是对属性信息进行分类和分级两种情况。分类是根据属性信息划分类别；分级即根据属性信息按照其重要程度划分不同的级别，并且赋予不同的权重值。\n空间特征处理 基于空间特征的处理则就多种多样了，比如可以基于数据的分布（密度）抽稀、基于周长或面积进行选取、基于几何形状进行简化。在处理的同时，还需要是实际情况考虑是否维持数据的拓扑关系。\n总而言之，可将上述的分层处理方法抽象为两种：\n选取（Filter）：属性分类分级，基于密度、周长、面积的处理都属于选取 简化（Simplify）：基于空间几何形状的处理属于简化 当然，上述两种只是最基础的分层处理方法，后续还可以有更多处理方法，比如合并、融合、夸大等。但我想来，如果能够比较合理的完成上述两个操作，应该也是达到了基本可用层次。\n不同行业的数据具有着不同的重点或侧重点，分层不仅需要结合矢量数据模型的特点，还需要结合行业背景与应用场景综合考虑。\n此处再借用公谨（遥想公瑾当年）的一句描述进行佐证：\n所谓的矢量金字塔模型，即基于制图综合的知识，分别设置海量数据在不同zoom下是否显示，是否简化，是否融合的一种策略，当动态提取切片时，根据这个策略选择数据，实际捞取的数据就非常少，有效解决了矢量切片不能解决数据太密集集中的问题。\n矢量金字塔 → 矢量数据的多尺度表达 → 矢量数据的自动制图综合（保证综合前后要素内部及要素之间的拓扑关系是矢量地图正确显示的基本需求 😯）\n注：此处所述分层，并不是比例尺等级。而是在不同比例尺层级下，矢量数据的详尽程度。而分层处理，即为通过一定的手段来控制数据的详尽程度。\n技术实现 在技术实现上，目前我看到的都是以瓦片金字塔结构为基础，叠加分层处理手段的方式实现的。因为瓦片金字塔是在分层金字塔的基础之上，而分层可分为两个部分：\n尺度分级定义 → 一系列的比例尺等级 分层分级处理（综合算子） → 一系列的处理算子（如：Filter、Simplify） OGC TileMatrixSet 定义 而尺度分级定义不论是在分层金字塔还是瓦片金字塔中都是一致的，也就是将分层分级处理（综合算子）剥离出来单独实现，在最终的瓦片生产流程中，接入瓦片金字塔即可。\nGeoWebCache 实现 瓦片金字塔结构 public class GridSet { private String name; // 投影坐标系 private SRS srs; // 瓦片宽, such as 256 private int tileWidth; // 瓦片高, such as 256 private int tileHeight; /** * Whether the y-coordinate of {@link #tileOrigin()} is at the top (true) or at the bottom * (false) */ protected boolean yBaseToggle = false; /** * By default the coordinates are {x,y}, this flag reverses the output for WMTS getcapabilities */ private boolean yCoordinateFirst = false; private boolean scaleWarning = false; // 将坐标参考系统 (CRS) 单位转换为米的系数 // 也就是说，这个参数表示的是给定的CRS中一个单元转换为米的系数。换句话说，也就是在指定的CRS中，一个单元表示多少米。 // 目前常用的就两种投影，一是以米为单位的（即metersPerUnit为1）；其次是以度为单位的经纬度投影（metersPerUnit表示为1度代表多少米，即：360/赤道周长，不同CRS使用不同的椭球体，所以其赤道周长也会存在一定差异。） private double metersPerUnit; // 像素大小, 通常给定0.28mm private double pixelSize; // 范围, 通常是投影坐标系的最大范围 private BoundingBox originalExtent; // 所有的金字塔层次集合 private Grid[] gridLevels; private String description; /** * {@code true} if the resolutions are preserved and the scaleDenominators calculated, {@code * false} if the resolutions are calculated based on the sacale denominators. */ private boolean resolutionsPreserved; } 单层金字塔模型实现 public class Grid { // 当前层横向上瓦片数量 private long numTilesWide; // 当前层纵向上瓦片数量 private long numTilesHigh; // 当前层的分辨率 private double resolution; // 当前层的比例尺分母 private double scaleDenom; private String name; } 现如今，不论是矢量还是栅格，在基于已有标准（比如电子地图数据规范中的地图分级）的情况下，已经给出了一个瓦片金字塔的框架结构，缺少的是金字塔每层的数据，也就是需要自行进行数据分层并放入给定的层级结构中。就拿GeoServer来说，默认情况下，他就直接套用了给定的瓦片金字塔层级结构，且仅做了几何形状的Simplify（无法处理密度问题），那么就会出现大数据量或高密度区域在小比例尺下瓦片尺寸很大，爆炸的大的情况 ☹️\n对于分层操作，一般情况下可以先对数据进行属性分类（如：河流\u0026amp;境区）和分级（如：一级河流、二级河流），再结合比例尺分层处理（综合算子集中实现）。 注：行业不同数据也不同，数据不同处理方式也不同，所以就没有标准的处理规则，但理论终归是相通的。\n感想 感觉GIS在应用上的知识还是比较封闭的，不像计算机相关的应用知识一般，很容易就能获取到，或者是获取的渠道很清晰。从我目前走过的道路来看，我获取这些知识的途径大概分为四种，收益从上到下：\n研究开源GIS应用软件源码 实际项目应用中的探索 理论书籍、论文 各位大佬的文章 不知道是不是我还没有获取到更正确的途径，总感觉比较封闭，新来的人不容易进入（像我已经工作接近6年了）。可能还是从业的人太少了 🙄\n参考 影像金字塔原理 《高性能影像数据瓦片化关键技术研究-刘世永-2016》 《矢量数据金字塔结构设计-董滨-2016》 WebGIS数据不切片或是时代必然 GeoWebCache Gridsets and Gridsubsets Tiles à la Google Maps Coordinates, Tile Bounds and Projection OGC Two Dimensional Tile Matrix Set and Tile Set Metadata ","permalink":"https://fuyi-atlas.github.io/posts/gis/vector-pyramid-technology/","summary":"前言 在图像切片时代，多层次模型依靠的是影响金字塔。得益于影像栅格数据分辨率的特点，基于影像金字塔可以较好的实现多分辨率模型。但是在矢量切片时代中，就无法直接从影像金字塔技术获利了，因为矢量数据不具有分辨率这个特性，而是采用矢量金字塔技术来实现多层次、多尺度模型。\n影像金字塔（分层） 影像金字塔技术通过影像重采样方法，建立一系列不同分辨率的影像图层，每个图层分割存储，并建立相应的空间索引机制，从而提高缩放浏览影像时的显示速度。如下图所示的影像金字塔，底部是影像的原始最高分辨率的表示，为512×512图像分辨率，越往上的影像的分辨率越小，分别为256×256，128×128，顶部是影像金字塔的最低分辨率的图像64×64，因此这个影像金字塔共有4层，即4个等级的分辨率。显然影像的图像分辨率越高，影像金字塔的等级越多。\n从给出的定义与图示来看，好像与我们目前使用的瓦片地图有一定的差别。是的，这是因为影像金字塔负责的内容仅仅是构建多分辨率层次，也就是每一层都是对应完整数据范围的一块数据，也就是我们常说的分层（栅格数据是均衡的，可以通过分辨率来作为尺度描述，所以多分辨率层级也就对应着多尺度层级）。\n瓦片金字塔（分块） 节选自《高性能影像数据瓦片化关键技术研究-刘世永-2016》\n要实现现如今使用的瓦片地图模型，还需要瓦片金字塔配合完成。\n瓦片金字塔模型是当前应用最广的多层次地图数据组织模型，通过瓦片金字塔模型，前端在进行放大和缩小操作时，可以有效地减少数据读取的空间查询时间。通过只加载可视区域范围内的瓦片，可以减少数据加载量，降低网络传输压力，提高前端的数据可视化速度。\n也就是说，瓦片金字塔实在影像金字塔的基础上，基于特定规格大小对每层影像进行切割。此操作也就是对应着我们常说的分块。\n数据原始分辨率并不是标准化的，分层结果即不够标准化，也不够细致。所以在此基础上再次分片，既是减少了数据量使得传输与加载效率提升，同时也是给出了标准，兼容性更好。\n瓦片金字塔的主要原理为：基于某个特定的地图投影坐标系（常规是Web墨卡托），将曲面的地球投影到二维平面，而后将该二维平面进行多尺度地划分，即相当于制作了多个不同分辨率层级的数字地图。各层级对应相应编码，层级越高地图所对应的分辨率越高；而后对每一层级的全球空间范围地图按照某种空间划分方法进行格网划分，划分成若干行和列的固定尺寸的正方形栅格图片，这些切分出来的规整的单个格网单元称为瓦片，各层级的划分方法都是相同的。\n瓦片划分方法需满足以下条件：\n每个层级下的所有瓦片可以无缝拼合成一张全球空间范围的世界地图 每个瓦片都有唯一编码，根据编码可以解算该瓦片对应的空间范围 在某一层级下给定一坐标点可以根据其空间坐标解算其所在瓦片的编号 每一层级瓦片对应一层金字塔分层，所有层级的瓦片便构成了整个瓦片金字塔模型。每一层中的瓦片划分方法一般采用均匀四分的划分方法，即以赤道和中央经线的交点为初始中心，不断地对地图进行四分，直到每个格网的大小为tilesize * tilesize为止，其中tilesize表示单个瓦片的边长。基于此种划分方法，第0层金字塔（金字塔顶层）用一个瓦片就能表示整张世界地图，第1层要用4^z个瓦片来表示整个世界地图，z为当前瓦片的金字塔层级。\n瓦片投影坐标系 瓦片金字塔模型中的投影坐标系可以有多种，目前最广泛采用的是Web Mercator投影，它是Mercator投影的一种变体。\n瓦片坐标系 所有瓦片的编码都是基于瓦片坐标系下进行的，瓦片坐标系的原点一般都在左上角或者左下角，TMS规范中是在左下角（GeoWebCache遵循该规范），但是现有的Google、Mapnik切片系统都是选用左上角作为原点，本文主要以原点在左上角的瓦片坐标系进行说明。 瓦片的编码方式如下图所示，层级用z表示，瓦片经线方向（指瓦片经度发生变化的方法，即东西向，东向为正）上编号为x，纬线方向（指瓦片维度发生变化的方向，即南北向，南向为正）上编号为y，因此每一个瓦片都可以通过一个三维元组（x,y,z）来唯一描述。\n总的来说，如今我们所说的影像（栅格）金字塔大多指代的是影像金字塔与瓦片金字塔的结合体或是瓦片金字塔（默认含有分层），而不是单独指代分层或者单独指代分块。\n矢量金字塔 影像金字塔是为栅格数据服务的，也是图像切片时代的核心产物。但是到了矢量切片时代后，由于矢量数据并不具备分辨率的特性，且矢量数据不同于栅格数据，它有着疏密不一致、分布不均与的特点，所以无法直接利用影像金字塔技术。不过瓦片金字塔是基于分层金字塔的基础上构建的，所以对数据类型并没有要求，在矢量切片中是可以直接复用的。总而言之，在矢量切片时代中，需要一个符合矢量数据特点的分层模型，作为矢量数据源与瓦片金字塔之间沟通的桥梁。同时考虑到应用上的兼容性，所以最终基于金字塔理论之上进行分层模型的定义，谓之：矢量金字塔。\n对于矢量数据的分层，将使用比例尺作为尺度描述，建立一系列不同比例尺的分层。不同于栅格金字塔的多分辨率层级，矢量数据金字塔没有分辨率的概念，但是不同层级之间的数据详尽程度也是不同的。随着比例尺的由小到大，矢量要素也变得越来越详细；而随着比例尺由大到小，矢量要素也将变得精简与概化，以符合人们的使用要求。\n总而言之，矢量金字塔的的目的就是解决在小比例尺下大数据量（或高密度区域）矢量数据聚集度高、要素重叠和显示速度慢的问题。（其实就是矢量数据的制图综合问题，矢量金字塔不过是其中的一个解而已）\n注：我们此处所谈矢量金字塔，只是矢量数据分层金字塔，只是分层。\n矢量分层 为了达成人们的使用要求，则需要对数据进行处理，以符合给定比例尺级别下保持相应的详尽程度。\n与栅格数据不同，矢量数据通常都具有空间特征与属性信息。空间特征体现在数据的空间坐标系、空间分布以及几何特征；属性信息则是数据实体相关的一些信息。那么矢量数据的空间特征与属性信息则可作为分层的依据，首要对数据的属性信息进行处理，而后再基于空间特征进行处理。\n属性分类分级 对于数据的属性特征处理主要是对属性信息进行分类和分级两种情况。分类是根据属性信息划分类别；分级即根据属性信息按照其重要程度划分不同的级别，并且赋予不同的权重值。\n空间特征处理 基于空间特征的处理则就多种多样了，比如可以基于数据的分布（密度）抽稀、基于周长或面积进行选取、基于几何形状进行简化。在处理的同时，还需要是实际情况考虑是否维持数据的拓扑关系。\n总而言之，可将上述的分层处理方法抽象为两种：\n选取（Filter）：属性分类分级，基于密度、周长、面积的处理都属于选取 简化（Simplify）：基于空间几何形状的处理属于简化 当然，上述两种只是最基础的分层处理方法，后续还可以有更多处理方法，比如合并、融合、夸大等。但我想来，如果能够比较合理的完成上述两个操作，应该也是达到了基本可用层次。\n不同行业的数据具有着不同的重点或侧重点，分层不仅需要结合矢量数据模型的特点，还需要结合行业背景与应用场景综合考虑。\n此处再借用公谨（遥想公瑾当年）的一句描述进行佐证：\n所谓的矢量金字塔模型，即基于制图综合的知识，分别设置海量数据在不同zoom下是否显示，是否简化，是否融合的一种策略，当动态提取切片时，根据这个策略选择数据，实际捞取的数据就非常少，有效解决了矢量切片不能解决数据太密集集中的问题。\n矢量金字塔 → 矢量数据的多尺度表达 → 矢量数据的自动制图综合（保证综合前后要素内部及要素之间的拓扑关系是矢量地图正确显示的基本需求 😯）\n注：此处所述分层，并不是比例尺等级。而是在不同比例尺层级下，矢量数据的详尽程度。而分层处理，即为通过一定的手段来控制数据的详尽程度。\n技术实现 在技术实现上，目前我看到的都是以瓦片金字塔结构为基础，叠加分层处理手段的方式实现的。因为瓦片金字塔是在分层金字塔的基础之上，而分层可分为两个部分：\n尺度分级定义 → 一系列的比例尺等级 分层分级处理（综合算子） → 一系列的处理算子（如：Filter、Simplify） OGC TileMatrixSet 定义 而尺度分级定义不论是在分层金字塔还是瓦片金字塔中都是一致的，也就是将分层分级处理（综合算子）剥离出来单独实现，在最终的瓦片生产流程中，接入瓦片金字塔即可。\nGeoWebCache 实现 瓦片金字塔结构 public class GridSet { private String name; // 投影坐标系 private SRS srs; // 瓦片宽, such as 256 private int tileWidth; // 瓦片高, such as 256 private int tileHeight; /** * Whether the y-coordinate of {@link #tileOrigin()} is at the top (true) or at the bottom * (false) */ protected boolean yBaseToggle = false; /** * By default the coordinates are {x,y}, this flag reverses the output for WMTS getcapabilities */ private boolean yCoordinateFirst = false; private boolean scaleWarning = false; // 将坐标参考系统 (CRS) 单位转换为米的系数 // 也就是说，这个参数表示的是给定的CRS中一个单元转换为米的系数。换句话说，也就是在指定的CRS中，一个单元表示多少米。 // 目前常用的就两种投影，一是以米为单位的（即metersPerUnit为1）；其次是以度为单位的经纬度投影（metersPerUnit表示为1度代表多少米，即：360/赤道周长，不同CRS使用不同的椭球体，所以其赤道周长也会存在一定差异。） private double metersPerUnit; // 像素大小, 通常给定0.","title":"矢量金字塔技术研究"},{"content":"前言 书接上回，此前提到地图瓦片切片技术的发展。矢量切片技术将瓦片的渲染由服务端迁移到客户端，此操作带来的影响力不可谓不大，基于此，完全可以随心所欲的定义地图的表达。那么在实际的应用当中，当渲染从服务端迁移后客户端后，是否会带来一些其他的问题？\n超20M的瓦片数据 此事发生在2023年，当时我们的技术组合是：空间数据库+GeoServer（vector tile plugin）+ Mapbox GL JS，基于此提供矢量瓦片服务。某一天，在某位心细如发的大佬的察觉下，突然发现提供的地图服务在层级为10级时，出现一个大小大于20M的瓦片，顿感惊人。经过多次核对后，确定在该份数据下，GeoServer在小比例尺下（约莫10级往下）生产的大部分瓦片尺寸都比较大，而数据密度越大的地方尤其严重。最终，我们认为：是由于GeoServer没有提供矢量分层抽稀简化的能力，所以导致在小比例尺+高密度的双重叠加下，瓦片大小暴增。\n在一段时间后，为了快速处理该问题，我曾提出：可以基于目前配图的思想（解析配图文件），在服务端去往数据库读取数据的时候便进行数据过滤（其实就是分层分级），避免大量不需要的数据经查询、传输、编码再传输带来的影响。比如我们按照口径进行约定，在13级往下，只显示大于xx口径的数据的数据，那么此时所有小于该口径的数据都不需要被传递。\n但我们前端配图人员提出一个说法：说如果你这般处理，那么意味着配图的时候数据就不是完整的，我无法自行选择数据，不同的项目需求也不尽相同，灵活性自然大打折扣。同时，也就意味着该项配置与瓦片缓存是绑定的，一旦存在配图项的变更，便意味着此前缓存均是失效的。\n配图的边界 时至今日，已过半年。目前想来，这里确定存在这么几个问题：\n到底什么是配图，配图配的又是什么？ 数据发布过程中，配图应该处于什么阶段？ 对于什么是配图，我目前还给不出一个十分恰当的回答，因为此前确实没有做过这方面的内容。但私以为狭义情况下，地图的符号化过程应该称之为配图。配图内容如下：\n要素符号化 文字标注 那么分层显示控制能力（属性分级）是否应该归属到配图中呢？在图像切片时代中，想来配图都是提前确定好的，而且受益于影像金字塔自带多层次分辨率的能力，所以在配图中压根不需要考虑分级问题（假定数据都是栅格数据）。但是矢量数据并没有分辨率的说法，所以无法从影像金字塔模型获利。在面对大数据量或高密度区域的情况时，需自行处理分级问题。\n对于矢量数据来说，在矢量瓦片技术出现之前，所有瓦片都是在服务端渲染，那么属性分级控制也是在服务端进行的。而在矢量瓦片技术出现之后，完全可以直接在客户端实现属性分层控制了。且目前开源GIS技术方案大多都选择：GeoServer+Mapbox，但在实际使用中感觉GeoServer并不理会什么属性分级和空间特征简化，而是一股脑的将数据丢给客户端，客户端自己筛选。\n从结果来看，好像在应用上确实是行得通的。实际上确实如此，或许大多数公司都是这样做的吧。但是我认为这是有问题的，这就好像是得益于矢量瓦片渲染能力后移的特点，服务端将原本应该自己管控的数据一股脑的丢到了客户端，由客户端自行控制。\n那么问题来了，数据的分层分级控制权到底应该给谁，是瓦片生产端还是客户端呢？\n我认为应该优先厘清整个生产流程，明确各节点能力边界\n数据的分类、分层肯定是在最前面，且分类、分层与金字塔分层息息相关。且应该形成公知，瓦片的生产端与应用的客户端都应该清晰的知道【数据分层分级控制策略 → 矢量金字塔分层规则定义】 其次是矢量数据的发布，此中应该完整的实现矢量数据分层，即形成矢量金字塔结构【矢量金字塔模型实现】 因矢量瓦片技术带来的配图后置，数据的符号化在此处完成【地图符号化】 其次，可以想一下数据分层控制权限归属到客户端后会带来什么样的影响：\n在大数据量或高密度区域的情况下，小比例尺级别瓦片尺寸可能爆炸 计算资源、存储资源（缓存）占用增多 对带宽要求高，移动端的话流量嗖嗖的跑 客户端性能下降 所以我认为，数据控制能力应该归属于瓦片生产端，客户端可以支持分层控制能力，但其对于数据的控制只能在全局统一的分类、分层策略所提供的范围内活动。也就是说客户端可以实现的分层范围是全局规定的范围的子集。\n对于目前在配图中进行全局数据的分层分级行为，我认为是矢量瓦片技术带来的配图动作后置产生的影响。同时在GeoServer + Mapbox 客户端控制数据操作的长期影响下，给后来人一种错觉便是数据控制也变更到客户端控制。\n论GeoServer的正确使用？ 最近在自己实现一个矢量瓦片服务，由于自己目前只会写Java，所以不得不对GeoServer进行借鉴，所以就进一步的研究了GeoServer的源码。\nGeoServer可以看作是GeoTools，GeoWebCache以及OGC API Implement的结合体。其中，GeoWebCache提供了金字塔结构的定义和缓存能力；GeoTools提供了空间数据相关的定义和操作。在本次研究后，我确定我需要向GeoServer道歉。在此前的描述中，我认为GeoServer是没有提供数据分层和空间简化的能力的，但是我错了。\n我拉取的是GeoServer的2.22.x分支，因为这个版本用的比较多，相对而言更具有代表性，所以没有选择最新版本。\n空间简化能力 也就是说，在GeoServer vector tile的生产过程中，已经集成了Simplify功能。\n还有一个情况，其实GeoServer Vector Tile Plugin采用的矢量瓦片编码器（java-vector-tile）中其实也提供了Simplify的功能，只不过默认是关闭的。\n数据分层（属性分级） GeoServer的vector tile实现中，居然是尝试从Style（SLD）中获取到Filter的信息，用以减少数据的检索（数据库中的Filter比Java基于内存的Filter更高效），同时还提供了一个将Mapbox style转换为SLD的拓展模块。\n看到这里的我很激动，感觉找到了知己一般，哈哈哈。\n在明白真相之后的我，只能是感叹GeoServer的历史包袱太重了，但应该被敬佩。\n而此时再次回头看我之前提出解析配图文件的想法，发现这种做法也是有问题的。\n矢量瓦片技术出现，实现了一套瓦片数据可以有N种配图方案 矢量瓦片技术完美的分离了数据与渲染，边界清晰 所以，基于配图剥离属性分级的方式不就又将渲染与数据耦合起来，配图也就会和缓存绑定了，也就是一种配图方案一套瓦片数据了（若一旦涉及到分级部分的变化），自然是有问题的。\n结论 综上所述，私以为：\n在矢量切片技术下，数据当有数据本身的分层规则，不应该依赖于配图，也不应该由配图来定义。\n分层控制当属于数据控制权限，应该归属于瓦片的生产端控制 在矢量瓦片时代，配图就是地图符号化的过程，对应矢量切片技术下，渲染后移到客户端的部分 全局分层分级策略应该是首先定义的，且应该形成公知的形态。服务端的数据控制应当严格遵循该策略，客户端可控分层范围是该策略的子集 参考 GeoServer Branch 2.22.x ","permalink":"https://fuyi-atlas.github.io/posts/gis/thoughts-on-mvt-front-end-rendering/","summary":"前言 书接上回，此前提到地图瓦片切片技术的发展。矢量切片技术将瓦片的渲染由服务端迁移到客户端，此操作带来的影响力不可谓不大，基于此，完全可以随心所欲的定义地图的表达。那么在实际的应用当中，当渲染从服务端迁移后客户端后，是否会带来一些其他的问题？\n超20M的瓦片数据 此事发生在2023年，当时我们的技术组合是：空间数据库+GeoServer（vector tile plugin）+ Mapbox GL JS，基于此提供矢量瓦片服务。某一天，在某位心细如发的大佬的察觉下，突然发现提供的地图服务在层级为10级时，出现一个大小大于20M的瓦片，顿感惊人。经过多次核对后，确定在该份数据下，GeoServer在小比例尺下（约莫10级往下）生产的大部分瓦片尺寸都比较大，而数据密度越大的地方尤其严重。最终，我们认为：是由于GeoServer没有提供矢量分层抽稀简化的能力，所以导致在小比例尺+高密度的双重叠加下，瓦片大小暴增。\n在一段时间后，为了快速处理该问题，我曾提出：可以基于目前配图的思想（解析配图文件），在服务端去往数据库读取数据的时候便进行数据过滤（其实就是分层分级），避免大量不需要的数据经查询、传输、编码再传输带来的影响。比如我们按照口径进行约定，在13级往下，只显示大于xx口径的数据的数据，那么此时所有小于该口径的数据都不需要被传递。\n但我们前端配图人员提出一个说法：说如果你这般处理，那么意味着配图的时候数据就不是完整的，我无法自行选择数据，不同的项目需求也不尽相同，灵活性自然大打折扣。同时，也就意味着该项配置与瓦片缓存是绑定的，一旦存在配图项的变更，便意味着此前缓存均是失效的。\n配图的边界 时至今日，已过半年。目前想来，这里确定存在这么几个问题：\n到底什么是配图，配图配的又是什么？ 数据发布过程中，配图应该处于什么阶段？ 对于什么是配图，我目前还给不出一个十分恰当的回答，因为此前确实没有做过这方面的内容。但私以为狭义情况下，地图的符号化过程应该称之为配图。配图内容如下：\n要素符号化 文字标注 那么分层显示控制能力（属性分级）是否应该归属到配图中呢？在图像切片时代中，想来配图都是提前确定好的，而且受益于影像金字塔自带多层次分辨率的能力，所以在配图中压根不需要考虑分级问题（假定数据都是栅格数据）。但是矢量数据并没有分辨率的说法，所以无法从影像金字塔模型获利。在面对大数据量或高密度区域的情况时，需自行处理分级问题。\n对于矢量数据来说，在矢量瓦片技术出现之前，所有瓦片都是在服务端渲染，那么属性分级控制也是在服务端进行的。而在矢量瓦片技术出现之后，完全可以直接在客户端实现属性分层控制了。且目前开源GIS技术方案大多都选择：GeoServer+Mapbox，但在实际使用中感觉GeoServer并不理会什么属性分级和空间特征简化，而是一股脑的将数据丢给客户端，客户端自己筛选。\n从结果来看，好像在应用上确实是行得通的。实际上确实如此，或许大多数公司都是这样做的吧。但是我认为这是有问题的，这就好像是得益于矢量瓦片渲染能力后移的特点，服务端将原本应该自己管控的数据一股脑的丢到了客户端，由客户端自行控制。\n那么问题来了，数据的分层分级控制权到底应该给谁，是瓦片生产端还是客户端呢？\n我认为应该优先厘清整个生产流程，明确各节点能力边界\n数据的分类、分层肯定是在最前面，且分类、分层与金字塔分层息息相关。且应该形成公知，瓦片的生产端与应用的客户端都应该清晰的知道【数据分层分级控制策略 → 矢量金字塔分层规则定义】 其次是矢量数据的发布，此中应该完整的实现矢量数据分层，即形成矢量金字塔结构【矢量金字塔模型实现】 因矢量瓦片技术带来的配图后置，数据的符号化在此处完成【地图符号化】 其次，可以想一下数据分层控制权限归属到客户端后会带来什么样的影响：\n在大数据量或高密度区域的情况下，小比例尺级别瓦片尺寸可能爆炸 计算资源、存储资源（缓存）占用增多 对带宽要求高，移动端的话流量嗖嗖的跑 客户端性能下降 所以我认为，数据控制能力应该归属于瓦片生产端，客户端可以支持分层控制能力，但其对于数据的控制只能在全局统一的分类、分层策略所提供的范围内活动。也就是说客户端可以实现的分层范围是全局规定的范围的子集。\n对于目前在配图中进行全局数据的分层分级行为，我认为是矢量瓦片技术带来的配图动作后置产生的影响。同时在GeoServer + Mapbox 客户端控制数据操作的长期影响下，给后来人一种错觉便是数据控制也变更到客户端控制。\n论GeoServer的正确使用？ 最近在自己实现一个矢量瓦片服务，由于自己目前只会写Java，所以不得不对GeoServer进行借鉴，所以就进一步的研究了GeoServer的源码。\nGeoServer可以看作是GeoTools，GeoWebCache以及OGC API Implement的结合体。其中，GeoWebCache提供了金字塔结构的定义和缓存能力；GeoTools提供了空间数据相关的定义和操作。在本次研究后，我确定我需要向GeoServer道歉。在此前的描述中，我认为GeoServer是没有提供数据分层和空间简化的能力的，但是我错了。\n我拉取的是GeoServer的2.22.x分支，因为这个版本用的比较多，相对而言更具有代表性，所以没有选择最新版本。\n空间简化能力 也就是说，在GeoServer vector tile的生产过程中，已经集成了Simplify功能。\n还有一个情况，其实GeoServer Vector Tile Plugin采用的矢量瓦片编码器（java-vector-tile）中其实也提供了Simplify的功能，只不过默认是关闭的。\n数据分层（属性分级） GeoServer的vector tile实现中，居然是尝试从Style（SLD）中获取到Filter的信息，用以减少数据的检索（数据库中的Filter比Java基于内存的Filter更高效），同时还提供了一个将Mapbox style转换为SLD的拓展模块。\n看到这里的我很激动，感觉找到了知己一般，哈哈哈。\n在明白真相之后的我，只能是感叹GeoServer的历史包袱太重了，但应该被敬佩。\n而此时再次回头看我之前提出解析配图文件的想法，发现这种做法也是有问题的。\n矢量瓦片技术出现，实现了一套瓦片数据可以有N种配图方案 矢量瓦片技术完美的分离了数据与渲染，边界清晰 所以，基于配图剥离属性分级的方式不就又将渲染与数据耦合起来，配图也就会和缓存绑定了，也就是一种配图方案一套瓦片数据了（若一旦涉及到分级部分的变化），自然是有问题的。\n结论 综上所述，私以为：\n在矢量切片技术下，数据当有数据本身的分层规则，不应该依赖于配图，也不应该由配图来定义。\n分层控制当属于数据控制权限，应该归属于瓦片的生产端控制 在矢量瓦片时代，配图就是地图符号化的过程，对应矢量切片技术下，渲染后移到客户端的部分 全局分层分级策略应该是首先定义的，且应该形成公知的形态。服务端的数据控制应当严格遵循该策略，客户端可控分层范围是该策略的子集 参考 GeoServer Branch 2.","title":"关于矢量瓦片技术支持前端渲染带来的思考"},{"content":"前言 本文80%内容节选自：《WebGIS数据不切片或是时代必然》，后在其基础上添加了部分内容。\n数据切片是解决大规模大体积地理数据在Web前端轻量化传输和显示的关键技术，是每一个开发者几乎每天都在使用的技术，有时候将服务端底图切成xyz图片，有时候将大影像数据切成xyz图片，也有时候将矢量数据切成xyz的矢量切片。\n数据切片起源\u0026ndash;图像切片时代（WMS → WMTS，服务端渲染+预处理） Web上古时代，人们浏览在线地图，一般是服务器端将页面地图要显示的地理范围内的地理数据都查询出来，然后在服务端按照专题地图的配置样式，渲染成地图图片，再返回客户端浏览，这个时代诞生的OGC标准就是WMS（Web Map Service）服务，该服务一直沿用至今。\n类似WMS这种服务存在很多致命的缺点：由于地理范围的不可控，获取范围内的数据不可控，数据有时候会很大，数据通常在数据库中，IO和网络传输就耗时很严重；服务端获取数据后会进行数据渲染成地图图片，占用大量CPU资源；克服一系列的困难终于成图传输给前端浏览。可以想象，这个过程如此艰难，用户可能等的花儿都开了。\n为了解决这个痛点，谷歌地图开创性提出了基于Web墨卡托投影，预先在服务端全量渲染，然后按照地图不同的显示级别（金字塔原理），切成了xyz的图片。\n该技术成为Web2.0时代使用最广泛的技术，成为WebGIS标准，诞生了一系列如基于工业标准的TMS服务，基于OGC规范的WMTS服务。很快，百度地图，高德地图、天地图都使用该技术原理建设了自己的切片地图服务。广大GIS从业者也开始了“项目一启动，先把底图切”的套路。\n基于该技术原理，对大的影像数据如几十G的GeoTiff数据也进行了切片，形成影像底图给客户端使用。\n这个时期的开源GIS主要技术是基于GeoServer的WMS、WFS、TMS、WMTS服务。\n核心技术：Web墨卡托投影、栅格金字塔\n数据切片发展\u0026ndash;矢量切片时代（WMTS，客户端渲染+预处理） 虽然基于XYZ的图片切片技术很成功，但是也存在不少问题：\n全图预切耗时长：通常切图zoom级别从0到20，通过金字塔原理图可知，上一级别的一张图片，下一级别会裂变成4张，级别越大，图片越多，同时地理范围越大，图片也越多，图片多了，切图耗时就很长，资源无论是基础底图切片还是影像底图切片都要耗时漫长。 资源要求高：需要庞大的服务端渲染和切图的计算资源，满足分布式渲染、切图、存储需求。 存储冗余：原始矢量和栅格数据，同时冗余存储了数据切片，存储压力大，数据切片转储IO压力大。ArcGIS为了提升转储性能，开创了离散性切片和紧凑型切片两个格式，紧凑型切片在转储时性能较高。 数据更新不及时：由于切图耗时长，通常很少会定期更新地图切片数据，导致数据显示落后于实际生活现状，不能很好的服务用户。 地图千篇一律：这个时期项目最常采用的是天地图地图服务，不管啥项目，不管什么地区，似乎地图都长一个样子。甲方可能审美疲惫了，他们希望底图能是定制化的，能不能暗黑一点？科幻一点？ 既然存在问题，自然有人想解决问题，从15年左右，Mapbox受够了传统地图显示风格了，它提出了MVT矢量切片技术，该技术一经推出就大受欢迎，开启了WebGIS底图定制化时代。\n很显然，矢量切片和图像切片最大的区别就是：渲染从服务端迁移到客户端了，换句话说，服务端减负了，带来的好处也是不言而喻：\n资源要求降低：服务器资源要求可见性降低，渲染很耗资源的。 数据更新比较及时：如果数据更新，简单更新下局部变更地区的数据的矢量切片即可，由于不用全图渲染，耗时极大减少，数据的有效性极大提高。 存储冗余降低：采用MbTiles的设计规范，将数据和序号索引剥离，复用重复区域数据，极大降低切片数据量。但是数据冗余还是存在的。 地图不再千篇一律：渲染是客户端的，那么甲方各种优秀的创意都能得到释放了，配置大屏的感觉已经是很容易的事情了。各行各业的地图五彩斑斓各有千秋，地图设计者们的春天来了。高德、百度跟的很快，目前也都提供矢量切片底图定制化服务了 这个时期的开源GIS主要技术是基于GeoServer的矢量切片服务，基于Mapbox的底图切图工具tippecanoe等。这个时期，3D GIS蓬勃发展，诞生了3DTiles、I3S等3D切片ogc规范，并大量在工程中使用。\n矢量切片解决了不少问题，但仍遗留问题是：全图预切耗时长，切片数据冗余占用存储空间。这个问题同样也是栅格切片、3D切片共同的问题。于是有人问，能不能数据不切片？也就不存在预切耗时长，切片数据冗余的问题了？答案是肯定的。\n核心技术：矢量金字塔，MVT\n正所谓尾大不掉，虽然已经出现了矢量切片技术，但是整体的工艺流程还是在预切路线。这可能和矢量切片解决的问题相关，因为矢量切片解决的是渲染的问题，可以更自由的进行渲染了，而且只需要提供一份切片数据即可。\n数据切片方向\u0026ndash;动态切片时代（WMTS，客户端渲染） 那么上一个阶段遗留的问题还有什么：预处理，也就是不管怎么样，我先切数据\n笔者（原文作者：遥想公瑾当年）曾经大量使用PostGIS+并行计算+动态矢量切片技术实现动态业务数据的快速前端矢量呈现，PostGIS是目前开源架构里唯一能支持动态矢量切片的数据技术。\n动态切片技术不再预先切图，也不会有大量切片的文件存储，将切片技术诞生以来所遗留的问题都一次清空。由于通常数据量很大，动态矢量切片技术基本上数据不出库，而由数据库汇总组织数据并直接生成切片结果出去，后台几乎啥也不做，时空数据库的地位越来越重要。\n这是一个崭新的时代，预示着数据不切片时代的来临，毕竟矢量和栅格不切片技术理论上还是比较成熟的，未来3DTiles这些3D切片理论也会逐渐成熟的。\n核心技术：动态矢量切片+矢量金字塔+MVT\n在这里我想要补充一下上面的示意图，这里不是说动态矢量切片技术只能应用在数据库中，前文中也提到当前开源架构中PostGIS是唯一能支持动态矢量切片的数据技术。这里是基于传统矢量切片技术与PostGIS的动态矢量切片技术的对比，更多的是想表达流程的变化（预切 → 动态，以及切片产生的位置），以及流程变化带来的性能提升和相关影响；还有就是原文作者认为的后续的发展方向，Based on spatial database。\n关于动态矢量切片技术 动态矢量切片技术可以说是传统矢量切片的动态应用（更广义的可以认为：是可以按需动态生成的，具备多层次模型，且每个层次包含适当选取及简化的数据的切片技术）。强调不再预切，而是按需生成，且应该同时满足可以快速显示的要求。\n按需动态生成（与数据源是链接着的或是可链接的，即可达成随时可访问），不做预切 多层次结构（如：矢量金字塔） 数据选取与简化 MVT支持 从发展进程上面看，技术的升级是为了解决此前存在的痛点问题。动态矢量切片技术是为了解决预切耗时，且会产生大量的瓦片存储（预切），数据越大，产生的瓦片越多。\n从实际的应用上看，此前的矢量切片技术我更愿意将之称为传统矢量切片技术，用以与矢量切片做一定的区分。传统矢量切片技术还是走的预处理的路子，产生的是静态矢量瓦片数据，即在流程上是无法做到快速更新的。与之对应的便是动态实时生产的路线，可称为动态矢量瓦片技术。私以为，两者更大的区别在于矢量瓦片生产的思路，也就是是否需要预切。\n所以，动态矢量切片技术可以有各种各样的实现，而PostGIS中的动态矢量瓦片技术便是其中的一种。且到目前，GeoServer同样实现了动态矢量瓦片技术（只不过其历史包袱过重 🫡）。\n值得注意的是，基于数据库的动态矢量切片有一个很大的特点：缩短了切片的传导路径（也就是所谓的数据不出库，出库即为切片）。\n参考 WebGIS数据不切片或是时代必然 PostGIS动态矢量切片（原理+实现） ","permalink":"https://fuyi-atlas.github.io/posts/gis/tiling-technology-development/","summary":"前言 本文80%内容节选自：《WebGIS数据不切片或是时代必然》，后在其基础上添加了部分内容。\n数据切片是解决大规模大体积地理数据在Web前端轻量化传输和显示的关键技术，是每一个开发者几乎每天都在使用的技术，有时候将服务端底图切成xyz图片，有时候将大影像数据切成xyz图片，也有时候将矢量数据切成xyz的矢量切片。\n数据切片起源\u0026ndash;图像切片时代（WMS → WMTS，服务端渲染+预处理） Web上古时代，人们浏览在线地图，一般是服务器端将页面地图要显示的地理范围内的地理数据都查询出来，然后在服务端按照专题地图的配置样式，渲染成地图图片，再返回客户端浏览，这个时代诞生的OGC标准就是WMS（Web Map Service）服务，该服务一直沿用至今。\n类似WMS这种服务存在很多致命的缺点：由于地理范围的不可控，获取范围内的数据不可控，数据有时候会很大，数据通常在数据库中，IO和网络传输就耗时很严重；服务端获取数据后会进行数据渲染成地图图片，占用大量CPU资源；克服一系列的困难终于成图传输给前端浏览。可以想象，这个过程如此艰难，用户可能等的花儿都开了。\n为了解决这个痛点，谷歌地图开创性提出了基于Web墨卡托投影，预先在服务端全量渲染，然后按照地图不同的显示级别（金字塔原理），切成了xyz的图片。\n该技术成为Web2.0时代使用最广泛的技术，成为WebGIS标准，诞生了一系列如基于工业标准的TMS服务，基于OGC规范的WMTS服务。很快，百度地图，高德地图、天地图都使用该技术原理建设了自己的切片地图服务。广大GIS从业者也开始了“项目一启动，先把底图切”的套路。\n基于该技术原理，对大的影像数据如几十G的GeoTiff数据也进行了切片，形成影像底图给客户端使用。\n这个时期的开源GIS主要技术是基于GeoServer的WMS、WFS、TMS、WMTS服务。\n核心技术：Web墨卡托投影、栅格金字塔\n数据切片发展\u0026ndash;矢量切片时代（WMTS，客户端渲染+预处理） 虽然基于XYZ的图片切片技术很成功，但是也存在不少问题：\n全图预切耗时长：通常切图zoom级别从0到20，通过金字塔原理图可知，上一级别的一张图片，下一级别会裂变成4张，级别越大，图片越多，同时地理范围越大，图片也越多，图片多了，切图耗时就很长，资源无论是基础底图切片还是影像底图切片都要耗时漫长。 资源要求高：需要庞大的服务端渲染和切图的计算资源，满足分布式渲染、切图、存储需求。 存储冗余：原始矢量和栅格数据，同时冗余存储了数据切片，存储压力大，数据切片转储IO压力大。ArcGIS为了提升转储性能，开创了离散性切片和紧凑型切片两个格式，紧凑型切片在转储时性能较高。 数据更新不及时：由于切图耗时长，通常很少会定期更新地图切片数据，导致数据显示落后于实际生活现状，不能很好的服务用户。 地图千篇一律：这个时期项目最常采用的是天地图地图服务，不管啥项目，不管什么地区，似乎地图都长一个样子。甲方可能审美疲惫了，他们希望底图能是定制化的，能不能暗黑一点？科幻一点？ 既然存在问题，自然有人想解决问题，从15年左右，Mapbox受够了传统地图显示风格了，它提出了MVT矢量切片技术，该技术一经推出就大受欢迎，开启了WebGIS底图定制化时代。\n很显然，矢量切片和图像切片最大的区别就是：渲染从服务端迁移到客户端了，换句话说，服务端减负了，带来的好处也是不言而喻：\n资源要求降低：服务器资源要求可见性降低，渲染很耗资源的。 数据更新比较及时：如果数据更新，简单更新下局部变更地区的数据的矢量切片即可，由于不用全图渲染，耗时极大减少，数据的有效性极大提高。 存储冗余降低：采用MbTiles的设计规范，将数据和序号索引剥离，复用重复区域数据，极大降低切片数据量。但是数据冗余还是存在的。 地图不再千篇一律：渲染是客户端的，那么甲方各种优秀的创意都能得到释放了，配置大屏的感觉已经是很容易的事情了。各行各业的地图五彩斑斓各有千秋，地图设计者们的春天来了。高德、百度跟的很快，目前也都提供矢量切片底图定制化服务了 这个时期的开源GIS主要技术是基于GeoServer的矢量切片服务，基于Mapbox的底图切图工具tippecanoe等。这个时期，3D GIS蓬勃发展，诞生了3DTiles、I3S等3D切片ogc规范，并大量在工程中使用。\n矢量切片解决了不少问题，但仍遗留问题是：全图预切耗时长，切片数据冗余占用存储空间。这个问题同样也是栅格切片、3D切片共同的问题。于是有人问，能不能数据不切片？也就不存在预切耗时长，切片数据冗余的问题了？答案是肯定的。\n核心技术：矢量金字塔，MVT\n正所谓尾大不掉，虽然已经出现了矢量切片技术，但是整体的工艺流程还是在预切路线。这可能和矢量切片解决的问题相关，因为矢量切片解决的是渲染的问题，可以更自由的进行渲染了，而且只需要提供一份切片数据即可。\n数据切片方向\u0026ndash;动态切片时代（WMTS，客户端渲染） 那么上一个阶段遗留的问题还有什么：预处理，也就是不管怎么样，我先切数据\n笔者（原文作者：遥想公瑾当年）曾经大量使用PostGIS+并行计算+动态矢量切片技术实现动态业务数据的快速前端矢量呈现，PostGIS是目前开源架构里唯一能支持动态矢量切片的数据技术。\n动态切片技术不再预先切图，也不会有大量切片的文件存储，将切片技术诞生以来所遗留的问题都一次清空。由于通常数据量很大，动态矢量切片技术基本上数据不出库，而由数据库汇总组织数据并直接生成切片结果出去，后台几乎啥也不做，时空数据库的地位越来越重要。\n这是一个崭新的时代，预示着数据不切片时代的来临，毕竟矢量和栅格不切片技术理论上还是比较成熟的，未来3DTiles这些3D切片理论也会逐渐成熟的。\n核心技术：动态矢量切片+矢量金字塔+MVT\n在这里我想要补充一下上面的示意图，这里不是说动态矢量切片技术只能应用在数据库中，前文中也提到当前开源架构中PostGIS是唯一能支持动态矢量切片的数据技术。这里是基于传统矢量切片技术与PostGIS的动态矢量切片技术的对比，更多的是想表达流程的变化（预切 → 动态，以及切片产生的位置），以及流程变化带来的性能提升和相关影响；还有就是原文作者认为的后续的发展方向，Based on spatial database。\n关于动态矢量切片技术 动态矢量切片技术可以说是传统矢量切片的动态应用（更广义的可以认为：是可以按需动态生成的，具备多层次模型，且每个层次包含适当选取及简化的数据的切片技术）。强调不再预切，而是按需生成，且应该同时满足可以快速显示的要求。\n按需动态生成（与数据源是链接着的或是可链接的，即可达成随时可访问），不做预切 多层次结构（如：矢量金字塔） 数据选取与简化 MVT支持 从发展进程上面看，技术的升级是为了解决此前存在的痛点问题。动态矢量切片技术是为了解决预切耗时，且会产生大量的瓦片存储（预切），数据越大，产生的瓦片越多。\n从实际的应用上看，此前的矢量切片技术我更愿意将之称为传统矢量切片技术，用以与矢量切片做一定的区分。传统矢量切片技术还是走的预处理的路子，产生的是静态矢量瓦片数据，即在流程上是无法做到快速更新的。与之对应的便是动态实时生产的路线，可称为动态矢量瓦片技术。私以为，两者更大的区别在于矢量瓦片生产的思路，也就是是否需要预切。\n所以，动态矢量切片技术可以有各种各样的实现，而PostGIS中的动态矢量瓦片技术便是其中的一种。且到目前，GeoServer同样实现了动态矢量瓦片技术（只不过其历史包袱过重 🫡）。\n值得注意的是，基于数据库的动态矢量切片有一个很大的特点：缩短了切片的传导路径（也就是所谓的数据不出库，出库即为切片）。\n参考 WebGIS数据不切片或是时代必然 PostGIS动态矢量切片（原理+实现） ","title":"切片技术发展"},{"content":"起源 天气小程序产生于2022年年初，目的是用于验证自己是否有进入全栈开发（仅前后端）的能力。\n受新冠疫情影响，2022年的春节是在杭州过的。还记得当时附近好几个地方都被划为了高风险，对整个区进行了管控。如果选择回家的话，得到将是14天的隔离，还不确定能否回来上班。因此便没有回去了。好在所在的区域情况并没有那么的严重，还是可以去买菜的。领了消费券，再加上公司发的年货，也没有想象中的那么糟糕。\n所以，既然没有回去，又有十来天的时间，总得做点什么东西才行，对吧 🙄！\n2022年，是我工作的第四个年头。受多方面的信息影响，我也想看看验证自己是否有进入全栈开发（仅前后端）的能力。\n历程 拂衣天气，又名微天气。\n一个集地理信息与天气预报为一体的天气预报类小程序，界面精美，使用便捷。【致敬：和风天气】！\n💡 主要是有人给我说拂衣天气，听着还以为是卖衣服的。so，我就想着换个名字，就有了微天气。但是由于微信认证的原因，所以有些地方还是拂衣天气。现在细细想来，好像也没有什么关系，那就先这样吧 🙄\n该项目从2022年1月12号正式启动，于2022年3月19日发布一阶段最终版本（1.1.9），总体耗时2个月零7天。从内容完整度以及界面友好程度来说，我给自己70分。 此前实在是没有经验，也没有相关的习惯，天气小程序开发过程中并没有编写相关的文档。所以在小程序开发完成之后，本来计划是将拂衣天气完整的开发过程通过文章的方式记录下来，并将该项目开源出去，甚至还想将该项目提交给和风天气。但最后的结果就是：文章就完成了四篇，也就是一个开头，没啥实际内容。@time 2022年8月16日 好的，现在时间来到2023年12月，我又有点时间了，因为我离职了（不是被裁）。这次的目标是完善文档，修复发现的一些BUG，然后新增一些内容，最终将该小程序开源，并贡献给和风天气。 接下来，让我来回忆一下每个阶段的详细内容 🤔\n项目初始 为什么会想到做一个天气小程序呢？\n嗯，首先我是一个做服务端开发的GIS开发工程师。在当时刚结束一个小程序的开发工作，觉得小程序这个东西还挺有意思。同时受到周边各种信息的影响，也想试一试写点前端的内容，最好是可以方便发布的那种，也算是自己的作品不是。为什么会选择天气类别，好像是当时刷网页还是什么，看到一个人分享自己做了一个天气机器人，然后给女朋友推天气信息。so， 🫣\n所以，我当时就想做一个天气类别的小程序，以此进行全栈开发能力的试炼。我想这会是一个微信小程序、是一个可以正常使用的小程序，以Java进行服务端开发，以Mapbox实现天气数据可视化。\n本阶段事务分为了三个阶段，分别是：调研、学习、实现\n调研 天气小程序什么最重要？当然是天气数据最为重要，所以首要内容便是确定天气数据的来源。其次便是确定本次天气小程序的技术实现构成。\n天气数据\n天气数据需要是真实的、可用的。那么可以通过网络中提供的天气API进行获取。\n通过一定的检索后，我选定了两个天气平台，分别是：和风天气、心知天气。\n高德天气：大平台，但是目前服务类目比较少\n彩云天气：免费接口几乎没有，收费又太贵\n心知天气\n心知天气试用版与开发者版开发产品几乎等同，且开发者版收费也不贵。最为关键的是，支持以经纬度方式进行天气查询。\n和风天气\n几乎可以免费使用其提供的所有 API，且同样支持经纬度方式进行天气查询。\n对比了这两者，发现至少都需要注册为开发者之后，才可以较好的使用其服务。且两者的开发者认证均需要实名。\n关于天气API的选择，我最终选择了和风天气，倒不是因为它可以免费使用。其实，刚开始的时候我更倾向于使用心知天气，因为它还可以直接查昨天的天气（和风对于历史天气的查询比较麻烦）。但是和风天气首页结合了地图进行可视化，而且还提供有APP可以使用（方便参考）。再加上，我想了想，其实我并没有迫切的需要知道昨天的天气情况。 🙄\n💡 其实最重要的原因在于：我先注册了心知天气（需要审核），过了半天后再去注册了和风天气（需要审核），但是最先通过审核的是和风天气（耗时大概也就半天左右，我是在春节期间注册的啊）。\n技术实现构成\n这里存在一个遗憾，小程序原生并不支持使用如mapbox这样的第三方地图框架，初始想法是通过webview的方式使用mapbox，但是遗憾的是，webview并不对个人类型的小程序开放使用。\n所以，退而求其次，选择腾讯地图（及其提供的样式）实现地图浏览。\n服务端程序则使用Java语言开发，天气数据是经过服务端代理的方式形成内部的数据接口，并非是小程序直接调用和风的接口\n最终将服务端程序部署到阿里云\n学习 我是一个后端开发工程师，我不怎么会写页面，我特别的讨厌写CSS。我也没有接触过前端开发和微信小程序开发，所以需要提前储备一下相关知识。\n前端知识\n我并没有想要精通前端技术，但是我需要比较体系的了解一下前端技术，方便进行小程序开发。所以我在B站找了两门前端视频学习（粗略的刷了一遍）\n尚硅谷Web前端零基础入门HTML5+CSS3基础教程丨初学者从入门到精通 千锋web前端开发项目教程_1000集完全零基础入门HTML5+CSS3+JS到精通 微信小程序知识\n其实就看官方文档就足够了，不过心虚的我还是在极客时间找了门课。最终发现：就官方文档就足够了，因为我并不需要很深入的东西\n实现 在有一定准备后，就开始进入实现环节了。\n在这里有一个大问题就是UI设计问题，我看和风天气APP就挺好看的，有天气有地图，身为GIS开发的我就很喜欢。又发现无论是和风天气官方，还是其他天气小程序应用，基本都没有携带地图的。\n所以，我当即决定参考和风天气官方APP界面，做一个类似的天气小程序。当然，我做的这个小程序就是奔着开源，同时将作为作品分享给和风天气，才想着如此操作的。请务必慎重 🤔\n在经过简单的设计过后，于2022年1月18日正式进行开发，2022年3月19日发布一阶段最终版本1.1.9。\n文档补充 今天偷的懒，明天都会让你加倍还回来。此前一是没意识，二是认为没必要，三还是太懒，所以并没有同步编写文档。\n现在计划将拂衣天气开发的完整过程通过文章的方式记录下来，下面是我对该整体内容的编写计划：\n但是，我又要开始说但是了。打工人还是打工人！\n天气小程序于2022年4月前就已经完成了开发，直至到今天（2022年8月16日）也就才完成了三篇文章，不得不说拖延症是真的严重。天气小程序只是一个应用，就目前的投入收益来说，不应该把过多的时间放在小程序上面。在加上距离开发已经过去了4个月，开发之时并没有进行文档产出，所以现在才进行复盘则是相当于重新实现了一遍。就目前来看，核心的行政区划数据合并已基本完成，所以，后面将暂停小程序相关内容，变更为GIS基础与计算机基础的学习。\n💡 对的，停更说明。\n死灰复燃 打工是不可能打工的，只要我不打工，我就有时间了 🙄\n那就修复、优化、新增 😎\nBUG修复 逻辑优化 文档编写 新增Web端、消息推送、接入AIGC 将作品分享给和风天气 我还特意画了两张图呢\n好的，我又要开始说但是了。但是，我错误估计了自己的空闲时间。所以，无情的鸽了新增的部分以及此前计划的开发过程文档。截至到2024年3月12日，已完成如下内容：\n既有BUG修复\n关注城市、登录、海报部分优化\n提供Docker Compose脚本，同时将阿里云的部署全部变更为使用Docker\n编写项目Readme文档\n已经给和风天气提了作品分享的PR，并联系他们表达了诉求 😃\n更新了3篇开发过程文档\n那么，小程序的版本也来到了1.1.12，当前就到这里了。 😮‍💨\n仓库地址：\n服务端：micro-weather-backend\n小程序端：micro-weather-wechat\n文档库：micro-weather-docs\n小结 拂衣天气从2022年1月12号正式启动，于2022年3月19日发布1.1.9版本，2024年3月14日发布1.1.12版本，时间几乎跨了2年。从内容完整度以及界面友好程度来说，我给自己75分。\n我知道，这个东西没有什么难度，整体也是很简单。但我就是感觉还不错的，哈哈哈！\n反正这2年间，我倒是一直在用，期间只出现因为更换服务器，证书等问题短暂的停过几天。\n不过，经过这个项目，也比较清晰的暴露出来我的几个问题：\n拖延症问题，执行力不够高 思考问题不够线性，有些散乱（好 也不好） 写文档的能力比较差 ☹️ 比较内向敏感，当时不太敢把自己的东西放出来，怕被人别吐槽 剩下的事情就是，一一克服咯！\n快速体验 微信搜索：拂衣天气\n扫描下方小程序码\n声明与致谢 天气数据均由和风天气免费提供 天气小程序的界面设计与交互设计主要参考和风天气APP 行政区域数据最终使用锐多宝提供的省市县数据，我仅在此基础上做了些许处理 部分图标来自阿里巴巴矢量图标库 天气预报模块的图标由和风天气提供 echarts组件由Apache Echarts提供 小程序内地图由腾讯地图提供 容器镜像仓库服务由阿里云提供 代码仓库以及服务端CI由Github提供 感谢和风天气、Github、微信、腾讯地图、阿里云、阿里巴巴矢量图标库、锐多宝以及所有为本项目提供支持的主体。\n吐槽 实在是不吐不快……\n腾讯地图自定义样式收费 其实，我是想说什么。就是你收费是可以的，但是对于个人开发者能不能单独拉个套餐啊。\n你说你要是399一年，我也就冲了。一个月399，我哪里买得起啊，这就不搭理个人开发的了啊 🙄\n看一下前后的对比图，左边的使用了个性化地图，右边的就是默认样式了。\n阿里云免费SSL证书收费 也就是说，此前一个域名每年有20个免费的单域名SSL证书还是可以使用的，但是每个单域名证书的有效期只有三个月。要是只是用来测试开发倒是没啥问题，但是如果如我这边，三个月换一次域名部的麻烦死。你只要花68块钱，你就可以获得一个和此前一摸一样的，有效期12个月的单域名证书 😯\n微信小程序认证收费 总体来说，就是：\n一年一审 审核费用按次数收费，失败也是要收费的哦 我付了30元，也就是30元/次。我是个人认证，不是企业 展望 拂衣天气（微天气）没做完的内容，我肯定还是会继续做的，不过内容可能会跟随着见闻发生变化。只不过当前它暂停了，因为我还有更重要的东西要写。\n下一个要做的就是：地图服务程序，它是一个类如Geoserver的中间服务程序，但是它需要是简单的，更容易上手的，以MVT为主的，程序性能和瓦片质量更好的程序。\n就如同geojson-vt一样的精简，但却是实用的。\n毕竟，我是一名以服务端开发为主的GIS开发工程师 🫡\n","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/009/","summary":"起源 天气小程序产生于2022年年初，目的是用于验证自己是否有进入全栈开发（仅前后端）的能力。\n受新冠疫情影响，2022年的春节是在杭州过的。还记得当时附近好几个地方都被划为了高风险，对整个区进行了管控。如果选择回家的话，得到将是14天的隔离，还不确定能否回来上班。因此便没有回去了。好在所在的区域情况并没有那么的严重，还是可以去买菜的。领了消费券，再加上公司发的年货，也没有想象中的那么糟糕。\n所以，既然没有回去，又有十来天的时间，总得做点什么东西才行，对吧 🙄！\n2022年，是我工作的第四个年头。受多方面的信息影响，我也想看看验证自己是否有进入全栈开发（仅前后端）的能力。\n历程 拂衣天气，又名微天气。\n一个集地理信息与天气预报为一体的天气预报类小程序，界面精美，使用便捷。【致敬：和风天气】！\n💡 主要是有人给我说拂衣天气，听着还以为是卖衣服的。so，我就想着换个名字，就有了微天气。但是由于微信认证的原因，所以有些地方还是拂衣天气。现在细细想来，好像也没有什么关系，那就先这样吧 🙄\n该项目从2022年1月12号正式启动，于2022年3月19日发布一阶段最终版本（1.1.9），总体耗时2个月零7天。从内容完整度以及界面友好程度来说，我给自己70分。 此前实在是没有经验，也没有相关的习惯，天气小程序开发过程中并没有编写相关的文档。所以在小程序开发完成之后，本来计划是将拂衣天气完整的开发过程通过文章的方式记录下来，并将该项目开源出去，甚至还想将该项目提交给和风天气。但最后的结果就是：文章就完成了四篇，也就是一个开头，没啥实际内容。@time 2022年8月16日 好的，现在时间来到2023年12月，我又有点时间了，因为我离职了（不是被裁）。这次的目标是完善文档，修复发现的一些BUG，然后新增一些内容，最终将该小程序开源，并贡献给和风天气。 接下来，让我来回忆一下每个阶段的详细内容 🤔\n项目初始 为什么会想到做一个天气小程序呢？\n嗯，首先我是一个做服务端开发的GIS开发工程师。在当时刚结束一个小程序的开发工作，觉得小程序这个东西还挺有意思。同时受到周边各种信息的影响，也想试一试写点前端的内容，最好是可以方便发布的那种，也算是自己的作品不是。为什么会选择天气类别，好像是当时刷网页还是什么，看到一个人分享自己做了一个天气机器人，然后给女朋友推天气信息。so， 🫣\n所以，我当时就想做一个天气类别的小程序，以此进行全栈开发能力的试炼。我想这会是一个微信小程序、是一个可以正常使用的小程序，以Java进行服务端开发，以Mapbox实现天气数据可视化。\n本阶段事务分为了三个阶段，分别是：调研、学习、实现\n调研 天气小程序什么最重要？当然是天气数据最为重要，所以首要内容便是确定天气数据的来源。其次便是确定本次天气小程序的技术实现构成。\n天气数据\n天气数据需要是真实的、可用的。那么可以通过网络中提供的天气API进行获取。\n通过一定的检索后，我选定了两个天气平台，分别是：和风天气、心知天气。\n高德天气：大平台，但是目前服务类目比较少\n彩云天气：免费接口几乎没有，收费又太贵\n心知天气\n心知天气试用版与开发者版开发产品几乎等同，且开发者版收费也不贵。最为关键的是，支持以经纬度方式进行天气查询。\n和风天气\n几乎可以免费使用其提供的所有 API，且同样支持经纬度方式进行天气查询。\n对比了这两者，发现至少都需要注册为开发者之后，才可以较好的使用其服务。且两者的开发者认证均需要实名。\n关于天气API的选择，我最终选择了和风天气，倒不是因为它可以免费使用。其实，刚开始的时候我更倾向于使用心知天气，因为它还可以直接查昨天的天气（和风对于历史天气的查询比较麻烦）。但是和风天气首页结合了地图进行可视化，而且还提供有APP可以使用（方便参考）。再加上，我想了想，其实我并没有迫切的需要知道昨天的天气情况。 🙄\n💡 其实最重要的原因在于：我先注册了心知天气（需要审核），过了半天后再去注册了和风天气（需要审核），但是最先通过审核的是和风天气（耗时大概也就半天左右，我是在春节期间注册的啊）。\n技术实现构成\n这里存在一个遗憾，小程序原生并不支持使用如mapbox这样的第三方地图框架，初始想法是通过webview的方式使用mapbox，但是遗憾的是，webview并不对个人类型的小程序开放使用。\n所以，退而求其次，选择腾讯地图（及其提供的样式）实现地图浏览。\n服务端程序则使用Java语言开发，天气数据是经过服务端代理的方式形成内部的数据接口，并非是小程序直接调用和风的接口\n最终将服务端程序部署到阿里云\n学习 我是一个后端开发工程师，我不怎么会写页面，我特别的讨厌写CSS。我也没有接触过前端开发和微信小程序开发，所以需要提前储备一下相关知识。\n前端知识\n我并没有想要精通前端技术，但是我需要比较体系的了解一下前端技术，方便进行小程序开发。所以我在B站找了两门前端视频学习（粗略的刷了一遍）\n尚硅谷Web前端零基础入门HTML5+CSS3基础教程丨初学者从入门到精通 千锋web前端开发项目教程_1000集完全零基础入门HTML5+CSS3+JS到精通 微信小程序知识\n其实就看官方文档就足够了，不过心虚的我还是在极客时间找了门课。最终发现：就官方文档就足够了，因为我并不需要很深入的东西\n实现 在有一定准备后，就开始进入实现环节了。\n在这里有一个大问题就是UI设计问题，我看和风天气APP就挺好看的，有天气有地图，身为GIS开发的我就很喜欢。又发现无论是和风天气官方，还是其他天气小程序应用，基本都没有携带地图的。\n所以，我当即决定参考和风天气官方APP界面，做一个类似的天气小程序。当然，我做的这个小程序就是奔着开源，同时将作为作品分享给和风天气，才想着如此操作的。请务必慎重 🤔\n在经过简单的设计过后，于2022年1月18日正式进行开发，2022年3月19日发布一阶段最终版本1.1.9。\n文档补充 今天偷的懒，明天都会让你加倍还回来。此前一是没意识，二是认为没必要，三还是太懒，所以并没有同步编写文档。\n现在计划将拂衣天气开发的完整过程通过文章的方式记录下来，下面是我对该整体内容的编写计划：\n但是，我又要开始说但是了。打工人还是打工人！\n天气小程序于2022年4月前就已经完成了开发，直至到今天（2022年8月16日）也就才完成了三篇文章，不得不说拖延症是真的严重。天气小程序只是一个应用，就目前的投入收益来说，不应该把过多的时间放在小程序上面。在加上距离开发已经过去了4个月，开发之时并没有进行文档产出，所以现在才进行复盘则是相当于重新实现了一遍。就目前来看，核心的行政区划数据合并已基本完成，所以，后面将暂停小程序相关内容，变更为GIS基础与计算机基础的学习。\n💡 对的，停更说明。\n死灰复燃 打工是不可能打工的，只要我不打工，我就有时间了 🙄","title":"微天气 终篇"},{"content":"前言 服务端部署：由于并没有建立全链路的自动化部署，目前还需要到云服务器上进行环境制作（数据库，Nginx），并拉取后端服务进行部署 小程序发布：需要先完成服务端部署，保证应用正常可用 服务端部署 数据库安装与数据初始化 最开始的时候，我是直接将在操作系统上面安装数据库，后面发现迁移的时候还是不方便，即使我可以放弃数据库中的数据，但是还是需要重新创建数据表结构。\n所以，在这一次中，我编写了一个Dockerfile脚本，使用一个空数据库作为模板，构建了postgis镜像。基于此，我可以实现快速的迁移与部署。由于这是一个实验性质的小程序，所以数据资产并不是十分重要（并不是说不安全，而是我可以丢弃），所以我可以安心的使用docker技术。在初始化postgis容器的时候，会同步释放模板以创建数据库。这将会带来一个问题是：如果重新创建容器，那么将会得到一个全新的数据库。当然你可以把数据库的data目录映射到宿主机上，应该可以解决这个问题。\ndocker build -t registry.cn-hangzhou.aliyuncs.com/fuyi-atlas/micro-weather-postgis:12-3.4 . 最后，我在本地完成该镜像的制作后，将其推送到我的阿里云镜像仓库中，便于后续使用。你可以注册一个阿里云账户去免费启用个人版的镜像仓库，也可以直接使用dockerhub。\ndocker push registry.cn-hangzhou.aliyuncs.com/fuyi-atlas/micro-weather-postgis:latest 应用部署 于本地完成镜像制作，并推送到阿里云镜像仓库中。\n为了更方便的进行部署，我编写了docker compose脚本，用于将数据库与应用服务端程序一并启动。目前程序中有一个海报分享的功能，该功能实现中需要一些额外的中文字体的支持，所以需要将此部分中文字体放置到宿主机的某个目录下，并在环境变量中指定该目录，脚本中会将该目录映射该到容器内的/usr/share/fonts目录下\n对于图片访问，还是延续此前的实现，即在服务器端生成分享海报，存储到本地（指服务器磁盘），而后通过Nginx代理访问。由于目前的服务器我是与他人共用，同时还需要配置域名证书，所以暂时没有将Nginx的部署同步放置到docker compose脚本中。\n💡 由于部署中需要提供部分敏感信息，比如：小程序的密钥、天气应用的key、docker镜像仓库地址。我将所有信息以环境变量的方式进行占位，通过docker compose的环境变量进行替换，即.env文件\n小程序发布 小程序端的发布就比较简单了\n先将base_url更换为正式环境的地址（我没有提交此部分代码，你可以自己更改） 本次调试没有问题后，就可以上传代码，即提交为体验版本 而后使用体验版本测试没有问题，就可以提交审核 审核通过后，就可以发布了 总体来看，本次发布很顺利，审核在十分钟就通过了。\n","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/008/","summary":"前言 服务端部署：由于并没有建立全链路的自动化部署，目前还需要到云服务器上进行环境制作（数据库，Nginx），并拉取后端服务进行部署 小程序发布：需要先完成服务端部署，保证应用正常可用 服务端部署 数据库安装与数据初始化 最开始的时候，我是直接将在操作系统上面安装数据库，后面发现迁移的时候还是不方便，即使我可以放弃数据库中的数据，但是还是需要重新创建数据表结构。\n所以，在这一次中，我编写了一个Dockerfile脚本，使用一个空数据库作为模板，构建了postgis镜像。基于此，我可以实现快速的迁移与部署。由于这是一个实验性质的小程序，所以数据资产并不是十分重要（并不是说不安全，而是我可以丢弃），所以我可以安心的使用docker技术。在初始化postgis容器的时候，会同步释放模板以创建数据库。这将会带来一个问题是：如果重新创建容器，那么将会得到一个全新的数据库。当然你可以把数据库的data目录映射到宿主机上，应该可以解决这个问题。\ndocker build -t registry.cn-hangzhou.aliyuncs.com/fuyi-atlas/micro-weather-postgis:12-3.4 . 最后，我在本地完成该镜像的制作后，将其推送到我的阿里云镜像仓库中，便于后续使用。你可以注册一个阿里云账户去免费启用个人版的镜像仓库，也可以直接使用dockerhub。\ndocker push registry.cn-hangzhou.aliyuncs.com/fuyi-atlas/micro-weather-postgis:latest 应用部署 于本地完成镜像制作，并推送到阿里云镜像仓库中。\n为了更方便的进行部署，我编写了docker compose脚本，用于将数据库与应用服务端程序一并启动。目前程序中有一个海报分享的功能，该功能实现中需要一些额外的中文字体的支持，所以需要将此部分中文字体放置到宿主机的某个目录下，并在环境变量中指定该目录，脚本中会将该目录映射该到容器内的/usr/share/fonts目录下\n对于图片访问，还是延续此前的实现，即在服务器端生成分享海报，存储到本地（指服务器磁盘），而后通过Nginx代理访问。由于目前的服务器我是与他人共用，同时还需要配置域名证书，所以暂时没有将Nginx的部署同步放置到docker compose脚本中。\n💡 由于部署中需要提供部分敏感信息，比如：小程序的密钥、天气应用的key、docker镜像仓库地址。我将所有信息以环境变量的方式进行占位，通过docker compose的环境变量进行替换，即.env文件\n小程序发布 小程序端的发布就比较简单了\n先将base_url更换为正式环境的地址（我没有提交此部分代码，你可以自己更改） 本次调试没有问题后，就可以上传代码，即提交为体验版本 而后使用体验版本测试没有问题，就可以提交审核 审核通过后，就可以发布了 总体来看，本次发布很顺利，审核在十分钟就通过了。","title":"微天气 小程序发布记录"},{"content":"前言 这里暂不作过多的操作，还是保持与此前一致。即通过Github Action完成Docker Image的build与push，目标仓库为阿里云容器镜像服务实例（个人版）registry.cn-hangzhou.aliyuncs.com\n那么一共分为三个部分：\nDockerfile编写 阿里云容器镜像服务配置 Github Action Dockerfile编写 jdk17 gradle FROM gradle:jdk17-alpine AS build # 设置语言，支持中文 ENV LANG C.UTF-8 COPY --chown=gradle:gradle . /opt/gradle/src WORKDIR /opt/gradle/src RUN gradle clean build -x test --no-daemon FROM eclipse-temurin:17-jdk-jammy COPY --from=build /opt/gradle/src/build/libs/*.jar /usr/app/ WORKDIR /usr/app/ RUN sh -c \u0026#39;touch micro-weather-backend-1.0.0-RELEASE.jar\u0026#39; ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;micro-weather-backend-1.0.0-RELEASE.jar\u0026#34;] Github Action 先在阿里云镜像服务中创建命名空间\n创建仓库（可选，因为可以自动创建）\n编写Github Action脚本\nname: Micro Weather Service Image Build And Push CI on: push: branches: - \u0026#39;main\u0026#39; jobs: docker: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 # setup-qemu 如果您想使用 QEMU 添加仿真支持以便能够针对更多平台进行构建，则 action 会很有用 - name: Set up QEMU uses: docker/setup-qemu-action@v1 # setup-buildx-action 将默认使用docker-container 构建器驱动程序创建和引导构建器。非必需 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to Aliyun DockerHub uses: docker/login-action@v1 with: registry: ${{secrets.ALIYUN_DOCKERHUB_REGISTRY}} username: ${{ secrets.ALIYUN_DOCKERHUB_USERNAME }} password: ${{ secrets.ALIYUN_DOCKERHUB_TOKEN }} - name: Build and push uses: docker/build-push-action@v2 with: push: true tags: registry.cn-hangzhou.aliyuncs.com/fuyi-atlas/micro-weather:latest 设置环境变量\n触发Action测试（提交代码）\n第一次失败是因为我是直接在github上面添加的action配置，通过commit提交后直接触发了action动作，但是此时还没有配置环境变量，所以失败了。所以可以在action触发前将环境变量设置好就不会失败了。\n参考文档 在github上使用workflow构建docker镜像并推送阿里云 💡 Github Action脚本内容来自于网络，存在少许更改。但由于记不得来源，所以没有做出记录\n","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/007/","summary":"前言 这里暂不作过多的操作，还是保持与此前一致。即通过Github Action完成Docker Image的build与push，目标仓库为阿里云容器镜像服务实例（个人版）registry.cn-hangzhou.aliyuncs.com\n那么一共分为三个部分：\nDockerfile编写 阿里云容器镜像服务配置 Github Action Dockerfile编写 jdk17 gradle FROM gradle:jdk17-alpine AS build # 设置语言，支持中文 ENV LANG C.UTF-8 COPY --chown=gradle:gradle . /opt/gradle/src WORKDIR /opt/gradle/src RUN gradle clean build -x test --no-daemon FROM eclipse-temurin:17-jdk-jammy COPY --from=build /opt/gradle/src/build/libs/*.jar /usr/app/ WORKDIR /usr/app/ RUN sh -c \u0026#39;touch micro-weather-backend-1.0.0-RELEASE.jar\u0026#39; ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;micro-weather-backend-1.0.0-RELEASE.jar\u0026#34;] Github Action 先在阿里云镜像服务中创建命名空间\n创建仓库（可选，因为可以自动创建）\n编写Github Action脚本\nname: Micro Weather Service Image Build And Push CI on: push: branches: - \u0026#39;main\u0026#39; jobs: docker: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 # setup-qemu 如果您想使用 QEMU 添加仿真支持以便能够针对更多平台进行构建，则 action 会很有用 - name: Set up QEMU uses: docker/setup-qemu-action@v1 # setup-buildx-action 将默认使用docker-container 构建器驱动程序创建和引导构建器。非必需 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to Aliyun DockerHub uses: docker/login-action@v1 with: registry: ${{secrets.","title":"微天气 Github Action镜像自动构建与推送"},{"content":"前言 此前提到微天气应用程序需要使用到行政区划数据，不过上一章所使用的数据来源于网络，或多或少都可以考虑一下是否还有其他获取的方式，所以也就有了本文的内容。\n在这里，将基于全国1:100万基础地理信息数据进行行政区划数据的提取。本文用于记录使用程序实现全国1:100万基础地理信息数据合并的全过程。\n当然，本文产生的最重要原因其实并不是受到微天气的启发，更多的是个人想试一试能不能用。\n数据说明 全国1：100万公众版基础地理信息数据（2021）覆盖全国陆地范围和包括台湾岛、海南岛、钓鱼岛、南海诸岛在内的主要岛屿及其临近海域，共77幅1:100万图幅，该数据集整体现势性为2019年。数据采用2000国家大地坐标系，1985国家高程基准，经纬度坐标。\n为满足广大社会群众对地理信息数据资源的需求，经自然资源部授权，全国地理信息资源目录服务系统提供全国1:100万全图层要素免费下载的服务。下载数据采用1：100万标准图幅分发，内容包括水系、居民地及设施、交通、管线、境界与政区、地貌与土质、植被、地名及注记9个数据集，且保存要素间空间关系和相关属性信息。\n💡 提供下载的是矢量数据，不是最终地图，与符号化后的地图再可视化表达上存在一定差异。用户利用此数据编制地图，应当严格执行《地图管理条例》有关规定；编制的地图如需向社会公开的，还应当依法履行地图审核程序。\n成果规格 分幅编号及范围\n1：100万公众版基础地理信息数据（2021）的图幅总数为77幅，分幅数据按照GB/T 13989-2012《国家基本比例尺地形图分幅和编号》执行。空间存储单元为6°（经差）×4°（纬差）。\n坐标系统\n平面坐标系： 2000国家大地坐标系。 高程基准：1985国家高程基准。 地图投影：分幅数据采用地理坐标，坐标单位为度。 几何精度\n更新后地物点对于附近野外控制点的平面位置和高程中误差符合下表的要求，以两倍中误差值为最大误差。\n地物点误差 最小 最大 平面位置 100 500 高程 50 200 现势性\n1:100万地形数据现势性与更新使用的数据源的现势性一致，数据整体现势性达到2019年。\n成果数据组织 全国1：100万公众版地形数据（2021）内容包括水系、居民地及设施、交通、管线、境界与政区、地貌与土质、植被、地名及注记9个数据集。\n数据分层的命名采用四个字符，第一个字符代表数据分类，第二三个字符是数据内容的缩写，第四个字符代表几何类型。\n目标 实现分图层合并（处理图幅合并时接边问题） 水系（暂缓） 交通（暂缓） 境界与政区（国、省、市、县） 地名及注记 根据合并后的行政区划与地名注记，制作行政区划数据库 数据库使用PostgreSQL（PostGIS） 设计 图层解析程序 Java程序， 使用GDAL 读取GDB\n逻辑 根据《国家基本比例尺地形图分幅和编号》规定可知网格范围，通过网格范围动态生成对应网格的分幅编号，并以该编号进行数据检索。如果命中则根据成果数据组织规格以及相关标准对数据进行解析，如果未命中则跳过，直至网格扫描完毕。\n经度：72~138（E），43~53（11） 纬度：0~56（N），A~N（14） 即，网格范围:[43,53] x [A,N] 数据的处理流程可使用责任链模式进行，后续也方便加入其他的处理流程。即，整体的执行框架为策略模式+责任链模式。为统一策略选择模型，在此提出图层定义（LayerDefinition）的概念。\nLayerDefinition由如下几个关键要素组成：\n图层数据源（LayerSource） driver：驱动，参考java.sql.UnWrapper实现 instance name catalog： schema： table： commonDefinitionKey ：常规定义缓存的key fieldDefinitionKey ：字段定义缓存的key featureCarrierKey ：要素载体缓存的key origin：数据来源（分幅文件路径） scale：比例尺（如：1000000，表示1:1000000） sourceSpatialRef：源坐标系（表现形式可为标准ID、PROJ Text、WKT Text） sinkSpatialRef：目标坐标系（表现形式可为标准ID、PROJ Text、WKT Text） featureCode：要素分类码，对应成果数据组织中的要素分类（如：C、B） name：图层命名，也是图层分类码 layerCode：图层分类码 release：释放格式（比如：WMTS、Shapefile、GDB…） 描述字符为：比例尺:源坐标系:目标坐标系:要素分类码:图层名称:释放格式，在描述字符串中，坐标系仅使用标准ID表示\n💡 源坐标系、要素分类码、图层名称均来自于源数据。当且仅当原始数据中无法获取到源坐标系定义时，源坐标系定义可来自于配置。\n基本流程 扫描网格定义（可配置）\nGDB读取（支持zip，并推荐使用zip）\n/home/fuyi/GeoDatabase/China_Basic_1-100/J50.gdb /home/fuyi/GeoDatabase/China_Basic_1-100/J50.gdb.zip 策略管理器（StrategyManager），职责：根据配置选择具体策略方案，并调用执行\n策略（Strategy），持有相应的处理器对象，责任链由此开始启动\n总体上分为三个阶段\n1、图层归一（数据、位置、坐标系） 2、图层合并 3、图层释放（service、dump、nothing） Layer normalization (data, location, coordinate system) Layer merging Layer release (service, dump, nothing) 归一化处理阶段（应保证空间参考归一、图层数据存储归一，即应保持整体一致） 合并处理阶段（或者称之为中间处理阶段，在这里不仅可以处理合并，还可以做其他的事情） 释放阶段（数据的处理总归是有目的存在的，那么本阶段就是确定数据最终存在或服务的形式，比如导出为Shapefile文件、写入PostGIS数据库或通过WMTS服务对外提供数据访问等） GDAL策略：默认策略，即全程基本上使用GDAL处理\n匹配获取指定图层处理器（归一化），职责为：解析\n先写缓存（redis）\nsupport（根据图层命名），如：boua，该值作为处理器的判断条件 可提供转换处理器，职责为：坐标系转换\n坐标系转换既可以在归一化处理阶段进行（保证不同空间参考的数据转入同一空间参考），也可以在中间处理阶段（统一进行数据转换）或是释放阶段（根据释放需求进行处理）\n提供数据合并处理器，职责为：跨图幅的图层合并\n处理逻辑，根据给定的关联列进行空间对象合并（如：在行政境界中使用行政区划编码以及名称进行空间合并）\n不同图层可以具有不同的合并规则，由不同的实例进行提供对应的合并行为\n释放逻辑也通过定义格式处理器完成处理，职责：格式处理\n需指明格式与比例尺 support，根据格式与比例尺，如：postgis:1000000，该值作为处理器的判断条件 不同比例尺创建不同的存储名称。如，存储格式为PostGIS：\n表命名规则：图层名称_比例尺，小写\n如：boua_1000000，意为1：1000000比例尺下boua数据层数据\n💡 LayerDefine结构可作为所有策略选择的入参，内部则根据自己依赖的条件进行决策。\nDriver(register) → GDALDriverManager → DataSet → Layer → Feature → Geometry\nRedis缓存作为转换过程中的数据存储与交换容器\n图层定义拆分为两个部分进行存储，常规内容使用hash结构存储，便于进行变更 prefix key : feature code : layer code : definition\nscale sourceSpatialRef sinkSpatialRef storage 属性定义使用set结构存储，便于序列化与反序列化\nprefix key : feature code : layer code : definition : fields\nfieldDefinitions 图层数据则使用list数据结构进行储存，便于序列化与反序列化，同时便于数据追加 prefix key : feature code : layer code : features\nfeatures 结构拆解图 开发 环境准备 操作系统：Debian 11 bookworm（testing） IDE：IntelliJ IDEA 2022.1.4 (Community Edition) PostgreSQL：14（13+即可） JDK：openjdk version \u0026ldquo;11\u0026rdquo; 2018-09-25 Maven：Apache Maven 3.8.6 GDAL：GDAL 3.5.1, released 2022/06/30 GDAL环境准备 debian 11 testing，自行编译gdal程序\n/debian 11 testing，已经自行安装了gdal程序，但是没有java binding模块，所以还是需要自行编译。\n我今天用的是Fedora 38 workstation，可以直接 install gdal, gdal-java，而后将libgdalalljni.so文件拷贝到/usr/java/packages/lib 目录下（如果不存在，请自行创建）\nsudo mkdir -p /usr/java/packages/lib \u0026amp; sudo ln -s /usr/lib/java/gdal/libgdalalljni.so /usr/java/packages/lib/libgdalalljni.so\n@time: 2023.10.08\n异常 下图是经过pac,name合并后的数据（并去除了境外与海洋数据），如图可见，仍然存在些许裂缝\n经过排查发现，发现部分原始图副之间便存在裂缝，且该裂缝的两边不一定是平行的。那么这就会导致合并出现两种情况\n待合并的多边形之间存在裂缝，且多边形互相不相交\n也就是隔海相望的那一种，此种情况是可以通过后续手段修复的，不过也是只有在裂缝的定义存在于合并后的多边形中（只不过可能被定义为洞）的情况下，如果不在的话，也不知道应该如何出来\n待合并的多边形之间存在裂缝，且多边形存在相交，并且交点在裂缝之上\n这种形式便不知道应该怎么处理了。\n💡 感觉本质上，是一个找到裂缝的过程，并将裂缝作为内容的一部分进行合并。但是如何确定裂缝的范围也是不容易的，这可能和多边形间相互位置关系有关系\n成果 归一化 即将分离的图幅数据提取到统一位置存放，坐标本来就是统一的，不需要额外操作\n去除无效区域与数据 去除境外数据（pac \u0026lt; 100000） 去除海域面数据（pac = 250100） 去除空间无效数据（ST_IsValid(geometry)==false） 合并处理 即进行图幅合并（使用pac与name）\n💡 从此处可以发现，部分区域存在裂缝。 1、pac=632722\n2、pac=140404\n3、pac=140427\n4、pac=520625\n5、pac=520628\n我同时对比了QGIS、ArcGIS中提供的Dissolve工具的融合效果（使用pac与name），就都存在裂缝情况上： 合并后的数量都是：2900\narcgis 优于 wukong 优于 qgis\nArcGIS QGIS WuKong Source Code wukong-github\n小结 因做境界合并，开发了WuKong程序，但是其含义并不仅仅只是进行境界合并，他是一个基于国家开放的基础地理信息数据的（目前是基于1：100玩基础地理信息数据），可以进行归一、转换、释放的通用程序。\n虽然目前仅完成了基础境界的合并（且存在瑕疵，但好歹也和QGIS的实现差不多不是），但这仅仅只是开始，还有更多的内容可以实现。\n参考 全国地理信息资源目录服务系统（https://www.webmap.cn/） 数据来源于全国地理信息资源目录服务系统：www.webmap.cn\nGB/T 13989-2012 国家基本比例尺地形图分幅和编号 1：1000000地形图的分幅 经度：72~138（E），43~53（11）\n纬度：0~56（N），A~N（14）\nGB/T 33183-2016 基础地理信息 1:50 000地形要素数据规范 1:50000地形要素属性表结构 境界与政区 地名 1:50000地形要素数据要素内容与选取指标 走天涯徐小洋公众号 2021版国家基础地理数据库批量下载与拼接方法\n【资源分享】如何查找靠谱的国标，全文免费看！全文免费看！全文免费看！\n1：100万基础地理数据库怎么看？怎么用？能干嘛？\nGDAL vector-openfilegdb Vector Data Model - GDAL documentation\nESRI File Geodatabase (OpenFileGDB) - GDAL documentation\nJava bindings - GDAL documentation\nOverview (GDAL/OGR 3.5.0 Java bindings API)\nOther https://github.com/modood/Administrative-divisions-of-China\nST_Centroid\nConda Conda - Conda documentation\nconda的安装与使用 2.0版（2022-08-12更新）\n","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/006/","summary":"前言 此前提到微天气应用程序需要使用到行政区划数据，不过上一章所使用的数据来源于网络，或多或少都可以考虑一下是否还有其他获取的方式，所以也就有了本文的内容。\n在这里，将基于全国1:100万基础地理信息数据进行行政区划数据的提取。本文用于记录使用程序实现全国1:100万基础地理信息数据合并的全过程。\n当然，本文产生的最重要原因其实并不是受到微天气的启发，更多的是个人想试一试能不能用。\n数据说明 全国1：100万公众版基础地理信息数据（2021）覆盖全国陆地范围和包括台湾岛、海南岛、钓鱼岛、南海诸岛在内的主要岛屿及其临近海域，共77幅1:100万图幅，该数据集整体现势性为2019年。数据采用2000国家大地坐标系，1985国家高程基准，经纬度坐标。\n为满足广大社会群众对地理信息数据资源的需求，经自然资源部授权，全国地理信息资源目录服务系统提供全国1:100万全图层要素免费下载的服务。下载数据采用1：100万标准图幅分发，内容包括水系、居民地及设施、交通、管线、境界与政区、地貌与土质、植被、地名及注记9个数据集，且保存要素间空间关系和相关属性信息。\n💡 提供下载的是矢量数据，不是最终地图，与符号化后的地图再可视化表达上存在一定差异。用户利用此数据编制地图，应当严格执行《地图管理条例》有关规定；编制的地图如需向社会公开的，还应当依法履行地图审核程序。\n成果规格 分幅编号及范围\n1：100万公众版基础地理信息数据（2021）的图幅总数为77幅，分幅数据按照GB/T 13989-2012《国家基本比例尺地形图分幅和编号》执行。空间存储单元为6°（经差）×4°（纬差）。\n坐标系统\n平面坐标系： 2000国家大地坐标系。 高程基准：1985国家高程基准。 地图投影：分幅数据采用地理坐标，坐标单位为度。 几何精度\n更新后地物点对于附近野外控制点的平面位置和高程中误差符合下表的要求，以两倍中误差值为最大误差。\n地物点误差 最小 最大 平面位置 100 500 高程 50 200 现势性\n1:100万地形数据现势性与更新使用的数据源的现势性一致，数据整体现势性达到2019年。\n成果数据组织 全国1：100万公众版地形数据（2021）内容包括水系、居民地及设施、交通、管线、境界与政区、地貌与土质、植被、地名及注记9个数据集。\n数据分层的命名采用四个字符，第一个字符代表数据分类，第二三个字符是数据内容的缩写，第四个字符代表几何类型。\n目标 实现分图层合并（处理图幅合并时接边问题） 水系（暂缓） 交通（暂缓） 境界与政区（国、省、市、县） 地名及注记 根据合并后的行政区划与地名注记，制作行政区划数据库 数据库使用PostgreSQL（PostGIS） 设计 图层解析程序 Java程序， 使用GDAL 读取GDB\n逻辑 根据《国家基本比例尺地形图分幅和编号》规定可知网格范围，通过网格范围动态生成对应网格的分幅编号，并以该编号进行数据检索。如果命中则根据成果数据组织规格以及相关标准对数据进行解析，如果未命中则跳过，直至网格扫描完毕。\n经度：72~138（E），43~53（11） 纬度：0~56（N），A~N（14） 即，网格范围:[43,53] x [A,N] 数据的处理流程可使用责任链模式进行，后续也方便加入其他的处理流程。即，整体的执行框架为策略模式+责任链模式。为统一策略选择模型，在此提出图层定义（LayerDefinition）的概念。\nLayerDefinition由如下几个关键要素组成：\n图层数据源（LayerSource） driver：驱动，参考java.sql.UnWrapper实现 instance name catalog： schema： table： commonDefinitionKey ：常规定义缓存的key fieldDefinitionKey ：字段定义缓存的key featureCarrierKey ：要素载体缓存的key origin：数据来源（分幅文件路径） scale：比例尺（如：1000000，表示1:1000000） sourceSpatialRef：源坐标系（表现形式可为标准ID、PROJ Text、WKT Text） sinkSpatialRef：目标坐标系（表现形式可为标准ID、PROJ Text、WKT Text） featureCode：要素分类码，对应成果数据组织中的要素分类（如：C、B） name：图层命名，也是图层分类码 layerCode：图层分类码 release：释放格式（比如：WMTS、Shapefile、GDB…） 描述字符为：比例尺:源坐标系:目标坐标系:要素分类码:图层名称:释放格式，在描述字符串中，坐标系仅使用标准ID表示","title":"微天气—行政区划数据（二）"},{"content":"前言 微天气程序中存在如下几个功能需要使用到行政区划数据：\n城市列表，需要支持城市搜索 根据经纬度获区域（城市）的天气数据 地图坐标拾取并获取所处区域（城市）信息，同时获取天气数据 对于城市的天气数据，不使用和风天气的城市列表，而是自行维护，通过空间位置（经纬度）进行关联。对于城市位置的定义，本可以选择如行政中心或市中心，但我没有这样的数据，就直接用城市区划范围的中心点代替。不准确不重要，过程已经满足了，且后续是可以替换的。\n其实完全可以使用官方提供的城市数据和GeoAPI覆盖这些功能，但既然我是做GIS开发的，而且手里也有可以用作研究学习的数据，为啥不用呢。\n今天突然发现，其实我可以爬一下和风的数据，这样就可以拿到城市选择的经纬度数据了 @time 2023.10.10\nAbout 行政区划解析程序，输入shape文件，写入Postgresql.\n大体完整的四级行政区划数据组织\ngithub link: fuyi-district-parse\n数据情况 数据原源于网络，由于时间久远，我已经忘记了是如何获取到的\n名词解释 省级行政区 中国的一级行政区，或称国家一级行政区或省级行政区，是指直属中央政府管辖的行政区划，在历史上曾有不同的称呼。如：省、自治区、直辖市、特别行政区。\n地级行政区 地级行政区即“地区级别行政区”，是现行中华人民共和国行政区划中常规的第二级行政区划单位，包括地级市、地区、盟、自治州等。地级行政区隶属于省、自治区、直辖市等省级行政区之下；下辖若干个县、区、县级市、旗等县级行政区。作为特例，东莞市、中山市、嘉峪关市、儋州市等四个地级市下辖街道办事处与乡镇，不辖县、区，因此也称作“直筒子（地级）市”。地级行政区的级别为正厅级，所以非正厅级的省直辖的行政区划不算作地级行政区，例如：湖北省辖的仙桃市、天门市、潜江市；河南省辖的济源市等等。直辖市下辖的区，虽然是正厅级，但未列入地级行政区的统计。\n县级行政区 县为中华人民共和国行政区划单位之一，县级行政区指行政地位与“县”相同的行政区划单位的总称，其管辖乡级行政区。为乡、镇的上一级行政区划单位。中华人民共和国成立后，随着行政督察区名称的变更，除各直辖市均隶属于专区（行政督察专区）、地区或地级行政区，现除各直辖市、海南省直管县外均为地级行政区的下一级行政区。\n按省、县、乡三级行政区划制度划分，县级行政区属于第二级行政区，为直辖市的下级行政区划单位。 按省、地、县、乡四级行政区划制度划分，县级行政区属于第三级行政区，属于省、自治区所辖地级行政区的下级行政区划单位。 乡级行政区 乡，中华人民共和国现行基层行政区划单位，区划层次介于县与村之间。“乡”为县、县级市下的主要行政区划类型之一。中国行政区划史上，“乡”一直为县的行政区划单元，因此现行处同一层次的区划单位归入乡级行政区。中国自改革开放以来，由于城市的快速扩张，行政区划制度出现了大的变革。1980年代以后“乡改镇”、“乡改街道”的现象越来越普遍。\n在乡级行政区划中，乡（包括镇）设有一级人民政府，属于基层政权；乡的行政区划单位为村（含民族村）。但很多乡设有社区，乡的区划单位设置与镇、街道看不出实质性差异。\n目的 解析所有数据文件，实现最终入库\n使用GeoTools实现\n数据库表结构：\n行政区划信息（district_info）\nid：自增Id（bigserial） name：行政区划名称 grade：行政区划等级（省级行政区：1， 地级行政区：2， 县级行政区：3，） code：行政区代码 center_point：中心点（geometry::point） bounds：行政区边界（geometry） 注：\n数据入库前审查，保证行政区代码唯一 使用grade区分省、市、县、乡镇 对于省、市、县code列，统一进行前6位截取（不满6位字符所在数据，直接丢弃），对于乡镇则统一进行前9位进行截取（不满9位字符所在数据，直接丢弃） 省级行政区 存在错乱数据，可以使用行政区代码识别（adcode）\n地级行政区 存在错乱数据，可以使用行政区代码识别（code），需要截取前6位\n县级行政区 存在错乱数据，可以使用行政区代码识别（code），需要截取前6位\n乡级行政区 成果 记录数：46652\n点数据为各个行政区中心点\n数据表 PostGIS CREATE EXTENSION IF NOT EXISTS postgis WITH SCHEMA public; 行政区划 -- -- Name: district_info_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres -- CREATE SEQUENCE public.district_info_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1; ALTER TABLE public.district_info_id_seq OWNER TO postgres; -- Table: public.district_info -- DROP TABLE IF EXISTS public.district_info; CREATE TABLE IF NOT EXISTS public.district_info ( name character varying COLLATE pg_catalog.\u0026#34;default\u0026#34;, grade integer, code character varying COLLATE pg_catalog.\u0026#34;default\u0026#34;, center_point geometry(Point,4326), bounds geometry(MultiPolygon,4326), create_time timestamp without time zone DEFAULT CURRENT_TIMESTAMP, uid character varying COLLATE pg_catalog.\u0026#34;default\u0026#34;, id bigint NOT NULL DEFAULT nextval(\u0026#39;district_info_id_seq\u0026#39;::regclass), CONSTRAINT district_info_pkey PRIMARY KEY (id) ) TABLESPACE pg_default; ALTER TABLE IF EXISTS public.district_info OWNER to postgres; COMMENT ON TABLE public.district_info IS \u0026#39;行政区划信息表\u0026#39;; COMMENT ON COLUMN public.district_info.name IS \u0026#39;行政区名称\u0026#39;; COMMENT ON COLUMN public.district_info.grade IS \u0026#39;行政区等级，目前支持：（1：省级行政区，2；市级行政区，3：县级行政区，4：乡级行政区）\u0026#39;; COMMENT ON COLUMN public.district_info.code IS \u0026#39;行政区编码。其中，县级与县级以上行政区编码为6位，县级以下（即乡级）行政区编码为9位\u0026#39;; COMMENT ON COLUMN public.district_info.center_point IS \u0026#39;行政区中心点，数据坐标系：4326\u0026#39;; COMMENT ON COLUMN public.district_info.bounds IS \u0026#39;行政区边界，数据坐标系：4326\u0026#39;; COMMENT ON COLUMN public.district_info.uid IS \u0026#39;唯一标识，便于数据迁移\u0026#39;; -- -- Name: district_info_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres -- ALTER SEQUENCE public.district_info_id_seq OWNED BY public.district_info.id; 日志 ... org.fuyi.district.DistrictParseApplication /home/fuyi/GeoDatabase/China ------------------- /home/fuyi/GeoDatabase/China/重庆市/重庆市_市界.shp current element size: 12 ------------------- /home/fuyi/GeoDatabase/China/重庆市/重庆市_省界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/重庆市/重庆市_县界.shp current element size: 75 ------------------- /home/fuyi/GeoDatabase/China/重庆市/重庆市_乡镇边界.shp current element size: 1218 ------------------- /home/fuyi/GeoDatabase/China/新疆自治区/新疆自治区_省界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/新疆自治区/新疆自治区_县界.shp current element size: 111 ------------------- /home/fuyi/GeoDatabase/China/新疆自治区/新疆自治区_乡镇边界.shp current element size: 1410 ------------------- /home/fuyi/GeoDatabase/China/新疆自治区/新疆自治区_市界.shp current element size: 28 ------------------- /home/fuyi/GeoDatabase/China/湖北省/湖北省_市界.shp current element size: 28 ------------------- /home/fuyi/GeoDatabase/China/湖北省/湖北省_省界.shp current element size: 6 ------------------- /home/fuyi/GeoDatabase/China/湖北省/湖北省_县界.shp current element size: 150 ------------------- /home/fuyi/GeoDatabase/China/湖北省/湖北省_乡镇边界.shp current element size: 1679 ------------------- /home/fuyi/GeoDatabase/China/陕西省/陕西省_市界.shp current element size: 24 ------------------- /home/fuyi/GeoDatabase/China/陕西省/陕西省_县界.shp current element size: 158 ------------------- /home/fuyi/GeoDatabase/China/陕西省/陕西省_省界.shp current element size: 7 ------------------- /home/fuyi/GeoDatabase/China/陕西省/陕西省_乡镇边界.shp current element size: 1559 ------------------- /home/fuyi/GeoDatabase/China/广东省/广东省_省界.shp current element size: 7 ------------------- /home/fuyi/GeoDatabase/China/广东省/广东省_县界.shp current element size: 160 ------------------- /home/fuyi/GeoDatabase/China/广东省/广东省_市界.shp current element size: 31 ------------------- /home/fuyi/GeoDatabase/China/广东省/广东省_乡镇边界.shp current element size: 1902 ------------------- /home/fuyi/GeoDatabase/China/云南省/云南省_乡镇边界.shp current element size: 1594 ------------------- /home/fuyi/GeoDatabase/China/云南省/云南省_省界.shp current element size: 5 ------------------- /home/fuyi/GeoDatabase/China/云南省/云南省_县界.shp current element size: 165 ------------------- /home/fuyi/GeoDatabase/China/云南省/云南省_市界.shp current element size: 27 ------------------- /home/fuyi/GeoDatabase/China/宁夏省/宁夏省_市界.shp current element size: 12 ------------------- /home/fuyi/GeoDatabase/China/宁夏省/宁夏省_乡镇边界.shp current element size: 326 ------------------- /home/fuyi/GeoDatabase/China/宁夏省/宁夏省_县界.shp current element size: 36 ------------------- /home/fuyi/GeoDatabase/China/宁夏省/宁夏省_省界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/内蒙古自治区/内蒙古自治区_乡镇边界.shp current element size: 1498 ------------------- /home/fuyi/GeoDatabase/China/内蒙古自治区/内蒙古自治区_市界.shp current element size: 35 ------------------- /home/fuyi/GeoDatabase/China/内蒙古自治区/内蒙古自治区_省界.shp current element size: 7 ------------------- /home/fuyi/GeoDatabase/China/内蒙古自治区/内蒙古自治区_县界.shp current element size: 173 ------------------- /home/fuyi/GeoDatabase/China/海南省/海南省_县界.shp current element size: 27 ------------------- /home/fuyi/GeoDatabase/China/海南省/海南省_省界.shp current element size: 1 ------------------- /home/fuyi/GeoDatabase/China/海南省/海南省_市界.shp current element size: 19 ------------------- /home/fuyi/GeoDatabase/China/海南省/海南省_乡镇边界.shp current element size: 214 ------------------- /home/fuyi/GeoDatabase/China/山东省/山东省_省界.shp current element size: 5 ------------------- /home/fuyi/GeoDatabase/China/山东省/山东省_乡镇边界.shp current element size: 1984 ------------------- /home/fuyi/GeoDatabase/China/山东省/山东省_市界.shp current element size: 27 ------------------- /home/fuyi/GeoDatabase/China/山东省/山东省_县界.shp current element size: 167 ------------------- /home/fuyi/GeoDatabase/China/浙江省/浙江省_省界.shp current element size: 6 ------------------- /home/fuyi/GeoDatabase/China/浙江省/浙江省_市界.shp current element size: 18 ------------------- /home/fuyi/GeoDatabase/China/浙江省/浙江省_县界.shp current element size: 110 ------------------- /home/fuyi/GeoDatabase/China/浙江省/浙江省_乡镇边界.shp current element size: 1431 ------------------- /home/fuyi/GeoDatabase/China/青海省/青海省_省界.shp current element size: 5 ------------------- /home/fuyi/GeoDatabase/China/青海省/青海省_乡镇边界.shp current element size: 470 ------------------- /home/fuyi/GeoDatabase/China/青海省/青海省_县界.shp current element size: 72 ------------------- /home/fuyi/GeoDatabase/China/青海省/青海省_市界.shp current element size: 19 ------------------- /home/fuyi/GeoDatabase/China/江苏省/江苏省_乡镇边界.shp current element size: 1634 ------------------- /home/fuyi/GeoDatabase/China/江苏省/江苏省_省界.shp current element size: 5 ------------------- /home/fuyi/GeoDatabase/China/江苏省/江苏省_市界.shp current element size: 26 ------------------- /home/fuyi/GeoDatabase/China/江苏省/江苏省_县界.shp current element size: 138 ------------------- /home/fuyi/GeoDatabase/China/江西省/江西省_市界.shp current element size: 27 ------------------- /home/fuyi/GeoDatabase/China/江西省/江西省_乡镇边界.shp current element size: 1927 ------------------- /home/fuyi/GeoDatabase/China/江西省/江西省_县界.shp current element size: 137 ------------------- /home/fuyi/GeoDatabase/China/江西省/江西省_省界.shp current element size: 7 ------------------- /home/fuyi/GeoDatabase/China/西藏自治区/西藏自治区_县界.shp current element size: 91 ------------------- /home/fuyi/GeoDatabase/China/西藏自治区/西藏自治区_省界.shp current element size: 5 ------------------- /home/fuyi/GeoDatabase/China/西藏自治区/西藏自治区_市界.shp current element size: 14 ------------------- /home/fuyi/GeoDatabase/China/西藏自治区/西藏自治区_乡镇边界.shp current element size: 737 ------------------- /home/fuyi/GeoDatabase/China/天津市/天津市_县界.shp current element size: 32 ------------------- /home/fuyi/GeoDatabase/China/天津市/天津市_省界.shp current element size: 2 ------------------- /home/fuyi/GeoDatabase/China/天津市/天津市_市界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/天津市/天津市_乡镇边界.shp current element size: 360 ------------------- /home/fuyi/GeoDatabase/China/台湾省/台湾省_村名.shp current element size: 0 ------------------- /home/fuyi/GeoDatabase/China/台湾省/台湾省_乡镇名称.shp current element size: 0 ------------------- /home/fuyi/GeoDatabase/China/台湾省/台湾省_县名称.shp current element size: 0 ------------------- /home/fuyi/GeoDatabase/China/广西省/广西省_省界.shp current element size: 3 ------------------- /home/fuyi/GeoDatabase/China/广西省/广西省_市界.shp current element size: 25 ------------------- /home/fuyi/GeoDatabase/China/广西省/广西省_县界.shp current element size: 142 ------------------- /home/fuyi/GeoDatabase/China/广西省/广西省_乡镇边界.shp current element size: 1433 ------------------- /home/fuyi/GeoDatabase/China/香港特别行政区/香港特别行政区_乡镇边界.shp current element size: 8 ------------------- /home/fuyi/GeoDatabase/China/香港特别行政区/香港特别行政区_县界.shp current element size: 21 ------------------- /home/fuyi/GeoDatabase/China/香港特别行政区/香港特别行政区_省界.shp current element size: 2 ------------------- /home/fuyi/GeoDatabase/China/香港特别行政区/香港特别行政区_市界.shp current element size: 1 ------------------- /home/fuyi/GeoDatabase/China/安徽省/安徽省_市界.shp current element size: 34 ------------------- /home/fuyi/GeoDatabase/China/安徽省/安徽省_乡镇边界.shp current element size: 1832 ------------------- /home/fuyi/GeoDatabase/China/安徽省/安徽省_省界.shp current element size: 6 ------------------- /home/fuyi/GeoDatabase/China/安徽省/安徽省_县界.shp current element size: 149 ------------------- /home/fuyi/GeoDatabase/China/河南省/河南省_市界.shp current element size: 38 ------------------- /home/fuyi/GeoDatabase/China/河南省/河南省_乡镇边界.shp current element size: 2722 ------------------- /home/fuyi/GeoDatabase/China/河南省/河南省_县界.shp current element size: 209 ------------------- /home/fuyi/GeoDatabase/China/河南省/河南省_省界.shp current element size: 7 ------------------- /home/fuyi/GeoDatabase/China/黑龙江省/黑龙江省_市界.shp current element size: 20 ------------------- /home/fuyi/GeoDatabase/China/黑龙江省/黑龙江省_省界.shp current element size: 3 ------------------- /home/fuyi/GeoDatabase/China/黑龙江省/黑龙江省_县界.shp current element size: 150 ------------------- /home/fuyi/GeoDatabase/China/黑龙江省/黑龙江省_乡镇边界.shp current element size: 1874 ------------------- /home/fuyi/GeoDatabase/China/澳门特别行政区/澳门特别行政区_市界.shp current element size: 1 ------------------- /home/fuyi/GeoDatabase/China/澳门特别行政区/澳门特别行政区_省界.shp current element size: 2 ------------------- /home/fuyi/GeoDatabase/China/澳门特别行政区/澳门特别行政区_乡镇边界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/澳门特别行政区/澳门特别行政区_县界.shp current element size: 9 ------------------- /home/fuyi/GeoDatabase/China/吉林省/吉林省_省界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/吉林省/吉林省_县界.shp current element size: 82 ------------------- /home/fuyi/GeoDatabase/China/吉林省/吉林省_乡镇边界.shp current element size: 1134 ------------------- /home/fuyi/GeoDatabase/China/吉林省/吉林省_市界.shp current element size: 19 ------------------- /home/fuyi/GeoDatabase/China/北京市/北京市_县界.shp current element size: 32 ------------------- /home/fuyi/GeoDatabase/China/北京市/北京市_乡镇边界.shp current element size: 395 ------------------- /home/fuyi/GeoDatabase/China/北京市/北京市_市界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/北京市/北京市_省界.shp current element size: 3 ------------------- /home/fuyi/GeoDatabase/China/四川省/四川省_乡镇边界.shp current element size: 4933 ------------------- /home/fuyi/GeoDatabase/China/四川省/四川省_市界.shp current element size: 32 ------------------- /home/fuyi/GeoDatabase/China/四川省/四川省_省界.shp current element size: 7 ------------------- /home/fuyi/GeoDatabase/China/四川省/四川省_县界.shp current element size: 237 ------------------- /home/fuyi/GeoDatabase/China/上海市/上海市_省界.shp current element size: 3 ------------------- /home/fuyi/GeoDatabase/China/上海市/上海市_乡镇边界.shp current element size: 251 ------------------- /home/fuyi/GeoDatabase/China/上海市/上海市_县界.shp current element size: 24 ------------------- /home/fuyi/GeoDatabase/China/上海市/上海市_市界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/甘肃省/甘肃省_县界.shp current element size: 137 ------------------- /home/fuyi/GeoDatabase/China/甘肃省/甘肃省_市界.shp current element size: 33 ------------------- /home/fuyi/GeoDatabase/China/甘肃省/甘肃省_省界.shp current element size: 7 ------------------- /home/fuyi/GeoDatabase/China/甘肃省/甘肃省_乡镇边界.shp current element size: 1601 ------------------- /home/fuyi/GeoDatabase/China/福建省/福建省_市界.shp current element size: 18 ------------------- /home/fuyi/GeoDatabase/China/福建省/福建省_乡镇边界.shp current element size: 1272 ------------------- /home/fuyi/GeoDatabase/China/福建省/福建省_省界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/福建省/福建省_县界.shp current element size: 109 ------------------- /home/fuyi/GeoDatabase/China/河北省/河北省_市界.shp current element size: 26 ------------------- /home/fuyi/GeoDatabase/China/河北省/河北省_乡镇边界.shp current element size: 2574 ------------------- /home/fuyi/GeoDatabase/China/河北省/河北省_省界.shp current element size: 8 ------------------- /home/fuyi/GeoDatabase/China/河北省/河北省_县界.shp current element size: 225 ------------------- /home/fuyi/GeoDatabase/China/贵州省/贵州省_乡镇边界.shp current element size: 1791 ------------------- /home/fuyi/GeoDatabase/China/贵州省/贵州省_县界.shp current element size: 122 ------------------- /home/fuyi/GeoDatabase/China/贵州省/贵州省_市界.shp current element size: 16 ------------------- /home/fuyi/GeoDatabase/China/贵州省/贵州省_省界.shp current element size: 5 ------------------- /home/fuyi/GeoDatabase/China/湖南省/湖南省_省界.shp current element size: 6 ------------------- /home/fuyi/GeoDatabase/China/湖南省/湖南省_市界.shp current element size: 28 ------------------- /home/fuyi/GeoDatabase/China/湖南省/湖南省_乡镇边界.shp current element size: 2662 ------------------- /home/fuyi/GeoDatabase/China/湖南省/湖南省_县界.shp current element size: 169 ------------------- /home/fuyi/GeoDatabase/China/山西省/山西省_县界.shp current element size: 154 ------------------- /home/fuyi/GeoDatabase/China/山西省/山西省_市界.shp current element size: 28 ------------------- /home/fuyi/GeoDatabase/China/山西省/山西省_乡镇边界.shp current element size: 1580 ------------------- /home/fuyi/GeoDatabase/China/山西省/山西省_省界.shp current element size: 5 ------------------- /home/fuyi/GeoDatabase/China/辽宁省/辽宁省_县界.shp current element size: 123 ------------------- /home/fuyi/GeoDatabase/China/辽宁省/辽宁省_市界.shp current element size: 21 ------------------- /home/fuyi/GeoDatabase/China/辽宁省/辽宁省_省界.shp current element size: 4 ------------------- /home/fuyi/GeoDatabase/China/辽宁省/辽宁省_乡镇边界.shp current element size: 1659 Process finished with exit code 0 参考 wiki-中华人民共和国行政区划代码 geotools-shapeplugin -- alter table district_info alter column center_point -- type Geometry(Point, 4326) USING ST_SetSRID(center_point, 4326); -- ALTER TABLE district_info ALTER COLUMN bounds TYPE Geometry(MultiPolygon, 4326) USING ST_SetSRID(bounds, 4326); select st_astext(center_point), * from district_info where grade = 1 order by id limit 1000 ; select st_area(bounds) as area, * from district_info where grade = 1 and name like \u0026#39;%海南%\u0026#39;; -- delete from district_info; select code, count(*) as _count from district_info group by code order by _count desc; select * from district_info; ","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/005/","summary":"前言 微天气程序中存在如下几个功能需要使用到行政区划数据：\n城市列表，需要支持城市搜索 根据经纬度获区域（城市）的天气数据 地图坐标拾取并获取所处区域（城市）信息，同时获取天气数据 对于城市的天气数据，不使用和风天气的城市列表，而是自行维护，通过空间位置（经纬度）进行关联。对于城市位置的定义，本可以选择如行政中心或市中心，但我没有这样的数据，就直接用城市区划范围的中心点代替。不准确不重要，过程已经满足了，且后续是可以替换的。\n其实完全可以使用官方提供的城市数据和GeoAPI覆盖这些功能，但既然我是做GIS开发的，而且手里也有可以用作研究学习的数据，为啥不用呢。\n今天突然发现，其实我可以爬一下和风的数据，这样就可以拿到城市选择的经纬度数据了 @time 2023.10.10\nAbout 行政区划解析程序，输入shape文件，写入Postgresql.\n大体完整的四级行政区划数据组织\ngithub link: fuyi-district-parse\n数据情况 数据原源于网络，由于时间久远，我已经忘记了是如何获取到的\n名词解释 省级行政区 中国的一级行政区，或称国家一级行政区或省级行政区，是指直属中央政府管辖的行政区划，在历史上曾有不同的称呼。如：省、自治区、直辖市、特别行政区。\n地级行政区 地级行政区即“地区级别行政区”，是现行中华人民共和国行政区划中常规的第二级行政区划单位，包括地级市、地区、盟、自治州等。地级行政区隶属于省、自治区、直辖市等省级行政区之下；下辖若干个县、区、县级市、旗等县级行政区。作为特例，东莞市、中山市、嘉峪关市、儋州市等四个地级市下辖街道办事处与乡镇，不辖县、区，因此也称作“直筒子（地级）市”。地级行政区的级别为正厅级，所以非正厅级的省直辖的行政区划不算作地级行政区，例如：湖北省辖的仙桃市、天门市、潜江市；河南省辖的济源市等等。直辖市下辖的区，虽然是正厅级，但未列入地级行政区的统计。\n县级行政区 县为中华人民共和国行政区划单位之一，县级行政区指行政地位与“县”相同的行政区划单位的总称，其管辖乡级行政区。为乡、镇的上一级行政区划单位。中华人民共和国成立后，随着行政督察区名称的变更，除各直辖市均隶属于专区（行政督察专区）、地区或地级行政区，现除各直辖市、海南省直管县外均为地级行政区的下一级行政区。\n按省、县、乡三级行政区划制度划分，县级行政区属于第二级行政区，为直辖市的下级行政区划单位。 按省、地、县、乡四级行政区划制度划分，县级行政区属于第三级行政区，属于省、自治区所辖地级行政区的下级行政区划单位。 乡级行政区 乡，中华人民共和国现行基层行政区划单位，区划层次介于县与村之间。“乡”为县、县级市下的主要行政区划类型之一。中国行政区划史上，“乡”一直为县的行政区划单元，因此现行处同一层次的区划单位归入乡级行政区。中国自改革开放以来，由于城市的快速扩张，行政区划制度出现了大的变革。1980年代以后“乡改镇”、“乡改街道”的现象越来越普遍。\n在乡级行政区划中，乡（包括镇）设有一级人民政府，属于基层政权；乡的行政区划单位为村（含民族村）。但很多乡设有社区，乡的区划单位设置与镇、街道看不出实质性差异。\n目的 解析所有数据文件，实现最终入库\n使用GeoTools实现\n数据库表结构：\n行政区划信息（district_info）\nid：自增Id（bigserial） name：行政区划名称 grade：行政区划等级（省级行政区：1， 地级行政区：2， 县级行政区：3，） code：行政区代码 center_point：中心点（geometry::point） bounds：行政区边界（geometry） 注：\n数据入库前审查，保证行政区代码唯一 使用grade区分省、市、县、乡镇 对于省、市、县code列，统一进行前6位截取（不满6位字符所在数据，直接丢弃），对于乡镇则统一进行前9位进行截取（不满9位字符所在数据，直接丢弃） 省级行政区 存在错乱数据，可以使用行政区代码识别（adcode）\n地级行政区 存在错乱数据，可以使用行政区代码识别（code），需要截取前6位\n县级行政区 存在错乱数据，可以使用行政区代码识别（code），需要截取前6位\n乡级行政区 成果 记录数：46652\n点数据为各个行政区中心点\n数据表 PostGIS CREATE EXTENSION IF NOT EXISTS postgis WITH SCHEMA public; 行政区划 -- -- Name: district_info_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres -- CREATE SEQUENCE public.","title":"微天气—行政区划数据（一）"},{"content":"前言 关于矢量瓦片（节选） 地图瓦片技术是在线地图服务常用的瓦片技术，瓦片就是地图瓦片的具体存储形态，提前切好的瓦片可以大大提高在线地图的访问效率。\n栅格瓦片 以图片为介质的栅格瓦片使得在线地图得以迅速普及，优势在于显示效率高、方便传输。但是，随着地图的移动化和应用的逐渐深入，栅格瓦片占用带宽和存储都较大，不利于地图在移动设备的应用。\n矢量瓦片 矢量瓦片的产生弥补了栅格瓦片的不足。矢量瓦片数据以矢量形式存在。矢量瓦片体积下，可高度压缩，占用的存储空间比栅格瓦片要小上千倍。数据传输体量小，地图更新的代价小\n常见的矢量瓦片制作工具（节选） 目前开源的矢量切片工具还是非常多的，列出一些主流的阐述下：\n基于GeoServer的矢量切片插件，适合熟悉GeoServer的用户，操作还比较简单，缺点是切片的行列号与一般的XYZ编号不同不容易单独部署。 基于tippecanoe的矢量切片工具方案，该工具提供了很多高级功能在数据定制化上有很强的优势，但只能部署在Linux，并不是跨平台，只能读取geojson文件，不能直连数据库，不是很好，如果有幸您是c++开发大神，可以改下库的编译绑定平台，使其支持windows，再更改下数据源底层，使其能支持空间数据库，那么该工具会有更多的应用空间。 基于PostGIS的矢量切片方案，该方案在熟悉PostGIS的用户中应该很受欢迎，优势是支持动态矢量切片，有PG社区的系统级加成。 总的来说，工具虽然很多，但是没有一款可以说覆盖一切场景的，具体应用还是看场景的，比如前两个方案都是做底图数据时比较有用，都是静态矢量切片方案，geoserver能直连数据库，tippecanoe有强数据定制性要求，那么如果用户侧重点是简单点的话geoserver够了，用户侧重点是希望对数据做很多高级过滤什么的操作用tippecanoe，但步骤麻烦点。这些矢量切片工具仅仅在处理很久不变的数据，就是切一次用很久的数据，如果数据频繁变化，这种静态数据切片工具就很不好用了。\n与其他方案相比，PostGIS方案的好处主要有两大点：\n资源开销低：空间数据一般存空间数据库中，传统工具会先从数据库中捞数据，这个数据通常很大，网络开销和服务器端内存都要很大，查询慢计算慢是肯定的。而PostGIS是在数据库中把数据处理完，只把结果传给后台转前台，可以很方便的使用数据库的索引，并行计算等，优化查询和处理速度。 动态矢量切片，数据时效性高：每当根据xyz请求时，数据库会动态查询范围内数据，裁剪简化并输出pbf格式的二进制数据出去，在数据变化频繁的场景下，可以保证用户看到的是最新的数据。 💡 GeoServer、Tippecanoe 皆为静态矢量切片方案，需提前准备切片数据，并进行持久化（GeoServer也可以在使用时进行切片，同时进行持久化）。PostGIS支持动态矢量切片方案，即实时计算生成切片，且不进行切片的持久化。\n为什么要自己写一个服务 于我个人而言，我目前仅接触和使用到了GeoServer，且对其中的实现细节并不太清楚，所以想通过参考模仿的方式实现一个示例服务。其次，还想测试在没有如GeoWebcache此类的瓦片缓存的情况下，服务的性能如何。综上所述，其实也就是为了如下这几方面的目的：\n学习，了解其中的实现细节\n更好的适配\n比如说，WMTS服务很明确存在缓存，WMS性能又不够好。如果使用WMTS服务确实可以提升服务的性能，但是对于源数据存在编辑的场景下，缓存问题还是会让人头疼。\n那么是否存在动态的矢量瓦片服务？既能解决缓存的问题，同时还没有太大的性能问题。\n你或许会提到基于PostGIS的动态矢量瓦片服务，但是有些历史的原因，短时间内没有变法变更数据库。当然也可以基于类如CDC这样的功能进行缓存的更新，但其实还是会存在缓存的问题，只是说可以通过一些手段降低缓存问题出现的概率，并无法从根本上解决问题\n所以，基于此，既然PostGIS可以实现动态矢量瓦片服务，我们自然也可以。公瑾大佬曾发文说过，当下地图服务去服务化、数据不切片基本上已经是必然的趋势，那么我为什么还要去做一个服务化的东西。大概就是下面这几个原因了：\n数据库技术，在某些特定的因素下，短时间内无法切换到PostGIS 数据量级 性能容忍度 小厂，我不思进取 🙄 后续知识储备 矢量瓦片标准 参见：矢量瓦片标准\n在这里贴几个关键点（对于目前使用上来说）：\n文件格式\n矢量瓦片文件采用Google Protocol Buffers进行编码。Google Protocol Buffers是一种兼容多语言、多平台、易扩展的数据序列化格式。\n投影和范围\n矢量瓦片表示的是投影在正方形区块上的数据。矢量瓦片不应该包含范围和投影信息。解码方被假定知道矢量瓦片的范围和投影信息。\nWeb Mercator是默认的投影方式，Google tile scheme是默认的瓦片编号方式。两者一起完成了与任意范围、任意精度的地理区域的一一对应，例如https://example.com/17/65535/43602.mvt。\n矢量瓦片可以用来表示任意投影方式、任意瓦片编号方案的数据。\n内部结构\n图层\n每块矢量瓦片应该至少包含一个图层。每个图层应该至少包含一个要素。\n几何图形编码\n矢量瓦片中的几何数据被定义为屏幕坐标系。瓦片的左上角（显示默认如此）是坐标系的原点。X轴向右为正，Y轴向下为正。几何图形中的坐标必须为整数。\n矢量瓦片服务构建 在这里，我选择抄GeoServer的作业。众所周知，PostGIS是开源的，那为什么没有选择抄PostGIS的作业呢？\n当下水平不够 想快速验证想法 想基于GeoServer做二次开发，或者说是基于现存的地图服务相关的实现，集各家之大成，合并成一个组在功能上可自由搭配的、较高性能服务端组件 接着说当前的事情。要实现一个动态矢量瓦片服务，我们需要先分析一下实现内容，在此先做出如下拆解：\n动态矢量瓦片服务可以理解为没有瓦片缓存的，实时生成的矢量瓦片服务，所以核心还是矢量瓦片服务(@time 20210503: 动态矢量瓦片技术是相对矢量瓦片技术提出的，而矢量瓦片技术的大规模应用还是以预切为主，所以动态矢量瓦片要解决的是不再预切动态生成，同时避免一下子生成大规模瓦片文件的问题) 矢量瓦片服务也就是根据调用端传递的参数，从数据源获取对应的数据，并将其转换为矢量瓦片格式，最终返回给调用端 这里选择瓦片坐标作为检索参数，可以便于服务降级（缓存）和性能优化（瓦片坐标值相对来说更加准确和可固定，且便于降维） 需要实现瓦片坐标系到数据源坐标系下数据范围的相互转换 需要实现数据源的范围查询 需要实现矢量数据到矢量瓦片的编解码 那么大体的实现路径可归结于如下所示：\n其中的核心要点总结如下：\n瓦片金字塔模型构建 瓦片坐标系与空间坐标系的映射关系 切片方案确定 瓦片大小 分辨率 … 矢量瓦片的编解码 矢量数据查询 下面将对以上几点作详细描述，本次内容基本上全部参考GeoServer实现完成。\n瓦片金字塔模型构建 瓦片金字塔模型（节选） 瓦片最早是由谷歌地图提出并进行应用的，其采用特定的切割方式对采用WebMercator投影坐标的世界地图栅格影像进行切片，由于WebMercator投影方便计算机进行计算，随后各大主流WebGIS和互联网地图应用商都采用基于WebMercator投影坐标系的方式进行切片。\n瓦片金字塔的主要原理为：基于某个特定的地图投影坐标系，将曲面 的地球投影到二维平面，而后将该二维平面进行多尺度地划分，即相当于制作了多个不同分辨率层级的数字地图。各层级对应相应编码，层级越高地图所对应的分辨率越高；而后对每一层级的全球空间范围地图按照某种空间划分方法进行格网划分，划分成若干行和列的固定尺寸的正方形栅格图片，这些切分出来的规整的单个格网单元称为瓦片，各层级的划分方法都是相同的。\n瓦片划分方法需满足以下条件：\n每个层级下的所有瓦片可以无缝拼合成一张全球空间范围的世界地图 每个瓦片都有唯一编码，根据编码可以解算该瓦片对应的空间范围 在某一层级下给定一坐标点可以根据其空间坐标解算其所在瓦片的编号 每一层级瓦片对应一层金字塔，各个层级的瓦片构成了整个瓦片金字塔模型。每一层中的瓦片划分方法一般采用均匀四分的划分方法，即以赤道和中央经线的交点为初始中心，不断地对地图进行四分，直到每个格网的大小为tilesize * tilesize为止，其中tilesize表示单个瓦片的边长。基于此种划分方法，第0层金字塔（金字塔顶层）用一个瓦片就能表示整张世界地图，第1层要用4^z个瓦片来表示整个世界地图，z为当前瓦片的金字塔层级。\n瓦片坐标系（节选） 所有瓦片的编码都是基于瓦片坐标系下进行的，瓦片坐标系的原点一般都在左上角或者左下角，TMS规范中是在左下角（GeoServer遵循该规范），但是现有的Google、MapNIK切片系统都是选用左上角作为原点，本文主要以原点在左上角的瓦片坐标系进行说明。\n瓦片的编码方式如下图所示，层级用z表示，瓦片经线方向（指瓦片经度发生变化的方法，即东西向，东向为正）上编号为x，纬线方向（指瓦片维度发生变化的方向，即南北向，南向为正）上编号为y，因此每一个瓦片都可以通过一个三维元组（x,y,z）来唯一描述。\n💡 节选自《高性能影像数据瓦片化关键技术研究-刘世永-2016》，章节：第二章，2.1-2.3\n瓦片金字塔程序模型 此处只显示核心模块\nGrid，对应单层金字塔\n// 指定层级横向的瓦片数量 private long numTilesWide; // 指定层级纵向的瓦片数量 private long numTilesHigh; // 分辨率 private double resolution; // 比例尺分母 private double scaleDenom; private String name; GridSet，对应完整的金字塔层级\n// 名称 private String name; private SRS srs; // 瓦片宽度，一般为256 private int tileWidth; // 瓦片高度，一般为256 private int tileHeight; protected boolean yBaseToggle = false; private boolean yCoordinateFirst = false; private boolean scaleWarning = false; private double metersPerUnit; private double pixelSize; // 数据范围 private BoundingBox originalExtent; private Grid[] gridLevels; private String description; private boolean resolutionsPreserved; 瓦片金字塔逻辑模型构建 瓦片金字塔需要又逻辑模型与物理模型共同构成，物理模型（即瓦片集）需要根据逻辑模型的定义才可进行构建。此处以常规瓦片金字塔逻辑模型为示例，通过上述程序模型，构建瓦片金字塔逻辑模型\n// Pixel size (to calculate scales). The default is 0.28mm/pixel, corresponding to 90.71428571428572 DPI. public static GridSet buildGriDSet(SRS srs, double pixelSize) { return GridSetFactory.createGridSet(String.valueOf(srs.getNumber()), srs, BoundingBox.WORLD3857, false, 30, null, pixelSize, 256, 256, false); } // 需要将其转换为米 buildGriDSet(SRS.getEPSG900913(), 0.00028D); public static GridSet createGridSet(String name, SRS srs, BoundingBox extent, boolean alignTopLeft, int levels, Double metersPerUnit, double pixelSize, int tileWidth, int tileHeight, boolean yCoordinateFirst) { double extentWidth = extent.getWidth(); double extentHeight = extent.getHeight(); double resX = extentWidth / (double)tileWidth; double resY = extentHeight / (double)tileHeight; int tilesWide; int tilesHigh; // 计算调整的基数, 以小边为主 if (resX \u0026lt;= resY) { tilesWide = 1; tilesHigh = (int)Math.round(resY / resX); resY /= (double)tilesHigh; } else { tilesHigh = 1; tilesWide = (int)Math.round(resX / resY); resX /= (double)tilesWide; } // 计算地图分辨率, 即单位像素内表示的地图单位(米或度) double res = Math.max(resX, resY); // 获取裁减后的范围 double adjustedExtentWidth = (double)(tilesWide * tileWidth) * res; double adjustedExtentHeight = (double)(tilesHigh * tileHeight) * res; // 根据上述范围重新构建范围表达 BoundingBox adjExtent = new BoundingBox(extent); adjExtent.setMaxX(adjExtent.getMinX() + adjustedExtentWidth); // 这里想要表达的就是原点在于左上角还是左下角（确定起算位置）, 因为这样会影响y方向上的范围计算。就是进行y方向上范围计算 if (alignTopLeft) { adjExtent.setMinY(adjExtent.getMaxY() - adjustedExtentHeight); } else { adjExtent.setMaxY(adjExtent.getMinY() + adjustedExtentHeight); } // 根据给定的层级参数, 构建各层级下分辨率值 double[] resolutions = new double[levels]; // 设置0级的分辨率 resolutions[0] = res; for(int i = 1; i \u0026lt; levels; ++i) { // 采用常规均匀四分（四叉树）方式进行构建 resolutions[i] = resolutions[i - 1] / 2.0D; } return createGridSet(name, srs, adjExtent, alignTopLeft, resolutions, (double[])null, metersPerUnit, pixelSize, (String[])null, tileWidth, tileHeight, yCoordinateFirst); } public static GridSet createGridSet(String name, SRS srs, BoundingBox extent, boolean alignTopLeft, double[] resolutions, double[] scaleDenoms, Double metersPerUnit, double pixelSize, String[] scaleNames, int tileWidth, int tileHeight, boolean yCoordinateFirst) { Assert.notNull(name, \u0026#34;name is null\u0026#34;); Assert.notNull(srs, \u0026#34;srs is null\u0026#34;); Assert.notNull(extent, \u0026#34;extent is null\u0026#34;); Assert.isTrue(!extent.isNull() \u0026amp;\u0026amp; extent.isSane(), \u0026#34;Extent is invalid: \u0026#34; + extent); Assert.isTrue(resolutions != null || scaleDenoms != null, \u0026#34;The gridset definition must have either resolutions or scale denominators\u0026#34;); Assert.isTrue(resolutions == null || scaleDenoms == null, \u0026#34;Only one of resolutions or scaleDenoms should be provided, not both\u0026#34;); int i; for(i = 1; resolutions != null \u0026amp;\u0026amp; i \u0026lt; resolutions.length; ++i) { if (resolutions[i] \u0026gt;= resolutions[i - 1]) { throw new IllegalArgumentException(\u0026#34;Each resolution should be lower than it\u0026#39;s prior one. Res[\u0026#34; + i + \u0026#34;] == \u0026#34; + resolutions[i] + \u0026#34;, Res[\u0026#34; + (i - 1) + \u0026#34;] == \u0026#34; + resolutions[i - 1] + \u0026#34;.\u0026#34;); } } for(i = 1; scaleDenoms != null \u0026amp;\u0026amp; i \u0026lt; scaleDenoms.length; ++i) { if (scaleDenoms[i] \u0026gt;= scaleDenoms[i - 1]) { throw new IllegalArgumentException(\u0026#34;Each scale denominator should be lower than it\u0026#39;s prior one. Scale[\u0026#34; + i + \u0026#34;] == \u0026#34; + scaleDenoms[i] + \u0026#34;, Scale[\u0026#34; + (i - 1) + \u0026#34;] == \u0026#34; + scaleDenoms[i - 1] + \u0026#34;.\u0026#34;); } } GridSet gridSet = baseGridSet(name, srs, tileWidth, tileHeight); // true：保留分辨率并计算 scaleDenominators // false：分辨率是根据 sacale 分母计算的 gridSet.setResolutionsPreserved(resolutions != null); gridSet.setPixelSize(pixelSize); gridSet.setOriginalExtent(extent); // tileOrigin()的y坐标是在顶部（true）还是在底部（false），相应的标记原点位置 gridSet.yBaseToggle = alignTopLeft; // 轴顺序, 默认经度优先 gridSet.setyCoordinateFirst(yCoordinateFirst); // 设置单位, 每个单位内表示多少米 // 经纬度直投的情况需要特殊处理 if (metersPerUnit == null) { if (srs.equals(SRS.getEPSG4326())) { gridSet.setMetersPerUnit(111319.49079327358D); } else if (srs.equals(SRS.getEPSG3857())) { gridSet.setMetersPerUnit(1.0D); } else { if (resolutions == null) { log.config(\u0026#34;GridSet \u0026#34; + name + \u0026#34; was defined without metersPerUnit, assuming 1m/unit. All scales will be off if this is incorrect.\u0026#34;); } else { log.config(\u0026#34;GridSet \u0026#34; + name + \u0026#34; was defined without metersPerUnit. Assuming 1m per SRS unit for WMTS scale output.\u0026#34;); gridSet.setScaleWarning(true); } gridSet.setMetersPerUnit(1.0D); } } else { gridSet.setMetersPerUnit(metersPerUnit); } if (resolutions == null) { gridSet.setGridLevels(new Grid[scaleDenoms.length]); } else { gridSet.setGridLevels(new Grid[resolutions.length]); } for(i = 0; i \u0026lt; gridSet.getNumLevels(); ++i) { Grid curGrid = new Grid(); if (scaleDenoms != null) { curGrid.setScaleDenominator(scaleDenoms[i]); curGrid.setResolution(pixelSize * (scaleDenoms[i] / gridSet.getMetersPerUnit())); } else { curGrid.setResolution(resolutions[i]); // 计算比例尺 (单位像素下表示的实际空间距离 * 每个单位情况表示多少米) / (像素大小，或称像元大小) // 约去像元, 即获取到比例尺分母 curGrid.setScaleDenominator(resolutions[i] * gridSet.getMetersPerUnit() / 2.8E-4D); } double mapUnitWidth = (double)tileWidth * curGrid.getResolution(); double mapUnitHeight = (double)tileHeight * curGrid.getResolution(); long tilesWide = (long)Math.ceil((extent.getWidth() - mapUnitWidth * 0.01D) / mapUnitWidth); long tilesHigh = (long)Math.ceil((extent.getHeight() - mapUnitHeight * 0.01D) / mapUnitHeight); curGrid.setNumTilesWide(tilesWide); curGrid.setNumTilesHigh(tilesHigh); if (scaleNames != null \u0026amp;\u0026amp; scaleNames[i] != null) { curGrid.setName(scaleNames[i]); } else { curGrid.setName(gridSet.getName() + \u0026#34;:\u0026#34; + i); } gridSet.setGrid(i, curGrid); } return gridSet; } 矢量瓦片编解码模块 参考GeoServer实现，使用第三方编解码组件：java-vector-tile\nhttps://github.com/ElectronicChartCentre/java-vector-tile\n矢量数据查询 为了更快的实现开发，此处选择使用GeoTools提供的JDBC相关组件进行数据查询，核心类如下：\nJDBCDataStore VirtualTable：可以支持自定义SQL MathTransform ContentFeatureSource SimpleFeatureCollection 主要逻辑步骤如下所示：\n通过JDBCDataStore创建VirtualTable实例 根据GridSet将传入参数转为BoundingBox 使用MathTransform将BoudingBox转换到源数据所对应的坐标系下 构建查询对象，并使用上述创建的虚拟表实例（ContentFeatureSource）进行数据检索 将检索的数据结果应用于矢量瓦片编码并返回结果 服务验证 叠加底图进行验证\n参考 Mapbox Vector Tile Specification awesome-vector-tiles Vector tiles standards 矢量瓦片概述 WebGIS数据不切片或是时代必然 PostGIS矢量切片技术助力GIS可视化 Geoserver Buffer around Vectortiles Gridsets and Gridsubsets Tiles à la Google Maps Coordinates, Tile Bounds and Projection 国内主要地图瓦片坐标系定义及计算原理 瓦片(Tile)地图原理 ","permalink":"https://fuyi-atlas.github.io/posts/program/map/how-to-build-a-vector-tile-service/","summary":"前言 关于矢量瓦片（节选） 地图瓦片技术是在线地图服务常用的瓦片技术，瓦片就是地图瓦片的具体存储形态，提前切好的瓦片可以大大提高在线地图的访问效率。\n栅格瓦片 以图片为介质的栅格瓦片使得在线地图得以迅速普及，优势在于显示效率高、方便传输。但是，随着地图的移动化和应用的逐渐深入，栅格瓦片占用带宽和存储都较大，不利于地图在移动设备的应用。\n矢量瓦片 矢量瓦片的产生弥补了栅格瓦片的不足。矢量瓦片数据以矢量形式存在。矢量瓦片体积下，可高度压缩，占用的存储空间比栅格瓦片要小上千倍。数据传输体量小，地图更新的代价小\n常见的矢量瓦片制作工具（节选） 目前开源的矢量切片工具还是非常多的，列出一些主流的阐述下：\n基于GeoServer的矢量切片插件，适合熟悉GeoServer的用户，操作还比较简单，缺点是切片的行列号与一般的XYZ编号不同不容易单独部署。 基于tippecanoe的矢量切片工具方案，该工具提供了很多高级功能在数据定制化上有很强的优势，但只能部署在Linux，并不是跨平台，只能读取geojson文件，不能直连数据库，不是很好，如果有幸您是c++开发大神，可以改下库的编译绑定平台，使其支持windows，再更改下数据源底层，使其能支持空间数据库，那么该工具会有更多的应用空间。 基于PostGIS的矢量切片方案，该方案在熟悉PostGIS的用户中应该很受欢迎，优势是支持动态矢量切片，有PG社区的系统级加成。 总的来说，工具虽然很多，但是没有一款可以说覆盖一切场景的，具体应用还是看场景的，比如前两个方案都是做底图数据时比较有用，都是静态矢量切片方案，geoserver能直连数据库，tippecanoe有强数据定制性要求，那么如果用户侧重点是简单点的话geoserver够了，用户侧重点是希望对数据做很多高级过滤什么的操作用tippecanoe，但步骤麻烦点。这些矢量切片工具仅仅在处理很久不变的数据，就是切一次用很久的数据，如果数据频繁变化，这种静态数据切片工具就很不好用了。\n与其他方案相比，PostGIS方案的好处主要有两大点：\n资源开销低：空间数据一般存空间数据库中，传统工具会先从数据库中捞数据，这个数据通常很大，网络开销和服务器端内存都要很大，查询慢计算慢是肯定的。而PostGIS是在数据库中把数据处理完，只把结果传给后台转前台，可以很方便的使用数据库的索引，并行计算等，优化查询和处理速度。 动态矢量切片，数据时效性高：每当根据xyz请求时，数据库会动态查询范围内数据，裁剪简化并输出pbf格式的二进制数据出去，在数据变化频繁的场景下，可以保证用户看到的是最新的数据。 💡 GeoServer、Tippecanoe 皆为静态矢量切片方案，需提前准备切片数据，并进行持久化（GeoServer也可以在使用时进行切片，同时进行持久化）。PostGIS支持动态矢量切片方案，即实时计算生成切片，且不进行切片的持久化。\n为什么要自己写一个服务 于我个人而言，我目前仅接触和使用到了GeoServer，且对其中的实现细节并不太清楚，所以想通过参考模仿的方式实现一个示例服务。其次，还想测试在没有如GeoWebcache此类的瓦片缓存的情况下，服务的性能如何。综上所述，其实也就是为了如下这几方面的目的：\n学习，了解其中的实现细节\n更好的适配\n比如说，WMTS服务很明确存在缓存，WMS性能又不够好。如果使用WMTS服务确实可以提升服务的性能，但是对于源数据存在编辑的场景下，缓存问题还是会让人头疼。\n那么是否存在动态的矢量瓦片服务？既能解决缓存的问题，同时还没有太大的性能问题。\n你或许会提到基于PostGIS的动态矢量瓦片服务，但是有些历史的原因，短时间内没有变法变更数据库。当然也可以基于类如CDC这样的功能进行缓存的更新，但其实还是会存在缓存的问题，只是说可以通过一些手段降低缓存问题出现的概率，并无法从根本上解决问题\n所以，基于此，既然PostGIS可以实现动态矢量瓦片服务，我们自然也可以。公瑾大佬曾发文说过，当下地图服务去服务化、数据不切片基本上已经是必然的趋势，那么我为什么还要去做一个服务化的东西。大概就是下面这几个原因了：\n数据库技术，在某些特定的因素下，短时间内无法切换到PostGIS 数据量级 性能容忍度 小厂，我不思进取 🙄 后续知识储备 矢量瓦片标准 参见：矢量瓦片标准\n在这里贴几个关键点（对于目前使用上来说）：\n文件格式\n矢量瓦片文件采用Google Protocol Buffers进行编码。Google Protocol Buffers是一种兼容多语言、多平台、易扩展的数据序列化格式。\n投影和范围\n矢量瓦片表示的是投影在正方形区块上的数据。矢量瓦片不应该包含范围和投影信息。解码方被假定知道矢量瓦片的范围和投影信息。\nWeb Mercator是默认的投影方式，Google tile scheme是默认的瓦片编号方式。两者一起完成了与任意范围、任意精度的地理区域的一一对应，例如https://example.com/17/65535/43602.mvt。\n矢量瓦片可以用来表示任意投影方式、任意瓦片编号方案的数据。\n内部结构\n图层\n每块矢量瓦片应该至少包含一个图层。每个图层应该至少包含一个要素。\n几何图形编码\n矢量瓦片中的几何数据被定义为屏幕坐标系。瓦片的左上角（显示默认如此）是坐标系的原点。X轴向右为正，Y轴向下为正。几何图形中的坐标必须为整数。\n矢量瓦片服务构建 在这里，我选择抄GeoServer的作业。众所周知，PostGIS是开源的，那为什么没有选择抄PostGIS的作业呢？\n当下水平不够 想快速验证想法 想基于GeoServer做二次开发，或者说是基于现存的地图服务相关的实现，集各家之大成，合并成一个组在功能上可自由搭配的、较高性能服务端组件 接着说当前的事情。要实现一个动态矢量瓦片服务，我们需要先分析一下实现内容，在此先做出如下拆解：\n动态矢量瓦片服务可以理解为没有瓦片缓存的，实时生成的矢量瓦片服务，所以核心还是矢量瓦片服务(@time 20210503: 动态矢量瓦片技术是相对矢量瓦片技术提出的，而矢量瓦片技术的大规模应用还是以预切为主，所以动态矢量瓦片要解决的是不再预切动态生成，同时避免一下子生成大规模瓦片文件的问题) 矢量瓦片服务也就是根据调用端传递的参数，从数据源获取对应的数据，并将其转换为矢量瓦片格式，最终返回给调用端 这里选择瓦片坐标作为检索参数，可以便于服务降级（缓存）和性能优化（瓦片坐标值相对来说更加准确和可固定，且便于降维） 需要实现瓦片坐标系到数据源坐标系下数据范围的相互转换 需要实现数据源的范围查询 需要实现矢量数据到矢量瓦片的编解码 那么大体的实现路径可归结于如下所示：\n其中的核心要点总结如下：","title":"如何构建一个矢量瓦片服务"},{"content":"前言 本文用于记录GeoServer开发环境的搭建过程\n通过GeoServer发布计划可以看到，在2.23.x版本开始，会移除对jdk1.8的支持。那么当前我们会选择2.22.x版本进行研究\n环境 JAVA：1.8或11 Maven Git Action 获取源码 git clone git://github.com/geoserver/geoserver.git geoserver # or git clone https://github.com/geoserver/geoserver.git geoserver 代码库结构 Each branch has the following structure:\nbuild - release and continuous integration scripts doc - sources for the user and developer guides src - java sources for GeoServer itself data - a variety of GeoServer data directories / configurations 切换到2.22.x分支 # 查看分支 git branch -av # 切换分支 git checkout -b 2.22.x origin/2.22.x Import modules into IntelliJ Run the IntelliJ IDE\nFrom the initial panel select Open.\nNavigate to the geoserver/src/pom.xml directory and click OK. When asked click on Open as a Project. Optionally, depending on which platform, IntelliJ may ask to Trust the Project. Wait for IntelliJ to Sync the dependencies, it’s possible to follow the process from the Build tab panel on the bottom. Finalize the GeoServer Project configuration Click File \u0026gt; Project Structure. Update the Name and select the correct SDK accordingly to the GeoServer version. Click File \u0026gt; Settings. From Build, Execution, Deployment \u0026gt; Compiler \u0026gt; Annotation Processors, enable the Annotation processing. Click Build \u0026gt; Rebuild Project. Run GeoServer from IntelliJ From the Project browser select the web-app module Navigate to the org.geoserver.web package Right-click the Start class and click to Modify Run Configuration... 如果没有该选项，那么直接去Edit Configurations处创建一个即可，Add Configuration，选择Application\nIt is important to correctly set the Working directory to src/web/app. While having the Edit Configurations dialog open, fine tune the launch environment (including setting a GEOSERVER_DATA_DIR or the jetty.port). When settings are satisfactory, click OK. It’s possible now to run GeoServer by selecting Run -\u0026gt; Run 'Start' 第一次启动比较慢\nAccess GeoServer front page After a few seconds, GeoServer should be accessible at: http://localhost:8080/geoserver The default admin password is geoserver. FAQ 依赖下载失败或过慢（该条仅供参考） 使用阿里云Maven镜像加速依赖下载，可以参考阿里云Maven向导进行设置【有代理（科学上网）应该就不需要了】\nmaven配置文件优先级，maven工具所在的conf配置 \u0026gt; 外部化配置\n编译提示Node不存在（wcs1_1） 网络上有一些提示是：手动添加Node的依赖。但是我认为应该不需要手动设置才是，因为从官网的maven quick start中，并未使用任何的编辑器，直接命令行搞定。\n# skip unit tests, skip spotless(Spotless is used as a fast way to check that the google-java-format is being applied to the codebase.) mvn clean install -DskipTests -Dspotless.apply.skip=true # 不想install, package也可以 mvn clean package -DskipTests -Dspotless.apply.skip=true 这个时候可以看到，Node这个类存在于编译后的代码中\n通过跟踪构建过程可以看到，是由javacc在编译过程中生成的（根据rangeset.jjt文件，知识盲区了）\n其实在官方文档中已经提到了如何处理该问题的，参见Troubleshooting#1，就是需要先完成wcs1_1模块的编译构建。\n我就不管那么多了，直接把所有的都构建了\nMaven 使用 参见：GeoServer Maven Guide\n参考 IntelliJ QuickStart\nGeoServer Maven Guide\n阿里云云效Maven\n","permalink":"https://fuyi-atlas.github.io/posts/program/geoserver/002/","summary":"前言 本文用于记录GeoServer开发环境的搭建过程\n通过GeoServer发布计划可以看到，在2.23.x版本开始，会移除对jdk1.8的支持。那么当前我们会选择2.22.x版本进行研究\n环境 JAVA：1.8或11 Maven Git Action 获取源码 git clone git://github.com/geoserver/geoserver.git geoserver # or git clone https://github.com/geoserver/geoserver.git geoserver 代码库结构 Each branch has the following structure:\nbuild - release and continuous integration scripts doc - sources for the user and developer guides src - java sources for GeoServer itself data - a variety of GeoServer data directories / configurations 切换到2.22.x分支 # 查看分支 git branch -av # 切换分支 git checkout -b 2.","title":"GeoServer开发环境搭建"},{"content":"为什么需要深入研究GeoServer GeoServer是一个开源的、流行的、Java实现的可发布和共享地理信息数据的软件服务器，是WMS、WCS、WFS的标准实现，含WMTS GeoServer功能强大，但是我可能只会用到其中的一两个部分，比如坐标转换与WMTS服务。可以通过研究将其进行提取和重组，形成符合我要求的，更加精简方便的软件服务器 GIS后端开发绕不过去的知识点 如何进行研究 开发环境搭建 通过WMTS请求链接到源码 通过源码进行研究学习 … 未完待续\u0026hellip;\n","permalink":"https://fuyi-atlas.github.io/posts/program/geoserver/001/","summary":"为什么需要深入研究GeoServer GeoServer是一个开源的、流行的、Java实现的可发布和共享地理信息数据的软件服务器，是WMS、WCS、WFS的标准实现，含WMTS GeoServer功能强大，但是我可能只会用到其中的一两个部分，比如坐标转换与WMTS服务。可以通过研究将其进行提取和重组，形成符合我要求的，更加精简方便的软件服务器 GIS后端开发绕不过去的知识点 如何进行研究 开发环境搭建 通过WMTS请求链接到源码 通过源码进行研究学习 … 未完待续\u0026hellip;","title":"深入研究GeoServer—开篇"},{"content":"前言 此为深入理解计算机系统（csapp）第一章的笔记，也算是一个开始了。\nHello程序 #include \u0026lt;studio.h\u0026gt; int main() { printf(\u0026#34;hello world\\n\u0026#34;); return 0; } 基本思想 系统中所有的信息——包括磁盘文件、内存中的程序、内存中存放的用户数据以及网络上传输的数据，都是由一串比特（bit）表示的。区分不同数据对象的唯一方法是我们读到这些数据对象时的上下文。比如，在不同的上下文中，一个同样的字节序列可能表示一个整数、浮点数、字符串或者机器指令。\n要点 源程序实际上就是一个由0和1组成的位（或称为比特：bit）序列，8个位为一组，称为字节（byte）。\n大部分的现代计算机系统都使用ASCII标准来表示文本字符，这种方式实际上就是用一个唯一的单字节大小的整数值来表示每个字符\nhello.c程序是以字节序列的方式存储在文件中。每个字节都有一个整数值，对应于某些字符（注意：每个文本行都是以一个看不见的换行符“/n”来结束的）\n只由ASCII字符构成的文件称为文本文件，所有其他文件都称为二进制文件\n编译系统（compilation system）：编译的四个阶段\ngcc -o hello hello.c hello.c（文本） —\u0026gt; 预处理器[cpp] —\u0026gt; hello.i（文本） —\u0026gt; 编译器[ccl] —\u0026gt; hello.s（文本） —\u0026gt; 汇编器[as] —\u0026gt; hello.o（二进制） —\u0026gt; 链接器[ld] —\u0026gt; hello（二进制）\n预处理：预处理器处理#开头的命令，修改原始的C程序 编译：编译器将hello.i翻译为hello.s，即将源文件翻译为汇编语言。汇编语言是非常有用的，因为它为不同高级语言的不同编译器提供了通用的输出语言。例如，C编译器和Fortran编译器产生的输出文件用的都是一样的汇编语言 汇编：汇编器将hello.s翻译为机器语言指令，把这些指令打包成一种叫做可重定位目标程序（relocation object program）的格式，并将结果保存在目标文件hello.o中。hello.o是一个二进制文件（即将汇编内容翻译为机器语言指令，并将这些指令打包成一种叫做可重定位目标程序的格式） 链接：链接器用于处理合并。比如在hello程序中调用了printf函数，它是每个C编译器都提供的标准C库中的一个函数。printf函数存在于一个名为printf.o的单独的预编译好了的目标文件中，而这个文件必须以某种方式合并到hello.o程序中。链接器就负责处理这种合并。最终得到hello文件，它是一个可执行目标文件（或简称为可执行文件），可以被加载到内存中，由系统执行 shell是一个命令行解释器，它输出一个提示符，等待输入一个命令行，然后执行这个命令。如果该命令行的单词不是一个内置的命令，那么shell就会u假设这是一个可执行文件的名字，它将加载并运行这个文件。\n系统的硬件组成 CPU：中央处理单元； ALU：算术/逻辑单元； PC：程序计数器； USB：通用串行总线 总线 贯穿整个系统的是一组电子管道，称做总线，它携带信息字节并负责在各个部件间传递。硬件之间进行信息交流需要有一个统一的标准，也就是二进制信息传递规则，为了高效考虑，通常总线被设计成传送定长的字节块，也就是字（word）。字中的字节数（即字长）是一个基本的系统参数，在各个系统中的情况都不尽相同。现在的大多数机器字长要么是4个字节（32位），要么是8个字节（64位）。\nI/O设备 输入 / 输出（I/O）设备是系统与外部世界的联系通道。每个 I/O 设备都通过一个控制器或适配器与 I/O 总线相连。控制器与适配器之间的区别主要在于它们的封装方式。控制器是 I/O 设备本身或者系统的主印制电路板（通常称为主板）上的芯片组。而适配器则是一块插在主板插槽上的卡。无论如何，它们的功能都是在 I/O 总线和 I/O 设备之间传递信息。\n主存 主存是一个临时存储设备，在处理器执行程序时，用来存放程序和程序处理的数据。从物理上来说，主存是由一组动态随机存取存储器（DRAM）芯片组成的。从逻辑上来说，存储器是一个线性的字节数组，每个字节都有其唯一的地址（即数组索引），这些地址是从零开始的。一般来说，组成程序的每条机器指令都是由不同数量的字节构成。与C程序变量相对应的数据项（程序处理的数据）的大小是根据类型（short、int、float…）变化的。\n处理器 中央处理单元（CPU），简称处理器，是解释（或执行）存储在主存中指令的引擎。处理器的核心是一个大小为一个字（Word）的存储设备（或寄存器），称为程序计数器（PC）。在任何时刻，PC 都指向主存中的某条机器语言指令（即含有该条指令的地址）。\n术业有专攻，计算机的世界里，你做你的，它做它的，大家各司其职。 如果想要 hello 可执行文件（存储在主存上）运行起来，就要调用程序计数器执行整个程序的每一条指令，并有专门的硬件来处理数值计算逻辑操作，并且在数值处理过程中会产生临时的数据，这就需要有个地方寄存，等待所有程序执行结束之后，再存到主存上。\n从系统通电开始，直到系统断电，处理器一直在不断地执行程序计数器指向的指令，再更新程序计数器，使其指向下一条指令。处理器看上去是按照一个非常简单的指令执行模型来操作的，这个模型是由指令集架构决定的。在这个模型中，指令按照严格的顺序执行，而执行一条指令包含一系列的步骤：\n处理器从程序计数器（PC）指向的存储器处读取指令，解释指令中的位（bit），执行该指令指示的简单操作，然后更新 PC，使其指向下一条指令，而这条指令并不一定与存储器中刚刚执行的指令相邻。\n操作是围绕着**主存、寄存器文件（register file）和算术 / 逻辑单元（ALU）进行的。**寄存器文件是一个小的存储设备，由一些单个字长的寄存器组成，每个寄存器都有唯一的名字。ALU 计算新的数据和地址值。下面是一些简单操作的例子，CPU 在指令的要求下可能会执行以下操作 ：\n加载 ：把一个字节或者一个字从主存复制到寄存器，以覆盖寄存器原来的内容。 存储 ：把一个字节或者一个字从寄存器复制到主存的某个位置，以覆盖这个位置上原来的内容。 操作 ：把两个寄存器的内容复制到 ALU，ALU 对这两个字做算术操作，并将结果存放到一个寄存器中，以覆盖该寄存器中原来的内容。 跳转 ：从指令本身中抽取一个字，并将这个字复制到程序计数器（PC）中，以覆盖 PC 中原来的值。 💡 关于处理器的指令集架构和微体系结构\n处理器看上去是它的指令集架构的简单实现，但是实际上现代处理器使用了非常复杂的机制来加速程序的\u0026gt; 执行。因此，我们将处理器的指令集架构和处理器的微体系架构区分开来\n指令集架构：描述的是每条机器代码指令的效果 微体系结构：描述的是处理器实际上是如何实现的\n存储设备 高速缓存局部性原理：即程序具有访问局部区域里的数据和代码的趋势。\n在处理器和一个较大较慢的设备（例如主存）之间插入一个更小更快的存储设备（例如高速缓存）的想法已经成为一个普遍的观念。实际上，每个计算机系统中的存储设备都被组织成了一个存储器层次结构，如图 1-9 所示。在这个层次结构中，从上至下，设备的访问速度越来越慢、容量越来越大，并且每字节的造价也越来越便宜。寄存器文件在层次结构中位于最顶部，也就是第 0 级或记为 L0。这里我们展示的是三层高速缓存 L1 到 L3，占据存储器层次结构的第 1 层到第 3 层。主存在第 4 层，以此类推。\n存储器层次结构的主要思想是上一层的存储器作为低一层存储器的高速缓存。因此，寄存器文件就是 L1 的高速缓存，L1 是 L2 的高速缓存，L2 是 L3 的高速缓存，L3 是主存的高速缓存，而主存又是磁盘的高速缓存。在某些具有分布式文件系统的网络系统中，本地磁盘就是存储在其他系统中磁盘上的数据的高速缓存。 正如可以运用不同的高速缓存的知识来提高程序性能一样，程序员同样可以利用对整个存储器层次结构的理解来提高程序性能。\n操作系统 我们可以把操作系统看作是应用程序与硬件之间插入的一层软件，所有应用程序对硬件的操作尝试都必须通过操作系统\n操作系统的两个基本功能 防止硬件被失控的应用程序滥用 向应用程序提供简单一致的机制来控制复杂而有通常不相同的低级硬件设备 操作系统通过几个基本的抽象概念（进程、虚拟内存和文件）来实现这两个功能\n文件是对I/O设备的抽象表示 虚拟内存是对主存和磁盘I/O的抽象表示 进程则是对处理器、主存和I/O设备的抽象表示 进程 进程是操作系统对一个正在运行的程序的一种抽象。在一个系统上可以同时运行多个进程，而每个进程都好像在独占地使用硬件。而并发运行，则是说一个进程的指令和另一进程的指令是交错执行的（进程级并发）。在大多数系统中，需要运行的进程数是多与可以运行它们的CPU个数的。传统系统（单核系统）在一个时刻只能执行一个程序，而多核处理器同时能够执行多个程序。无论是在单核还是多核系统中，一个CPU看上去都像是在并发地执行多个进程，这是通过处理器在进程间切换实现的。操作系统实现这种交错执行的机制称为上下文切换。（进程是计算机科学中最重要和最成功的概念之一）\n上下文（进程上下文） 操作系统保持跟踪进程运行所需的所有状态信息。这种状态，也就是上下文，包括许多信息，比如PC（程序计数器）和寄存器文件的当前值，以及主存的内容。在任何一个时刻，单处理器系统都只能执行一个进程的代码。当操作系统决定要把控制权从当前进程转移到某个新进程时，就会进行上下文切换，即保存当前进程的上下文，恢复新进程的上下文，然后将控制权传递到新进程。新进程就会从它上次停止的地方开始。\n从一个进程到另一个进程的切换（上下文切换）是由操作系统内核（kernel）管理的。内核是操作系统代码常驻主存的部分。*当应用程序需要操作系统的某些操作时，比如读写文件，它（应用程序）就执行一条特殊的系统调用（system call）指令，将控制权传递给内核。然后内核执行被请求的操作并返回给应用程序。（用户态与内核态切换）*注意：内核不是一个独立的进程。相反，它是系统管理全部进程所用代码和数据结构的集合。\n线程 在现代系统中，一个进程实际上可以由多个称为线程的执行单元组成，每个线程都运行在进程的上下文中，并共享同样的代码和全局数据。\n虚拟内存 虚拟内存是一个抽象概念，它为每个进程都提供一个假象，即每个进程都在独占地使用主存。每个进程看到的内存都是一致的，称为虚拟地址空间。\n在 Linux 中，地址空间最上面的区域是保留给操作系统中的代码和数据的，这对所有进程来说都是一样。地址空间的底部区域存放用户进程定义的代码和数据。请注意，图中的地址是从下往上增大的。\n每个进程看到的虚拟地址空间由大量准确定义的区构成，每个区都有专门的功能。我们从最低的地址开始，逐步向上介绍。\n程序代码和数据对所有的进程来说，代码是从同一固定地址开始，紧接着的是和 C 全局变量相对应的数据位置。代码和数据区是直接按照可执行目标文件的内容初始化的，在示例中就是可执行文件 hello。 堆代码和数据区后紧随着的是运行时堆。代码和数据区在进程一开始运行时就被指定了大小，与此不同，当调用像 malloc 和 free 这样的 C 标准库函数时，堆可以在运行时动态地扩展和收缩。 共享库大约在地址空间的中间部分是一块用来存放像 C 标准库和数学库这样的共享库的代码和数据的区域。共享库的概念非常强大，也相当难懂。 栈位于用户虚拟地址空间顶部的是用户栈，编译器用它来实现函数调用。和堆一样，用户栈在程序执行期间可以动态地扩展和收缩。特别地，每次我们调用一个函数时，栈就会增长；从一个函数返回时，栈就会收缩。 内核虚拟内存地址空间顶部的区域是为内核保留的。不允许应用程序读写这个区域的内容或者直接调用内核代码定义的函数。相反，它们必须调用内核来执行这些操作。 虚拟内存的运作需要硬件和操作系统软件之间精密复杂的交互，包括对处理器生成的每个地址的硬件翻译。基本思想是把一个进程虚拟内存的内容存储在磁盘上，然后用主存作为磁盘的高速缓存。\n文件 文件就是字节序列，仅此而已。每个I/O设备，包括磁盘、键盘、显示器，甚至网络，都可以看成是文件。系统中的所有输入输出都是通过使用一小组称为 Unix I/O 的系统函数调用读写文件来实现的。它（文件）向应用程序提供了一个统一的视图，来看待系统中可能含有的所有各式各样的I/O设备。\nAmdahl定律 对提升系统某一部分性能所带来的效果做出了简单却有效的观察。这个观察被称为Amdahl定律（Amdahls law）。该定律的主要思想是，当我们对系统的某个部分加速时，其对系统整体性能的影响取决于该部分的重要性和加速程度。若系统执行某应用程序所需要时间为Told。假设系统某部分所需执行时间与该时间的比例为a，而该部分性能提升的比例为k。即该部分初始所需时间为aTold，现在所需时间为aTold/k 。因此，总的执行时间为：\nTnew=(1-a)Told + (aTold)/k = Told[(1-a) + a/k]\n由此，可以计算加速比S=Told/Tnew\nS=1/((1-a) + a/k)\n💡 主要观点：要想显著加速整个系统，必须提升全系统中相当大的部分的速度。\n性能提升最好的表示防范就是用比例的形式Told/Tnew，其中，Told为原始系统所需时间，Tnew为修改后的系统所需时间。如果有所改进，则比值应大于1。我们用后缀“X”来表示比例，因此，“2.2X”读作“2.2倍”\n并发与并行 我们用的术语并发（concurrency）是一个通用的概念，指一个同时具有多个活动的系统；而术语并行（parallelism）指的是用并发来使一个系统运行得更快。并行可以在计算机系统的多个抽象层次上运用。在此，我们按照系统层次结构中由高到低的顺序重点强调三个层次。\n线程级并发 构建在进程这个抽象之上，我们能够设计出同时有多个程序执行的系统，这就导致了并发。使用线程，我们甚至能够在一个进程中执行多个控制流。自 20 世纪 60 年代初期出现时间共享以来，计算机系统中就开始有了对并发执行的支持。传统意义上，这种并发执行只是模拟出来的，是通过使一台计算机在它正在执行的进程间快速切换来实现的，就好像一个杂耍艺人保持多个球在空中飞舞一样。在以前，即使处理器必须在多个任务间切换，大多数实际的计算也都是由一个处理器来完成的。这种配置称为单处理器系统。\n多核处理器是将多个 CPU（称为“核”）集成到一个集成电路芯片上。图 1-17 描述的是一个典型多核处理器的组织结构，其中微处理器芯片有 4 个 CPU 核，每个核都有自己的 L1 和 L2 高速缓存，其中的 L1 高速缓存分为两个部分——一个保存最近取到的指令，另一个存放数据。这些核共享更高层次的高速缓存，以及到主存的接口。\n超线程，有时称为同时多线程（simultaneous multi-threading），是一项允许一个 CPU 执行多个控制流的技术。它涉及 CPU 某些硬件有多个备份，比如程序计数器和寄存器文件，而其他的硬件部分只有一份，比如执行浮点算术运算的单元。常规的处理器需要大约 20000 个时钟周期做不同线程间的转换，而超线程的处理器可以在单个周期的基础上决定要执行哪一个线程。这使得 CPU 能够更好地利用它的处理资源。比如，假设一个线程必须等到某些数据被装载到高速缓存中，那 CPU 就可以继续去执行另一个线程（时间空档切换）。举例来说，Intel Core i7 处理器可以让每个核执行两个线程，所以一个 4 核的系统实际上可以并行地执行 8 个线程。\n指令级并行 在较低的抽象层次上，现代处理器可以同时执行多条指令的属性称为指令级并行。早期的微处理器，如 1978 年的 Intel 8086，需要多个（通常是 3～10 个）时钟周期来执行一条指令。最近的处理器可以保持每个时钟周期 2～4 条指令的执行速率。其实每条指令从开始到结束需要长得多的时间，大约 20 个或者更多周期，但是处理器使用了非常多的聪明技巧来同时处理多达 100 条指令。在第 4 章中，我们会研究流水线（pipelining）的使用。在流水线中，将执行一条指令所需要的活动划分成不同的步骤，将处理器的硬件组织成一系列的阶段，每个阶段执行一个步骤。这些阶段可以并行地操作，用来处理不同指令的不同部分。我们会看到一个相当简单的硬件设计，它能够达到接近于一个时钟周期一条指令的执行速率。\n如果处理器可以达到比一个周期一条指令更快的执行速率，就称之为超标量（super-scalar）处理器。大多数现代处理器都支持超标量操作。第 5 章中，我们将描述超标量处理器的高级模型。应用程序员可以用这个模型来理解程序的性能。然后，他们就能写出拥有更高程度的指令级并行性的程序代码，因而也运行得更快。\n单指令、多数据并行 在最低层次上，许多现代处理器拥有特殊的硬件，允许一条指令产生多个可以并行执行的操作，这种方式称为单指令、多数据，即 SIMD 并行。例如，较新几代的 Intel 和 AMD 处理器都具有并行地对 8 对单精度浮点数（C 数据类型 float）做加法的指令。\n关于抽象 抽象的使用是计算机科学中最为重要的概念之一。\n此图表示计算机系统提供的一些抽象。计算机系统中的一个重大主题就是提供不同层次的抽象表示，来隐藏实际实现的复杂性\n操作系统的三个抽象 文件：是对 I/O 设备的抽象 虚拟内存：是对主存和磁盘的抽象 进程：是对处理器、主存和 I/O 设备的抽象 参考 csapp 电子书 深入理解计算机系统|学习笔记（二） CPU上下文切换 并发、并行和高并发 ","permalink":"https://fuyi-atlas.github.io/posts/computer/computer-system-roaming/","summary":"前言 此为深入理解计算机系统（csapp）第一章的笔记，也算是一个开始了。\nHello程序 #include \u0026lt;studio.h\u0026gt; int main() { printf(\u0026#34;hello world\\n\u0026#34;); return 0; } 基本思想 系统中所有的信息——包括磁盘文件、内存中的程序、内存中存放的用户数据以及网络上传输的数据，都是由一串比特（bit）表示的。区分不同数据对象的唯一方法是我们读到这些数据对象时的上下文。比如，在不同的上下文中，一个同样的字节序列可能表示一个整数、浮点数、字符串或者机器指令。\n要点 源程序实际上就是一个由0和1组成的位（或称为比特：bit）序列，8个位为一组，称为字节（byte）。\n大部分的现代计算机系统都使用ASCII标准来表示文本字符，这种方式实际上就是用一个唯一的单字节大小的整数值来表示每个字符\nhello.c程序是以字节序列的方式存储在文件中。每个字节都有一个整数值，对应于某些字符（注意：每个文本行都是以一个看不见的换行符“/n”来结束的）\n只由ASCII字符构成的文件称为文本文件，所有其他文件都称为二进制文件\n编译系统（compilation system）：编译的四个阶段\ngcc -o hello hello.c hello.c（文本） —\u0026gt; 预处理器[cpp] —\u0026gt; hello.i（文本） —\u0026gt; 编译器[ccl] —\u0026gt; hello.s（文本） —\u0026gt; 汇编器[as] —\u0026gt; hello.o（二进制） —\u0026gt; 链接器[ld] —\u0026gt; hello（二进制）\n预处理：预处理器处理#开头的命令，修改原始的C程序 编译：编译器将hello.i翻译为hello.s，即将源文件翻译为汇编语言。汇编语言是非常有用的，因为它为不同高级语言的不同编译器提供了通用的输出语言。例如，C编译器和Fortran编译器产生的输出文件用的都是一样的汇编语言 汇编：汇编器将hello.s翻译为机器语言指令，把这些指令打包成一种叫做可重定位目标程序（relocation object program）的格式，并将结果保存在目标文件hello.o中。hello.o是一个二进制文件（即将汇编内容翻译为机器语言指令，并将这些指令打包成一种叫做可重定位目标程序的格式） 链接：链接器用于处理合并。比如在hello程序中调用了printf函数，它是每个C编译器都提供的标准C库中的一个函数。printf函数存在于一个名为printf.o的单独的预编译好了的目标文件中，而这个文件必须以某种方式合并到hello.o程序中。链接器就负责处理这种合并。最终得到hello文件，它是一个可执行目标文件（或简称为可执行文件），可以被加载到内存中，由系统执行 shell是一个命令行解释器，它输出一个提示符，等待输入一个命令行，然后执行这个命令。如果该命令行的单词不是一个内置的命令，那么shell就会u假设这是一个可执行文件的名字，它将加载并运行这个文件。\n系统的硬件组成 CPU：中央处理单元； ALU：算术/逻辑单元； PC：程序计数器； USB：通用串行总线 总线 贯穿整个系统的是一组电子管道，称做总线，它携带信息字节并负责在各个部件间传递。硬件之间进行信息交流需要有一个统一的标准，也就是二进制信息传递规则，为了高效考虑，通常总线被设计成传送定长的字节块，也就是字（word）。字中的字节数（即字长）是一个基本的系统参数，在各个系统中的情况都不尽相同。现在的大多数机器字长要么是4个字节（32位），要么是8个字节（64位）。\nI/O设备 输入 / 输出（I/O）设备是系统与外部世界的联系通道。每个 I/O 设备都通过一个控制器或适配器与 I/O 总线相连。控制器与适配器之间的区别主要在于它们的封装方式。控制器是 I/O 设备本身或者系统的主印制电路板（通常称为主板）上的芯片组。而适配器则是一块插在主板插槽上的卡。无论如何，它们的功能都是在 I/O 总线和 I/O 设备之间传递信息。","title":"Computer System Roaming"},{"content":"前言 I have a dream too!\n开源技术赋予我们站在巨人的肩膀上做到更高更强的可能，我想通过开源技术来构建一个地理信息的世界（such as: one personal cloud gis），愿给地理信息数据带来更多的使用价值。当前主要关注空间数据库、数据处理与应用以及数据渲染，由内到外，逐步构建。既然都说人类活动所接触、产生的信息80%以上都与地理空间位置有关，那么这些空间数据就应该很容易的被使用，而不是仅被围困在专业领域内，不是吗？\nFuyi Atlas（拂衣志），取自 《侠客行》、Cloud Atlas以及Discovery Atlas。愿自己也能行此间小事，而非蹉跎岁月。\nFuyi Atlas Website将会使用Hugo + Github Pages进行构建，本文用于记录Fuyi Atlas的初始构建过程。\nHUGO是什么 Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nHugo是一个使用GO语言编写的，开源的，静态站点生成器。\n也就是说，你可以使用Hugo来快速构建你自己的静态站点。比如个人博客、知识库、产品介绍、文档等等。\n我此前使用vuepress来构建我的个人站点，接触Hugo之后，更愿意使用Hugo。那么我将使用Hugo代替vuepress来构建我的个人站点。理由有那么几个：\nHugo是一个纯粹的生成器 Hugo没有过多的生态依赖 Hugo更轻 据说更快，其LiveReload可以实现近实时的内容刷新 挺多可选的开源主题 使用更加简单 我也不是很喜欢node_modules\nFuyi Atlas Site Build Hugo Install Hugo is available in two editions: standard and extended. With the extended edition you can:\nEncode WebP images (you can decode WebP images with both editions) Transpile Sass to CSS using the embedded LibSass transpiler We recommend that you install the extended edition.\nHugo提供两种版本：标准版和拓展版，官方更加推荐使用拓展版本\nRepository packages Most Linux distributions maintain a repository for commonly installed applications. Please note that these repositories may not contain the latest release.\n我的主机操作系统是Fedora Linux 37(Workstation Edition)，所以直接使用存储库进行安装\nDerivatives of the Fedoradistribution of Linux include CentOS, Red Hat Enterprise Linux, and others. This will install the extended edition of Hugo:\nsudo dnf install hugo gudo version # hugo v0.98.0+extended linux/amd64 BuildDate=unknown 这里可以看到版本是0.98，这已经是一个比较旧的版本了，是22年4月份发布的，最新版本是0.110.0。那么我打算换一种方式安装\nBuild from source To build Hugo from source you must:\nInstall Git Install Go version 1.18 or later Update your PATH environment variable as described in the Go documentation The install directory is controlled by the GOPATH and GOBIN environment variables. If GOBIN is set, binaries are installed to that directory. If GOPATH is set, binaries are installed to the bin subdirectory of the first directory in the GOPATH list. Otherwise, binaries are installed to the bin subdirectory of the default GOPATH ($HOME/go or %USERPROFILE%\\go).\nThen build and test:\ngo install -tags extended github.com/gohugoio/hugo@latest hugo version # 使用root账户 sudo su - # 设置一下代理（clash for windwos） export https_proxy=http://127.0.0.1:7890;export http_proxy=http://127.0.0.1:7890;export all_proxy=socks5://127.0.0.1:7890 # 设置一下安装使用的环境变量 export GOBIN=/usr/local/go/bin/ # 安装hugo go install -tags extended github.com/gohugoio/hugo@latest # 验证 hugo version # hugo v0.110.0+extended linux/amd64 BuildDate=unknown 好了，目前已经是最新版本了\nSite Build 根据个人喜好（挑了许久），我最终选择PaperMod主题。\n后续的计划是：Hugo + PaperMod + Github Pages\n那么github仓库名称为：fuyi-atlas.github.io\n参见Quick Start以及PaperMod Installation说明，下面开始进行构建\nGit Clone\ngit clone https://github.com/fuyi-atlas/fuyi-atlas.github.io.git Create a site\nhugo new site fuyi-atlas.github.io --force Install theme\ncd fuyi-atlas.github.io git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod 将config.toml修改为hugo.yaml，并作出基础\nmv config.toml hugo.yaml # 将其中语法修改为yaml格式 baseURL: https://fuyi-atlas.github.io/ languageCode: en-us title: Fuyi Atlas theme: PaperMod 将config.toml修改为hugo.yaml，并作出基础\nhugo server 至此，Fuyi Atlas的初始构建便结束了。\nUpdate 上述环境是在Fedora Linux 37下执行的。由于部分软件在Linux无法使用，所以我现在又换到了Windows（Win10 + WSL2）\n在WSl2 + Debian中安装hugo与上述存在一定的差异\n代理设置 这里ip需要指向宿主机的IP，且Clash需要开启LAN访问（ALLOW LAN）\nexport https_proxy=http://192.168.199.121:7890;export http_proxy=http://192.168.199.121:7890;export all_proxy=socks5://192.168.199.121:7890 没有预装GCC\napt install build-essential GO编译相关的环境变量设置\nexport GOOS=\u0026#34;linux\u0026#34; export CGO_ENABLED=\u0026#34;1\u0026#34; 同时，git submodule需要手动同步一下\ngit submodule update --init --recursive @update_time 2023.05.22\n参考 Github Pages\nGitHub Pages 使用入门\nHugo Documentation\nGetting Started With Hugo | FREE COURSE\nPaperMod Installation | Update\n云图 – 云计算图志\n适用于 Linux 的 Windows 子系统文档\nDev on Windows with WSL\n为 WSL2 一键设置代理\ngo build编译失败：imports xxx/xxx/xxx: build constraints exclude all Go files in xxx/xxx/xxx\ngit-github 子模块仓库更新（git submodule）/git中submodule子模块的添加、使用和删除\n","permalink":"https://fuyi-atlas.github.io/posts/other/fuyi-atlas-init/","summary":"前言 I have a dream too!\n开源技术赋予我们站在巨人的肩膀上做到更高更强的可能，我想通过开源技术来构建一个地理信息的世界（such as: one personal cloud gis），愿给地理信息数据带来更多的使用价值。当前主要关注空间数据库、数据处理与应用以及数据渲染，由内到外，逐步构建。既然都说人类活动所接触、产生的信息80%以上都与地理空间位置有关，那么这些空间数据就应该很容易的被使用，而不是仅被围困在专业领域内，不是吗？\nFuyi Atlas（拂衣志），取自 《侠客行》、Cloud Atlas以及Discovery Atlas。愿自己也能行此间小事，而非蹉跎岁月。\nFuyi Atlas Website将会使用Hugo + Github Pages进行构建，本文用于记录Fuyi Atlas的初始构建过程。\nHUGO是什么 Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nHugo是一个使用GO语言编写的，开源的，静态站点生成器。\n也就是说，你可以使用Hugo来快速构建你自己的静态站点。比如个人博客、知识库、产品介绍、文档等等。\n我此前使用vuepress来构建我的个人站点，接触Hugo之后，更愿意使用Hugo。那么我将使用Hugo代替vuepress来构建我的个人站点。理由有那么几个：\nHugo是一个纯粹的生成器 Hugo没有过多的生态依赖 Hugo更轻 据说更快，其LiveReload可以实现近实时的内容刷新 挺多可选的开源主题 使用更加简单 我也不是很喜欢node_modules\nFuyi Atlas Site Build Hugo Install Hugo is available in two editions: standard and extended.","title":"Fuyi Atlas Init"},{"content":"前言 拖延症真的存在！！！\n今天是2023年2月13日晚，我在此时写下本文的第二行内容。其实从年前就开始计划写一篇关于2022年的年终总结，无奈受到拖延病毒的威胁，一直拖到现在才暂时摆脱控制。\n如题，本文将对2022年进行简要总结，同时对2023年做一个初步的展望（仅作记录）。\n2022年大事记 第一个在外过的春节 受新冠疫情影响，2022年的春节是在杭州过的。还记得当时附近好几个地方都被划为了高风险，对整个区进行了管控。如果选择回家的话，得到将是14天的隔离，还不确定能否回来上班。因此便没有回去了。\n好在所在的区域情况并没有那么的严重，还是可以去买菜的。领了消费券，再加上公司发的年货，也没有想象中的那么糟糕。\n拂衣天气 集地理信息与天气预报为一体的天气预报类小程序，界面精美，使用便捷。【致敬：和风天气】\n2022年，是我工作的第四个年头。受多方面的信息影响，我也想看看验证自己是否有进入全栈开发的能力。该项目从2022年1月12号正式启动，于2022年3月19日发布一阶段最终版本（1.1.9），总体耗时2个月零7天。从内容完整度以及界面友好程度来说，我给自己70分。\n其实最开始是想基于webview来实现更多的地图方面的特性的，但webview对个人认证不开放。\n在完成之后，本来计划是将拂衣天气完整的开发过程通过文章的方式记录下来，并将该项目开源出去，甚至还想将该项目提交给和风天气。但最后的结果就是：文章就完整了四篇，也就是一个开头，没啥实际内容。\n那么原因是什么呢？经过深深的反思过后，主要的原因有那么几点：\n拂衣天气的实现太乱了，本想借着文章梳理重构之后再开源 同阶段想要做的事情太多了 拖延症后期患者 对，都是借口，就是懒 WebGIS项目开发 我是2021年5月份进入的现在的这家公司，任职岗位是GIS开发工程师，主要负责服务端GIS内容的实现。从入职后到今年年初，一直在做一个小程序，算是公司的一个实验性产品，和已有产品的实际关联不大。\n从3月份开始，将基于既有的产品实现，结合新项目的特性以及通用性要求，实现产品的升级。这对我来说，也算是一个重要的时间节点。因为自此我算是真正进入了WebGIS开发领域，以服务端开发为切入点。\n项目部署工作 大概是22年的7-8月份，上面提到的WebGIS项目实现将作为基线产品的一部分，为后续的项目应用提供支持。而后面我将会同时负责WebGIS服务端的项目部署工作。\n截至到2023年初，我已经经历了很多个项目。思来想去，我觉得这对我来说也是一个有意义的时间节点。从项目应用开发到基线产品，再到基线产品项目应用部署。一方面也算是证明了当初选择的路线，同时也更加坚定。另一方面，在疫情时代，公司能有项目那应该是好事，虽然我大抵是不喜欢做部署工作。\n公司年会 对，这也是另一个重要的时间节点。2022年12月4号晚，发布了《杭州市关于优化调整疫情防控相关措施的通告》。2023年1月13日下午，公司举办了年会。这么结合起来看，是不是看起来明年会更好！\n今年其实发生了挺多的事情，自己在心态上也发生了一定的变化（主要指工作方面）。从3月的充满希望，到年底的身心俱疲。正好借年会的氛围想了想，我觉得可以换一种心态开启2023年。当初选择来这里，也就是因为自己的路线规划与公司的技术发展方向是基本一致的，到今天公司并没有发生较大的变化。那么既然是自己想要做的，那么就更应该去做好，主动去做，你是在借助公司的资源实现自我的目标，你甚至可以尝试去干预以符合你的目标。未来的岳父大人也给我说：有的时候，不要计较那么多，即使辛苦一点也没关系，但是你要清楚你在做什么。\n做了再多，不如做好一个，这就是你最好的名片。——自我PUA\n2023展望 技术 围绕地理信息，更加深入底层实现。以空间数据库为核心，以数据处理与使用为重点，以数据渲染为次重点，由内到外，逐步构建技术体系。\n写作 2022年的成果惨淡，整整一年只有仅仅四篇文章，还都是拂衣天气的内容。\n希望从2023年开始，可以实现一周双更，单更是底线。同时还要能够提高文字表达的能力。\n生活 可以一夜暴富吗？\n后记 第一次更新：2023年2月13日晚，完成前言编写\n第二次更新：2023年2月18日下午，完稿\n","permalink":"https://fuyi-atlas.github.io/posts/other/2022-summary/","summary":"前言 拖延症真的存在！！！\n今天是2023年2月13日晚，我在此时写下本文的第二行内容。其实从年前就开始计划写一篇关于2022年的年终总结，无奈受到拖延病毒的威胁，一直拖到现在才暂时摆脱控制。\n如题，本文将对2022年进行简要总结，同时对2023年做一个初步的展望（仅作记录）。\n2022年大事记 第一个在外过的春节 受新冠疫情影响，2022年的春节是在杭州过的。还记得当时附近好几个地方都被划为了高风险，对整个区进行了管控。如果选择回家的话，得到将是14天的隔离，还不确定能否回来上班。因此便没有回去了。\n好在所在的区域情况并没有那么的严重，还是可以去买菜的。领了消费券，再加上公司发的年货，也没有想象中的那么糟糕。\n拂衣天气 集地理信息与天气预报为一体的天气预报类小程序，界面精美，使用便捷。【致敬：和风天气】\n2022年，是我工作的第四个年头。受多方面的信息影响，我也想看看验证自己是否有进入全栈开发的能力。该项目从2022年1月12号正式启动，于2022年3月19日发布一阶段最终版本（1.1.9），总体耗时2个月零7天。从内容完整度以及界面友好程度来说，我给自己70分。\n其实最开始是想基于webview来实现更多的地图方面的特性的，但webview对个人认证不开放。\n在完成之后，本来计划是将拂衣天气完整的开发过程通过文章的方式记录下来，并将该项目开源出去，甚至还想将该项目提交给和风天气。但最后的结果就是：文章就完整了四篇，也就是一个开头，没啥实际内容。\n那么原因是什么呢？经过深深的反思过后，主要的原因有那么几点：\n拂衣天气的实现太乱了，本想借着文章梳理重构之后再开源 同阶段想要做的事情太多了 拖延症后期患者 对，都是借口，就是懒 WebGIS项目开发 我是2021年5月份进入的现在的这家公司，任职岗位是GIS开发工程师，主要负责服务端GIS内容的实现。从入职后到今年年初，一直在做一个小程序，算是公司的一个实验性产品，和已有产品的实际关联不大。\n从3月份开始，将基于既有的产品实现，结合新项目的特性以及通用性要求，实现产品的升级。这对我来说，也算是一个重要的时间节点。因为自此我算是真正进入了WebGIS开发领域，以服务端开发为切入点。\n项目部署工作 大概是22年的7-8月份，上面提到的WebGIS项目实现将作为基线产品的一部分，为后续的项目应用提供支持。而后面我将会同时负责WebGIS服务端的项目部署工作。\n截至到2023年初，我已经经历了很多个项目。思来想去，我觉得这对我来说也是一个有意义的时间节点。从项目应用开发到基线产品，再到基线产品项目应用部署。一方面也算是证明了当初选择的路线，同时也更加坚定。另一方面，在疫情时代，公司能有项目那应该是好事，虽然我大抵是不喜欢做部署工作。\n公司年会 对，这也是另一个重要的时间节点。2022年12月4号晚，发布了《杭州市关于优化调整疫情防控相关措施的通告》。2023年1月13日下午，公司举办了年会。这么结合起来看，是不是看起来明年会更好！\n今年其实发生了挺多的事情，自己在心态上也发生了一定的变化（主要指工作方面）。从3月的充满希望，到年底的身心俱疲。正好借年会的氛围想了想，我觉得可以换一种心态开启2023年。当初选择来这里，也就是因为自己的路线规划与公司的技术发展方向是基本一致的，到今天公司并没有发生较大的变化。那么既然是自己想要做的，那么就更应该去做好，主动去做，你是在借助公司的资源实现自我的目标，你甚至可以尝试去干预以符合你的目标。未来的岳父大人也给我说：有的时候，不要计较那么多，即使辛苦一点也没关系，但是你要清楚你在做什么。\n做了再多，不如做好一个，这就是你最好的名片。——自我PUA\n2023展望 技术 围绕地理信息，更加深入底层实现。以空间数据库为核心，以数据处理与使用为重点，以数据渲染为次重点，由内到外，逐步构建技术体系。\n写作 2022年的成果惨淡，整整一年只有仅仅四篇文章，还都是拂衣天气的内容。\n希望从2023年开始，可以实现一周双更，单更是底线。同时还要能够提高文字表达的能力。\n生活 可以一夜暴富吗？\n后记 第一次更新：2023年2月13日晚，完成前言编写\n第二次更新：2023年2月18日下午，完稿","title":"2022, 迟到的年终总结"},{"content":"前言 本文用于说明本次开发所使用的环境，以及环境的搭建过程。\n操作系统 Windows 10 专业版\n其实我当时使用的操作系统的Arch Linux，开发完成后才又重装回Windows。\n现在又用回了Fedora 38 Workstation @time 2023.10.07\n服务端 服务端使用Java语言进行开发，项目构建使用Maven(Gradle)，开发工具使用Idea，服务发布使用Docker，下面是具体的版本：\nJDK: OpenJDK 11 Gradle: 7.3.2 Maven: Idea: IntelliJ IDEA 2022.1.3 (Community Edition) Docker: Linx Docker Desktop Version 4.23.0 (120376) [Engine: 24.0.6] 小程序 首先需要自行完成小程序的注册，具体可以参考官方文档。\n其次，下载并安装小程序开发工具，参见下载页面。\n最后，调试基础库选择：2.24.6\n参考 开发者工具中无法显示echart或显示异常\n在MobaXterm进入ssh终端后，键入上下左右出现乱码\nwin10上修改docker的镜像文件存储位置\nWSL2 迁移 Docker 镜像存储位置\n说明 如有冒犯，我在这里先向您道歉，还请联系我进行处理\nemail: thread_zhou@126.com\n","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/004/","summary":"前言 本文用于说明本次开发所使用的环境，以及环境的搭建过程。\n操作系统 Windows 10 专业版\n其实我当时使用的操作系统的Arch Linux，开发完成后才又重装回Windows。\n现在又用回了Fedora 38 Workstation @time 2023.10.07\n服务端 服务端使用Java语言进行开发，项目构建使用Maven(Gradle)，开发工具使用Idea，服务发布使用Docker，下面是具体的版本：\nJDK: OpenJDK 11 Gradle: 7.3.2 Maven: Idea: IntelliJ IDEA 2022.1.3 (Community Edition) Docker: Linx Docker Desktop Version 4.23.0 (120376) [Engine: 24.0.6] 小程序 首先需要自行完成小程序的注册，具体可以参考官方文档。\n其次，下载并安装小程序开发工具，参见下载页面。\n最后，调试基础库选择：2.24.6\n参考 开发者工具中无法显示echart或显示异常\n在MobaXterm进入ssh终端后，键入上下左右出现乱码\nwin10上修改docker的镜像文件存储位置\nWSL2 迁移 Docker 镜像存储位置\n说明 如有冒犯，我在这里先向您道歉，还请联系我进行处理\nemail: thread_zhou@126.com","title":"微天气-开发环境准备"},{"content":"前言 这是一个前后端分离的项目，后端使用Java进行开发，而前端通过微信小程序实现。\n功能结构 可从上图得知，部分功能已去除：\n消息 消息推送 紧急情况推送 用户 个人中心 模型设计 用户信息（UserInfo） id Long 主键 oid String OpenID uid String UnionID name String 昵称 phone_num String 手机号 avatar String 头像地址 authState String 登录状态 Silence 静默登录（目前程序的访问是需要存在登录态的） Authorized 已授权 createTime Timestamp 记录创建时间 updateTime Timestamp 记录更新时间 行政区划信息（DistrictInfo） 使用全国行政区划信息填充，含空间数据，数据粒度到区县。目前仅支持国内数据。\nid Long 主键 name String 行政区名称 grade Integer 行政区级别 1：省级行政区 2：地级行政区 3：县级行政区 4：乡镇级行政区 code String 行政区代码 centerPoint Point 行政区中心点（空间数据） bounds Geometry 行政区边界（空间数据） 关注城市（FollowCity） id Long 主键 userId Long 用户ID districtId Long 行政区划ID districtName String 行政区划名称 districtCode String 行政区划编码 orderNum Integer 序号，自然数，从1开始 createTime Timestamp 记录创建时间 天气信息 由于天气数据均来自第三方，目前数据格式于和风天气对齐。\n通过对和风天气响应数据结构研究，抽象出基础响应，实际响应均继承该基础响应类，并实现getDataColumn方法，以标记对应数据列。\n目前使用代理方式进行第三方接口调用，且将结构对齐交由前端进行，所以此处并未进行强格式定义。\n基础响应信息 package org.fuyi.weather.infra.common.entity; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import java.io.Serializable; /** * 天气代理实体基础类 * * @author: \u0026lt;a href=\u0026#34;mailto:thread.zhou@gmail.com\u0026#34;\u0026gt;Fuyi\u0026lt;/a\u0026gt; * @time: 2022/2/1 下午3:22 * @since: 1.0 */ public abstract class AbstractWeatherProxyWrappedEntity implements Serializable { private static final long serialVersionUID = 5559333755209092109L; /** * API状态码，具体含义请参考状态码 * 参考 \u0026lt;a\u0026gt;https://dev.qweather.com/docs/resource/status-code/\u0026lt;/a\u0026gt; */ private String code; /** * 当前API的最近更新时间 * 参考 \u0026lt;a\u0026gt;https://dev.qweather.com/docs/resource/glossary/#update-time\u0026lt;/a\u0026gt; */ private String updateTime; /** * 当前数据的响应式页面，便于嵌入网站或应用 */ private String fxLink; private JSONObject refer; private JSON data; public AbstractWeatherProxyWrappedEntity() { } // 标记数据列 public abstract String getDataColumn(); // get and set method. ... } 实时空气数据（例） package org.fuyi.weather.domain.entity; import lombok.*; import org.fuyi.weather.infra.common.constant.WeatherProxyConstant; import org.fuyi.weather.infra.common.entity.AbstractWeatherProxyWrappedEntity; /** * @author: \u0026lt;a href=\u0026#34;mailto:thread.zhou@gmail.com\u0026#34;\u0026gt;Fuyi\u0026lt;/a\u0026gt; * @time: 2022/2/1 下午4:07 * @since: 1.0 */ @Data @Builder @NoArgsConstructor @ToString(callSuper = true) @EqualsAndHashCode(callSuper = false) public class AirRealTimeEntity extends AbstractWeatherProxyWrappedEntity { @Override public String getDataColumn() { // WeatherProxyConstant.DataColumn.NOW --\u0026gt; \u0026#34;now\u0026#34; return WeatherProxyConstant.DataColumn.NOW; } } 参考 小程序静默登录方案设计 说明 如有冒犯，我在这里先向您道歉，还请联系我进行处理\nemail: thread_zhou@126.com\n","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/003/","summary":"前言 这是一个前后端分离的项目，后端使用Java进行开发，而前端通过微信小程序实现。\n功能结构 可从上图得知，部分功能已去除：\n消息 消息推送 紧急情况推送 用户 个人中心 模型设计 用户信息（UserInfo） id Long 主键 oid String OpenID uid String UnionID name String 昵称 phone_num String 手机号 avatar String 头像地址 authState String 登录状态 Silence 静默登录（目前程序的访问是需要存在登录态的） Authorized 已授权 createTime Timestamp 记录创建时间 updateTime Timestamp 记录更新时间 行政区划信息（DistrictInfo） 使用全国行政区划信息填充，含空间数据，数据粒度到区县。目前仅支持国内数据。\nid Long 主键 name String 行政区名称 grade Integer 行政区级别 1：省级行政区 2：地级行政区 3：县级行政区 4：乡镇级行政区 code String 行政区代码 centerPoint Point 行政区中心点（空间数据） bounds Geometry 行政区边界（空间数据） 关注城市（FollowCity） id Long 主键 userId Long 用户ID districtId Long 行政区划ID districtName String 行政区划名称 districtCode String 行政区划编码 orderNum Integer 序号，自然数，从1开始 createTime Timestamp 记录创建时间 天气信息 由于天气数据均来自第三方，目前数据格式于和风天气对齐。","title":"微天气-模型设计"},{"content":"前言 俗话说：磨刀不误砍柴工。\n我想做一个天气类别的小程序，以此进行全栈开发能力的试炼。我想这会是一个微信小程序、是一个可以正常使用的小程序，以Java进行服务端开发，以Mapbox实现天气数据可视化。\n但是我是一个后端开发工程师，我不怎么会写页面，我特别的讨厌写CSS。我也没有接触过微信小程序开发，也仅仅知道过Mapbox可以实现好看的地图。所以我需要进行一定的预研，避免后期花费更大的精力用来调整本可以避免的问题。\n下面是我计划实现的功能列表：\n利用网络接口获取数据（昨日天气、当前天气、预报天气） 实现实时天气与预报数据查看 紧急情况推送 利用地图（mapbox）进行数据可视化 天气分享（图片分享，页面分享） 常用地址 消息推送 个人信息 登录授权 应用Promise进行异步网络请求 使用阿里巴巴矢量图标库作为图标数据源 echarts图标展示 接入疫情数据？ 预研对象 天气数据 天气数据需要是真实的、可用的。那么可以通过网络中提供的天气API进行获取。\n通过一定的检索后，我选定了两个天气平台，分别是：和风天气、心知天气。\n高德天气：大平台，但是目前服务类目比较少\n彩云天气：免费接口几乎没有，收费又太贵\n心知天气 心知天气试用版与开发者版开发产品几乎等同，且开发者版收费也不贵。最为关键的是，支持以经纬度方式进行天气查询。\n和风天气 几乎可以免费使用其提供的所有 API，且同样支持经纬度方式进行天气查询。\n小结 对比了这两者，发现至少都需要注册为开发者之后，才可以较好的使用其服务。且两者的开发者认证均需要实名。\n关于天气API的选择，我最终选择了和风天气，倒不是因为它可以免费使用。其实，刚开始的时候我更倾向于使用心知天气，因为它还可以直接查昨天的天气（和风对于历史天气的查询比较麻烦）。但是和风天气首页结合了地图进行可视化，而且还提供有APP可以使用（方便参考）。再加上，我想了想，其实我并没有迫切的需要知道昨天的天气情况。\n其实最重要的原因在于：我先注册了心知天气（需要审核），过了半天后再去注册了和风天气（需要审核），但是最先通过审核的是和风天气（耗时大概也就半天左右，我是在春节期间注册的）。\n前端技术 我并没有想要精通前端技术，但是我需要比较体系的了解一下前端技术，方便进行小程序开发。所以我在B站找了两门前端视频学习（粗略的刷了一遍）：\n【尚硅谷】Web前端零基础入门HTML5+CSS3基础教程丨初学者从入门到精通 千锋web前端开发项目教程_1000集完全零基础入门HTML5+CSS3+JS到精通 微信小程序 官方文档足以\nMapbox 这里存在一个遗憾，小程序原生并不支持使用如mapbox这样的第三方地图框架，初始想法是通过webview的方式使用mapbox，但是遗憾的是，webview并不对个人类型的小程序开放使用。\n所以，退而求其次，选择腾讯地图（及其提供的样式）实现地图浏览。\nUI UI部分参考和风天气APP，以及WEUI\n总结 天气API使用和风天气（若有空余可考虑抽取一套统一的API，可组合或切换数据来源） 地图使用腾讯地图（微信小程序解决方案，主要在于个性化样式的使用） 基于自定义服务端实现天气代理以及小程序静默登录 UI参考和风天气APP实现 参考 最好的6个免费天气 API 接口对比测评 心知天气-价格方案 和风天气-FAQ H5+Javascript技术结构图 腾讯位置服务-微信小程序解决方案-个性化地图样式 说明 如有冒犯，我在这里先向您道歉，还请联系我进行处理\nemail: thread_zhou@126.com\n","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/002/","summary":"前言 俗话说：磨刀不误砍柴工。\n我想做一个天气类别的小程序，以此进行全栈开发能力的试炼。我想这会是一个微信小程序、是一个可以正常使用的小程序，以Java进行服务端开发，以Mapbox实现天气数据可视化。\n但是我是一个后端开发工程师，我不怎么会写页面，我特别的讨厌写CSS。我也没有接触过微信小程序开发，也仅仅知道过Mapbox可以实现好看的地图。所以我需要进行一定的预研，避免后期花费更大的精力用来调整本可以避免的问题。\n下面是我计划实现的功能列表：\n利用网络接口获取数据（昨日天气、当前天气、预报天气） 实现实时天气与预报数据查看 紧急情况推送 利用地图（mapbox）进行数据可视化 天气分享（图片分享，页面分享） 常用地址 消息推送 个人信息 登录授权 应用Promise进行异步网络请求 使用阿里巴巴矢量图标库作为图标数据源 echarts图标展示 接入疫情数据？ 预研对象 天气数据 天气数据需要是真实的、可用的。那么可以通过网络中提供的天气API进行获取。\n通过一定的检索后，我选定了两个天气平台，分别是：和风天气、心知天气。\n高德天气：大平台，但是目前服务类目比较少\n彩云天气：免费接口几乎没有，收费又太贵\n心知天气 心知天气试用版与开发者版开发产品几乎等同，且开发者版收费也不贵。最为关键的是，支持以经纬度方式进行天气查询。\n和风天气 几乎可以免费使用其提供的所有 API，且同样支持经纬度方式进行天气查询。\n小结 对比了这两者，发现至少都需要注册为开发者之后，才可以较好的使用其服务。且两者的开发者认证均需要实名。\n关于天气API的选择，我最终选择了和风天气，倒不是因为它可以免费使用。其实，刚开始的时候我更倾向于使用心知天气，因为它还可以直接查昨天的天气（和风对于历史天气的查询比较麻烦）。但是和风天气首页结合了地图进行可视化，而且还提供有APP可以使用（方便参考）。再加上，我想了想，其实我并没有迫切的需要知道昨天的天气情况。\n其实最重要的原因在于：我先注册了心知天气（需要审核），过了半天后再去注册了和风天气（需要审核），但是最先通过审核的是和风天气（耗时大概也就半天左右，我是在春节期间注册的）。\n前端技术 我并没有想要精通前端技术，但是我需要比较体系的了解一下前端技术，方便进行小程序开发。所以我在B站找了两门前端视频学习（粗略的刷了一遍）：\n【尚硅谷】Web前端零基础入门HTML5+CSS3基础教程丨初学者从入门到精通 千锋web前端开发项目教程_1000集完全零基础入门HTML5+CSS3+JS到精通 微信小程序 官方文档足以\nMapbox 这里存在一个遗憾，小程序原生并不支持使用如mapbox这样的第三方地图框架，初始想法是通过webview的方式使用mapbox，但是遗憾的是，webview并不对个人类型的小程序开放使用。\n所以，退而求其次，选择腾讯地图（及其提供的样式）实现地图浏览。\nUI UI部分参考和风天气APP，以及WEUI\n总结 天气API使用和风天气（若有空余可考虑抽取一套统一的API，可组合或切换数据来源） 地图使用腾讯地图（微信小程序解决方案，主要在于个性化样式的使用） 基于自定义服务端实现天气代理以及小程序静默登录 UI参考和风天气APP实现 参考 最好的6个免费天气 API 接口对比测评 心知天气-价格方案 和风天气-FAQ H5+Javascript技术结构图 腾讯位置服务-微信小程序解决方案-个性化地图样式 说明 如有冒犯，我在这里先向您道歉，还请联系我进行处理\nemail: thread_zhou@126.com","title":"微天气-技术预研"},{"content":"前言 天气小程序产生于2022年年初，目的是用于验证自己是否有进入全栈开发（仅前后端）的能力。该项目从2022年1月12号正式启动，于2022年3月19日发布一阶段最终版本（1.1.9），总体耗时2个月零7天。从内容完整度以及界面友好程度来说，我给自己70分。\n完成内容 和风天气API接入，可实现实时天气、实时空气、24小时天气预报、7天天气预报 使用echarts for wechat进行24小时天气预报展示 通过腾讯位置服务提供的微信小程序解决方案实现地图个性化展示（目前使用风格：白浅） 通过自建服务实现小程序静默登录 通过自建服务实现关注城市的持久化管理 实现天气分享功能，通过生成一张图片进行分享，可直接分享给朋友或群组 默认通过定位获取所在位置的天气数据 天气数据均通过服务端代理进行获取，从而避免相关key直接暴露在客户端 提供通过经纬度查询所处行政区划的服务，提供行政区划查询服务，皆可获取对应行政区划中心点基于边界数据，数据坐标为4326 服务通过github action自动化进行docker构建，并推送到阿里云镜像服务仓库，而后在阿里云ecs中直接拉取docker进行部署 图片服务则是通过nginx实现反向代理，图片由docker容器内服务创建，通过docker文件映射功能映射到云主机，再通过nginx可实现图片的访问 服务与图片均实现域名访问，且均提供ssl证书 不足 未实现天气地图可视化，即基于mapbox进行地图可视化（webview需要企业认证资质） 未提供坐标转换功能，因目前使用的是腾讯地图（gcj02，即从地图获取的数据均为gcj02），行政区划数据为4326（wgs84），目前是直接将4326作为gcj02进行使用（因为数据的粒度为区县，所以差异不会很大，除非在行政区边界处可能会出现行政区显示错误问题） 未完整实现小程序朋友圈分享功能，因该功能需要所分享页面的数据可直接获取，且分享页面小程序登录功能已被限制，所以目前无法直接提供数据获取服务（需要进行登录态校验，考虑到被攻击的可能） 未完成天气预警数据接入与提示功能 未完成数据推送、个人主页、图层管理功能 分享图片中未实现天气云图叠加功能，仅获取了所在位置范围的影像图与相关注记 服务端代码封装度不够，且DDD的认知不足，导致实践乱七八糟 内容 我计划将拂衣天气开发的完整过程通过文章的方式记录下来，下面是我对该整体内容的编写计划：\nName Key Words Summary 序章 前期调研 模型设计 开发环境准备 行政区划数据准备 小程序静默登陆实现 小程序开发（一） 说明主页布局构建，组件拆分情况，实现地图加载以及定位 小程序开发（二） 实现上下滑动功能开发，以及左右滑动组件封装 小程序开发（三） 实时天气栏卡片、文字描述、底部7天天气预报以及Footer部分开发 小程序开发（四） 完成echarts组件封装 天气代理服务开发 提供实时天气、实时空气质量、24小时天气预报、7天天气预报代理服务开发 小程序开发（五） 完成天气部分API对接 行政区划服务开发 提供行政区划查询服务 关注城市服务开发 提供对城市的关注与取消关注能力 小程序开发（六） 完成天气图片分享功能 小程序开发（七） 完成城市选择页面开发与数据API对接 Github Action服务发布 小程序服务发布 终篇 参考 注：拂衣天气小程序的界面设计与交互设计主要参考和风天气APP。当前所述拂衣天气小程序的开发主要用于学习。\n致敬和风天气！\n和风天气 和风天气开发平台 ","permalink":"https://fuyi-atlas.github.io/posts/program/micro-weather/001/","summary":"前言 天气小程序产生于2022年年初，目的是用于验证自己是否有进入全栈开发（仅前后端）的能力。该项目从2022年1月12号正式启动，于2022年3月19日发布一阶段最终版本（1.1.9），总体耗时2个月零7天。从内容完整度以及界面友好程度来说，我给自己70分。\n完成内容 和风天气API接入，可实现实时天气、实时空气、24小时天气预报、7天天气预报 使用echarts for wechat进行24小时天气预报展示 通过腾讯位置服务提供的微信小程序解决方案实现地图个性化展示（目前使用风格：白浅） 通过自建服务实现小程序静默登录 通过自建服务实现关注城市的持久化管理 实现天气分享功能，通过生成一张图片进行分享，可直接分享给朋友或群组 默认通过定位获取所在位置的天气数据 天气数据均通过服务端代理进行获取，从而避免相关key直接暴露在客户端 提供通过经纬度查询所处行政区划的服务，提供行政区划查询服务，皆可获取对应行政区划中心点基于边界数据，数据坐标为4326 服务通过github action自动化进行docker构建，并推送到阿里云镜像服务仓库，而后在阿里云ecs中直接拉取docker进行部署 图片服务则是通过nginx实现反向代理，图片由docker容器内服务创建，通过docker文件映射功能映射到云主机，再通过nginx可实现图片的访问 服务与图片均实现域名访问，且均提供ssl证书 不足 未实现天气地图可视化，即基于mapbox进行地图可视化（webview需要企业认证资质） 未提供坐标转换功能，因目前使用的是腾讯地图（gcj02，即从地图获取的数据均为gcj02），行政区划数据为4326（wgs84），目前是直接将4326作为gcj02进行使用（因为数据的粒度为区县，所以差异不会很大，除非在行政区边界处可能会出现行政区显示错误问题） 未完整实现小程序朋友圈分享功能，因该功能需要所分享页面的数据可直接获取，且分享页面小程序登录功能已被限制，所以目前无法直接提供数据获取服务（需要进行登录态校验，考虑到被攻击的可能） 未完成天气预警数据接入与提示功能 未完成数据推送、个人主页、图层管理功能 分享图片中未实现天气云图叠加功能，仅获取了所在位置范围的影像图与相关注记 服务端代码封装度不够，且DDD的认知不足，导致实践乱七八糟 内容 我计划将拂衣天气开发的完整过程通过文章的方式记录下来，下面是我对该整体内容的编写计划：\nName Key Words Summary 序章 前期调研 模型设计 开发环境准备 行政区划数据准备 小程序静默登陆实现 小程序开发（一） 说明主页布局构建，组件拆分情况，实现地图加载以及定位 小程序开发（二） 实现上下滑动功能开发，以及左右滑动组件封装 小程序开发（三） 实时天气栏卡片、文字描述、底部7天天气预报以及Footer部分开发 小程序开发（四） 完成echarts组件封装 天气代理服务开发 提供实时天气、实时空气质量、24小时天气预报、7天天气预报代理服务开发 小程序开发（五） 完成天气部分API对接 行政区划服务开发 提供行政区划查询服务 关注城市服务开发 提供对城市的关注与取消关注能力 小程序开发（六） 完成天气图片分享功能 小程序开发（七） 完成城市选择页面开发与数据API对接 Github Action服务发布 小程序服务发布 终篇 参考 注：拂衣天气小程序的界面设计与交互设计主要参考和风天气APP。当前所述拂衣天气小程序的开发主要用于学习。\n致敬和风天气！\n和风天气 和风天气开发平台 ","title":"微天气-序章"},{"content":"1. 为什么 Java 需要运行时环境 1.1. Java 程序的启动方式 IDE中启动，比如：Eclipse、IntelliJ IDEA 构建为 jar，通过命令行的方式启动，比如：java -jar application.jar 使用构建工具（如：Gradle、Maven）启动，比如 SpringBoot 应用启动：gradle bootRun、mvn spring-boot:run 1.2. JRE 是什么 在这里引用极客时间课程Java核心技术面试精讲中的一段话\n我们日常会接触到 JRE（Java Runtime Environment） 或者 JDK（Java Development Kit）。JRE，也就是 Java 运行时环境，仅包含运行 Java 程序的必须组件，包括 Java 虚拟机以及 Java 核心类库等。而 JDK 可以看作是 JRE 的一个超集，提供了更多工具，比如编译器、各种诊断工具等。\n1.3. 为什么需要运行时环境 不是有这么一句话么，计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决。话说回来，Java 代码运行之所以需要运行时环境，主要是由于以下几个方面的原因：\nJava 语言语法非常复杂，抽象程度高，直接在硬件上运行这种复杂的程序并不现实（并不是不可以，但是这样造成与相应硬件的强耦合，且不便于抽象和复杂语法的实现。），所以需要在运行之前进行一番转换。\n实现平台无关性、做到 “Write once, run anywhere”，获得跨平台的能力。这样便需要一个中间层进行解耦，达到上层统一编码、下层跨越平台、中间实现兼容（所以，Java 语言的跨平台特性是由 Java 运行时环境实现的。也就是在不同平台皆有与之相对应的 Java 运行时环境，实现相同定义、不同实现。这样的思想是不是很熟悉，当然，这仅是我的理解）。\n提供托管环境（Managed Runtime），该托管环境可以代替我们处理一些通用的、容易出错的、高难度的行为，比如自动内存管理、垃圾回收、安全权限动态检测等。\n2. Java 代码在虚拟机中是怎样运行的 2.1. 虚拟机视角 执行 Java 代码首先需要将它编译而成的 class 文件加载到 Java 虚拟机中，加载后的 Java 类会被存放到方法区中。实际运行时，虚拟机会执行方法区内的代码（需要将字节码翻译为机器码，在 HotSpot 实现中，有解释执行和即时编译两种）。\n下图为 Java虚拟机的整体内存结构图（并不细致），从图中可以看到，Java 虚拟机将栈细分为 Java 方法栈（Java 虚拟机栈）和面向本地方法（用 C++ 写的 native 方法）的本地方法栈，以及存放各个线程的 PC 寄存器（每个线程都有自己的 PC 寄存器，也是该线程启动时创建的）。\n在运行过程中，每当调用进入一个 Java 方法，Java 虚拟机会在当前线程的 Java 方法栈中生成一个栈帧（并入栈），用于存放局部变量以及字节码的操作数。这个栈帧的大小是提前计算好的（编译为字节码的过程中计算的），而且 Java 虚拟机不要求栈帧在内存空间里连续分布。当退出当前执行的方法是，不管是正常返回还是异常返回，Java 虚拟机均会将该方法对应的栈帧弹出（出栈并舍弃）。\n2.2. 硬件视角 Java 字节码无法在硬件上直接执行，因此，Java 虚拟机需要将字节码翻译成机器码。在 HotSpot 里面，上述翻译过程有两种形式：第一种是解释执行，即逐条将字节码翻译成机器码执行；第二种是即时编译（JIT），即将一个方法中包含的所有字节编译成机器码后再执行。（编译执行与编译执行的区别可类比为同声传译与放录音，同声传译每一次都需要完整的执行，但是不需要过多等待；而放录音则是一劳永逸，但是存在第一次的录音的过程）\n解释执行的优势在于无需等待编译，而即时编译的优势在于实际运行速度更快。HotSpot 默认采用混合模式，综合了解释执行和即时编译两者的优点。它会先解释执行字节码，而后将其中反复执行的热点代码，以方法为单位进行即时编译。\n3. 作业解析 \u0026ndash;\u0026gt; 待办\n4. 总结 Java 需要运行时环境支持的原因主要有：\n语法复杂、高度抽象 实现跨平台 提供托管环境 Java 虚拟机将运行时内存区域划分为五个部分，分别为方法区、堆、PC 寄存器、Java 方法栈和本地方法栈。Java 程序编译而成的 class 文件，需要先加载至方法区中，方能在 Java 虚拟机中运行。\n5. 参考 Java核心技术面试精讲 深入拆解Java虚拟机 6. 说明 我不是在卖课 本文大部分内容来源于极客时间以及网络博文节选，如有冒犯，我先向您道歉，另还请告知我进行处理，谢谢 邮箱：thread_zhou@126.com ","permalink":"https://fuyi-atlas.github.io/posts/program/java/jvm/002/","summary":"1. 为什么 Java 需要运行时环境 1.1. Java 程序的启动方式 IDE中启动，比如：Eclipse、IntelliJ IDEA 构建为 jar，通过命令行的方式启动，比如：java -jar application.jar 使用构建工具（如：Gradle、Maven）启动，比如 SpringBoot 应用启动：gradle bootRun、mvn spring-boot:run 1.2. JRE 是什么 在这里引用极客时间课程Java核心技术面试精讲中的一段话\n我们日常会接触到 JRE（Java Runtime Environment） 或者 JDK（Java Development Kit）。JRE，也就是 Java 运行时环境，仅包含运行 Java 程序的必须组件，包括 Java 虚拟机以及 Java 核心类库等。而 JDK 可以看作是 JRE 的一个超集，提供了更多工具，比如编译器、各种诊断工具等。\n1.3. 为什么需要运行时环境 不是有这么一句话么，计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决。话说回来，Java 代码运行之所以需要运行时环境，主要是由于以下几个方面的原因：\nJava 语言语法非常复杂，抽象程度高，直接在硬件上运行这种复杂的程序并不现实（并不是不可以，但是这样造成与相应硬件的强耦合，且不便于抽象和复杂语法的实现。），所以需要在运行之前进行一番转换。\n实现平台无关性、做到 “Write once, run anywhere”，获得跨平台的能力。这样便需要一个中间层进行解耦，达到上层统一编码、下层跨越平台、中间实现兼容（所以，Java 语言的跨平台特性是由 Java 运行时环境实现的。也就是在不同平台皆有与之相对应的 Java 运行时环境，实现相同定义、不同实现。这样的思想是不是很熟悉，当然，这仅是我的理解）。\n提供托管环境（Managed Runtime），该托管环境可以代替我们处理一些通用的、容易出错的、高难度的行为，比如自动内存管理、垃圾回收、安全权限动态检测等。\n2. Java 代码在虚拟机中是怎样运行的 2.1. 虚拟机视角 执行 Java 代码首先需要将它编译而成的 class 文件加载到 Java 虚拟机中，加载后的 Java 类会被存放到方法区中。实际运行时，虚拟机会执行方法区内的代码（需要将字节码翻译为机器码，在 HotSpot 实现中，有解释执行和即时编译两种）。","title":"深入拆解Java虚拟机（一）"},{"content":"前言 标准先行并不是一个新鲜玩意，不过是先制定事实标准，而后使用这些标准约束之后的行为。俗话说得好：“没有规矩，不成方圆”。在这里，我想说一下我认为的标准先行以及与持续交付的结合，希望不会对你产生误导。\n为什么需要标准先行 我们可以先不去问为什么，而是反过来看这个问题。假设在我们的产物交付过程中，没有任何的标准，给你自由，会发生些什么问题。在一阵头脑风暴（抽搐）之后，我做出如下假设：\n乱象百出，群魔乱舞。不论是正确的还是错误的产出，解释如此。比如，整体代码的可读性和可维护性直线下降；分支来回穿插，Commit 信息奇奇怪怪；开发、测试环境存在混用，测试数据过于随意；部署由心，同一个应用在不用的主机上存放位置可能还不同等等。\n过程、阶段性产物不统一，且质量高低不齐，严重影响系统集成与交付。\n高效完成，但前提是有着意见一致、理念相同、水平相当的一群人。\n在我的假想中，自由带来的有好也有坏。但是我仍认为大部分的时候出现的都是坏的影响，毕竟要寻找到一群志同道合的人一起共事，就已经是一件很不容易的事了，且不说每个人都是单独的个体，其思想与灵魂亦是独一无二的。所以，不管在什么样的情况下，我们都需要制定标准来约束我们的行为。统一标准下的行为造成的影响几乎是一致的，那么即使是排错、修改时也是有迹可循。（在这里不会抬杠标准与自由的取舍，存在即是合理。但是从两者在大部分时候所能产生的影响来看，我会采取强标准，若自由的方式，但这绝非将两者置为对立关系。）\n都有哪些标准先行 没有绝对通用的标准，任何的标准都是不同场景下最佳实践的总结，且需要持续的进行优化，是可以传承的宝贵知识集合。之所以扯这么一段话，我只是想说目前还处于实践阶段，但是这并不影响制定相关的标准。使用过 SpringBoot 开发的大佬都知道什么是约定优于配置，我认为标准亦是如此，即标准之上是约定。在这里，我将需要先行的标准分为了如下几个方面：\n开发模式与分支策略（含 Commit 规范） 编码规范（Java、JavaScript） API 设计规范 数据库设计规范 环境隔离 部署规范 发布流水线标准 标准先行与持续交付 总结 参考文献 持续交付36讲 说明 我不是在卖课 本文大部分内容来源于极客时间以及网络博文节选，如有冒犯，我先向您道歉，另还请告知我进行处理，谢谢 邮箱：thread_zhou@126.com ","permalink":"https://fuyi-atlas.github.io/posts/program/continuous-delivery/002/","summary":"前言 标准先行并不是一个新鲜玩意，不过是先制定事实标准，而后使用这些标准约束之后的行为。俗话说得好：“没有规矩，不成方圆”。在这里，我想说一下我认为的标准先行以及与持续交付的结合，希望不会对你产生误导。\n为什么需要标准先行 我们可以先不去问为什么，而是反过来看这个问题。假设在我们的产物交付过程中，没有任何的标准，给你自由，会发生些什么问题。在一阵头脑风暴（抽搐）之后，我做出如下假设：\n乱象百出，群魔乱舞。不论是正确的还是错误的产出，解释如此。比如，整体代码的可读性和可维护性直线下降；分支来回穿插，Commit 信息奇奇怪怪；开发、测试环境存在混用，测试数据过于随意；部署由心，同一个应用在不用的主机上存放位置可能还不同等等。\n过程、阶段性产物不统一，且质量高低不齐，严重影响系统集成与交付。\n高效完成，但前提是有着意见一致、理念相同、水平相当的一群人。\n在我的假想中，自由带来的有好也有坏。但是我仍认为大部分的时候出现的都是坏的影响，毕竟要寻找到一群志同道合的人一起共事，就已经是一件很不容易的事了，且不说每个人都是单独的个体，其思想与灵魂亦是独一无二的。所以，不管在什么样的情况下，我们都需要制定标准来约束我们的行为。统一标准下的行为造成的影响几乎是一致的，那么即使是排错、修改时也是有迹可循。（在这里不会抬杠标准与自由的取舍，存在即是合理。但是从两者在大部分时候所能产生的影响来看，我会采取强标准，若自由的方式，但这绝非将两者置为对立关系。）\n都有哪些标准先行 没有绝对通用的标准，任何的标准都是不同场景下最佳实践的总结，且需要持续的进行优化，是可以传承的宝贵知识集合。之所以扯这么一段话，我只是想说目前还处于实践阶段，但是这并不影响制定相关的标准。使用过 SpringBoot 开发的大佬都知道什么是约定优于配置，我认为标准亦是如此，即标准之上是约定。在这里，我将需要先行的标准分为了如下几个方面：\n开发模式与分支策略（含 Commit 规范） 编码规范（Java、JavaScript） API 设计规范 数据库设计规范 环境隔离 部署规范 发布流水线标准 标准先行与持续交付 总结 参考文献 持续交付36讲 说明 我不是在卖课 本文大部分内容来源于极客时间以及网络博文节选，如有冒犯，我先向您道歉，另还请告知我进行处理，谢谢 邮箱：thread_zhou@126.com ","title":"持续交付之标准现行"},{"content":"前言 老生长谈，Java是一个什么事物，都有些什么样的特性呢？\nJava：Java 是一种广泛使用的计算机编程语言，拥有跨平台、面向对象、泛型编程的特性，广泛应用于企业级Web应用开发和移动应用开发。\nJava 编程语言的风格十分接近C++语言。继承了C++语言面向对象技术的核心，舍弃了容易引起错误的指针，以引用取代；移除了C++中的运算符重载和多重继承特性，用接口取代；增加垃圾回收器功能。在Java SE 1.5版本中引入了泛型编程、类型安全的枚举、不定长参数和自动装/拆箱特性。Sun微系统对 Java语言 的解释是：“Java 编程语言是个简单、面向对象、分布式、解释性、健壮、安全与系统无关、可移植、高性能、多线程和动态的语言”。\nJava 不同于一般的编译语言或解释型语言。它首先将源代码编译成字节码，再依赖各种不同平台上的虚拟机来解释执行字节码，从而具有“一次编写，到处运行”的跨平台特性。在早期 JVM 中，这在一定程度上降低了 Java 程序的运行效率。但在J2SE1.4.2发布后，Java 的运行速度有了大幅提升。\n以上内容来自于维基百科，给定了 Java 一个明确的定义，但是对于Java平台以及特性并没有很好的归结，下面是我截取自极客时间Java核心技术面试精讲的部分内容：\nJava 本身是一种面向对象的语言，最显著的特性有两个方面，一是所谓的“书写一次，到处运行”（Write once, run anywhere），能够非常容易地获得跨平台能力；另外就是垃圾收集（GC, Garbage Collection），Java 通过垃圾收集器（Garbage Collector）回收分配内存，大部分情况下，程序员不需要自己操心内存的分配和回收。\n我们日常会接触到 JRE（Java Runtime Environment）或者 JDK（Java Development Kit）。 JRE，也就是 Java 运行环境，包含了 JVM 和 Java 类库，以及一些模块等。而 JDK 可以看作是 JRE 的一个超集，提供了更多工具，比如编译器、各种诊断工具等。\n对于“Java 是解释执行”这句话，这个说法不太准确。我们开发的 Java 的源代码，首先通过 Javac 编译成为字节码（bytecode），然后，在运行时，通过 Java 虚拟机（JVM）内嵌的解释器将字节码转换成为最终的机器码。但是常见的 JVM，比如我们大多数情况使用的 Oracle JDK 提供的 Hotspot JVM，都提供了 JIT（Just-In-Time）编译器，也就是通常所说的动态编译器，JIT 能够在运行时将热点代码编译成机器码，这种情况下部分热点代码就属于编译执行，而不是解释执行了。\n在维基百科给出的定义中，提到 Java 编程语言是解释性的语言，而在Java核心技术面试精讲中提出该说法不太准确，我比较认可后者的看法。类似 JIT、AOT（Ahead-of-Time Compilation，在执行前直接将字节码编译成机器代码，静态编译） 技术的出现以及实践，使得 Java 的解释性变得不那么纯粹，此时从不同的角度可能会得到不同的结果，比如从占比上考虑，那么 Java 还是解释性的；如果从 JIT 的角度看，此刻的 Java 是编译性的；而从总体的角度来看，则是混合性的（解释和编译的混合）。\n为什么要学习JVM 不仅要知其然，还要知其所以然。Java 语言对底层平台良好的封装性，让我们编码更加的简单，如不再需要自己管理内存。但是强大的封装同样会将其透明化，大部分时候不需要知道这透明墙的背后是什么，而当涉及透明墙后面的知识时，便会不知所措。当然，学习 JVM 不仅仅是获得遇到问题时解决问题的能力，还是更好的理解 Java 程序设计以及提升编码能力的不二法门（有的设计真的可以说是优美，且富有哲理）。\nJava字节码技术 什么是字节码 字节码（Java bytecode）：由单字节（byte）的指令组成，理论上最多支持256个操作码（opcode）。实际上 Java 只使用了200左右的操作码，还有一些操作码则保留给调试操作。\n按照指令的性质，主要分为4个大类（其中2、3、4皆和 Java 语言相关）：\n栈操作指令，包括与局部变量交互的指令 程序流程控制指令 对象操作指令，包括方法调用指令 算术运算以及类型转换指令 关于助记符，其实就是字节码操作码（二进制存储，目视解译难度也太高了）的可读性编码，主要就是提供开发人员对于字节码操作码的可读性，每个助记符都对应一个字节码操作码。\n如何生成字节码 我们平时编写的基本上都是 Java 源码，也就是后缀为 .java 类型的文件，通过 JDK 中的编译工具 javac 就可以将源码编译为字节码（后缀为 .class）。如果在生成字节码之前，我们需要有一份符合 Java 语法规范的源码文件。\n源码准备 这里有一个简单的类，源码如下：\npackage demo.jvm0104; public class HelloByteCode { public static void main(String[] args) { HelloByteCode helloByteCode = new HelloByteCode(); } } 生成字节码 使用 javac命令进行编译，使用 javap 命令可查看字节码 javac HelloByteCode.java # 在HelloByte.java的同级目录下执行 javap -c HelloByteCode.class # 在HelloByte.class的同级目录下执行 Compiled from \u0026#34;HelloByteCode.java\u0026#34; public class demo.jvm0104.HelloByteCode { public demo.jvm0104.HelloByteCode(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V 4: return public static void main(java.lang.String[]); Code: 0: new #2 // class demo/jvm0104/HelloByteCode 3: dup 4: invokespecial #3 // Method \u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V 7: astore_1 8: return } 如上所视，这是一个很简单的类编译后生成的字节码，这里看到的字节码本不完整，可以使用 javap -c -verbose HelloByteCode.class 查看更加详细的字节码内容。这里可以先简单的看一下字节码，此处显示的为字节码对应的助记符\ndup: 用于复制栈顶的值。由于构造函数调用不会返回值，所以如果没有 dup 指令，在对象上调用方法并初始化之后，操作数栈就会是空的，在初始化之后就会出问题，接下来的代码就无法对其进行处理\naload_0：a表示引用（对象引用），类似于 aload_0 和 astore_0 为一对操作，栈操作，用于入栈与出栈，栈操作可用如下示意图表示： 在Java程序中，计算的行为都是发生在栈上面的，Load 和 Store 分别是在计算前从本地变量表中将变量加载至栈中，计算完成后将所得的变量值写回本地变量表。\n字节码的运行时结构（栈帧结构） 栈帧结构示意图\nJVM 是一台基于栈的计算机其，能够像计算机一样解读机器码（此处机器码类比为字节码），所以又将其称之为虚拟机。\n在 Java 程序中，每个线程都有一个独属于自己的线程栈（JVM Stack），用于存储栈帧（Frame）。每一次的方法调用，JVM 都会自动创建一个栈帧并将其入栈。栈帧由操作数栈（Operand Stack）、本地变量表（Local Variable）以及一个 Class 引用组成。此处的 Class 引用指向当前方法在运行时常量池中对应的 Class。\n字节码示例解读 在这里准备了一个涉及简单四则运算的程序\npackage demo.jvm0104; public class MovingAverage { private int count = 0; private double sum = 0.0D; public void submit(double value) { this.count ++; this.sum += value; } public double getAvg() { if (0 == this.count) { return sum; } return this.sum / this.count; } } package demo.jvm0104; public class LocalVariableTest { public static void main(String[] args) { MovingAverage movingAverage = new MovingAverage(); int num1 = 1; int num2 = 2; movingAverage.submit(num1); movingAverage.submit(num2); double avg = movingAverage.getAvg(); } } 编译成字节码\n使用 javac命令进行编译，使用 javap 命令可查看字节码 javac demo/jvm0104/LocalVariableTest.java # 在包结构最外层的同级目录（demo）下执行 javap -c -verbose demo/jvm0104/LocalVariableTest.class # 在包结构最外层的同级目录（demo）下执行 Classfile /home/zhoujian/Downloads/Git/java-000-repository/src/demo/jvm0104/LocalVariableTest.class Last modified Nov 10, 2020; size 434 bytes MD5 checksum ad14266d95e706aca83dea47e32fe139 Compiled from \u0026#34;LocalVariableTest.java\u0026#34; public class demo.jvm0104.LocalVariableTest minor version: 0 major version: 55 flags: (0x0021) ACC_PUBLIC, ACC_SUPER this_class: #6 // demo/jvm0104/LocalVariableTest super_class: #7 // java/lang/Object interfaces: 0, fields: 0, methods: 2, attributes: 1 Constant pool: #1 = Methodref #7.#16 // java/lang/Object.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #2 = Class #17 // demo/jvm0104/MovingAverage #3 = Methodref #2.#16 // demo/jvm0104/MovingAverage.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #4 = Methodref #2.#18 // demo/jvm0104/MovingAverage.submit:(D)V #5 = Methodref #2.#19 // demo/jvm0104/MovingAverage.getAvg:()D #6 = Class #20 // demo/jvm0104/LocalVariableTest #7 = Class #21 // java/lang/Object #8 = Utf8 \u0026lt;init\u0026gt; #9 = Utf8 ()V #10 = Utf8 Code #11 = Utf8 LineNumberTable #12 = Utf8 main #13 = Utf8 ([Ljava/lang/String;)V #14 = Utf8 SourceFile #15 = Utf8 LocalVariableTest.java #16 = NameAndType #8:#9 // \u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #17 = Utf8 demo/jvm0104/MovingAverage #18 = NameAndType #22:#23 // submit:(D)V #19 = NameAndType #24:#25 // getAvg:()D #20 = Utf8 demo/jvm0104/LocalVariableTest #21 = Utf8 java/lang/Object #22 = Utf8 submit #23 = Utf8 (D)V #24 = Utf8 getAvg #25 = Utf8 ()D { public demo.jvm0104.LocalVariableTest(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V 4: return LineNumberTable: line 3: 0 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=6, args_size=1 0: new #2 // class demo/jvm0104/MovingAverage 3: dup 4: invokespecial #3 // Method demo/jvm0104/MovingAverage.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V 7: astore_1 8: iconst_1 9: istore_2 10: iconst_2 11: istore_3 12: aload_1 13: iload_2 14: i2d 15: invokevirtual #4 // Method demo/jvm0104/MovingAverage.submit:(D)V 18: aload_1 19: iload_3 20: i2d 21: invokevirtual #4 // Method demo/jvm0104/MovingAverage.submit:(D)V 24: aload_1 25: invokevirtual #5 // Method demo/jvm0104/MovingAverage.getAvg:()D 28: dstore 4 30: return LineNumberTable: line 6: 0 line 7: 8 line 8: 10 line 9: 12 line 10: 18 line 11: 24 line 12: 30 } SourceFile: \u0026#34;LocalVariableTest.java\u0026#34; 字节码含义说明：\niconst_1: （如：int num = 1;）\ni：int类型值 const：常量值 1：表示该常量的实际数值 iload_2：\ni：int类型值 load：栈操作，将数据从本地变量表中加载至操作数栈 2：对应本地变量表中的 slot 值，即槽位 dstore 4：\nd：double类型值 store：栈操作，将数据从操作数栈写回本地变量表 4：对应本地变量表中的 slot 值 iload_2 和 dstore 4 都是栈操作字节码，只不过后者是在槽位大于3之后，以参数的方式跟在字节码指令后面，所以 iload_2 是连在一起写的，dstore 4 是分开写的。\ni2d： i：int类型值 2：表示类型转换（to），i2d 表示 int 类型值转换为 double 类型值 d：double类型值 字节码执行流程 \u0026ndash;\u0026gt; 待补充\n在算术操作与类型转换中，只存在4中数据类型，如下：\nint long float double 而在 Java 语言中，存在8大基本数据类型：byte（8bit）、short（16bit）、char（16bit）、boolean（8bit）、int（32bit）、long（64bit）、float（32bit）、double（64bit）。但是在字节码处理中，像 boolean、byte 都是使用 int 进行表示。也就是说，在字节码处理中，int 是最小单位，小于 32bit 的数值皆使用 int 表示。（所以这里也存在存储占用的情况，比如 byte 只需要1字节，却使用了4字节进行存储）\n关于算术运算和类型转换操作，在将源码编译成字节码的过程中就已经完成了将相应的操作转换为对应的字节码指令，这样在具体执行的时候已经无关数据类型是什么了。（就不再需要每一次执行都计算前后类型是什么了，执行效率更高）\n关于 int（32bit）和 long（64bit）类型的操作是否为原子操作问题，从理论上来看，这和 CPU 的总线带宽是存在直接关系的。32位的 CPU 其总线带宽就是 32bit，对于 long 类型的数据需要分为两次读取，理论上是存在原子问题的。而在64位的 CPU 上则不存在这样的问题。（即为原子操作）\n在 JVM 中，一共存在5种数据类型，分别是 int、long、float、；double 以及对象引用（4种数据类型+对象引用），其中对象引用是无法参与算术计算，也不能同另外的4种数据类型进行相互转换。\n这里准备了一个完整的循环控制的程序\npackage demo.jvm0104; public class ForLoopTest { private static int[] NUMBERS = {1, 6, 8}; public static void main(String[] args) { MovingAverage movingAverage = new MovingAverage(); for (int number : NUMBERS) { movingAverage.submit(number); } double avg = movingAverage.getAvg(); } } 生成字节码\n使用 javac命令进行编译，使用 javap 命令可查看字节码 javac demo/jvm0104/ForLoopTest.java # 在包结构最外层的同级目录（demo）下执行 javap -c -verbose demo/jvm0104/ForLoopTest.class # 在包结构最外层的同级目录（demo）下执行 Classfile /home/zhoujian/Downloads/Git/java-000-repository/src/demo/jvm0104/ForLoopTest.class Last modified Nov 10, 2020; size 612 bytes MD5 checksum 2613b808dee182beb04e7130316d06c4 Compiled from \u0026#34;ForLoopTest.java\u0026#34; public class demo.jvm0104.ForLoopTest minor version: 0 major version: 55 flags: (0x0021) ACC_PUBLIC, ACC_SUPER this_class: #7 // demo/jvm0104/ForLoopTest super_class: #8 // java/lang/Object interfaces: 0, fields: 1, methods: 3, attributes: 1 Constant pool: #1 = Methodref #8.#23 // java/lang/Object.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #2 = Class #24 // demo/jvm0104/MovingAverage #3 = Methodref #2.#23 // demo/jvm0104/MovingAverage.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #4 = Fieldref #7.#25 // demo/jvm0104/ForLoopTest.NUMBERS:[I #5 = Methodref #2.#26 // demo/jvm0104/MovingAverage.submit:(D)V #6 = Methodref #2.#27 // demo/jvm0104/MovingAverage.getAvg:()D #7 = Class #28 // demo/jvm0104/ForLoopTest #8 = Class #29 // java/lang/Object #9 = Utf8 NUMBERS #10 = Utf8 [I #11 = Utf8 \u0026lt;init\u0026gt; #12 = Utf8 ()V #13 = Utf8 Code #14 = Utf8 LineNumberTable #15 = Utf8 main #16 = Utf8 ([Ljava/lang/String;)V #17 = Utf8 StackMapTable #18 = Class #30 // \u0026#34;[Ljava/lang/String;\u0026#34; #19 = Class #10 // \u0026#34;[I\u0026#34; #20 = Utf8 \u0026lt;clinit\u0026gt; #21 = Utf8 SourceFile #22 = Utf8 ForLoopTest.java #23 = NameAndType #11:#12 // \u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #24 = Utf8 demo/jvm0104/MovingAverage #25 = NameAndType #9:#10 // NUMBERS:[I #26 = NameAndType #31:#32 // submit:(D)V #27 = NameAndType #33:#34 // getAvg:()D #28 = Utf8 demo/jvm0104/ForLoopTest #29 = Utf8 java/lang/Object #30 = Utf8 [Ljava/lang/String; #31 = Utf8 submit #32 = Utf8 (D)V #33 = Utf8 getAvg #34 = Utf8 ()D { public demo.jvm0104.ForLoopTest(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V 4: return LineNumberTable: line 3: 0 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=6, args_size=1 0: new #2 // class demo/jvm0104/MovingAverage 3: dup 4: invokespecial #3 // Method demo/jvm0104/MovingAverage.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V 7: astore_1 8: getstatic #4 // Field NUMBERS:[I 11: astore_2 12: aload_2 13: arraylength 14: istore_3 15: iconst_0 16: istore 4 18: iload 4 20: iload_3 21: if_icmpge 43 24: aload_2 25: iload 4 27: iaload 28: istore 5 30: aload_1 31: iload 5 33: i2d 34: invokevirtual #5 // Method demo/jvm0104/MovingAverage.submit:(D)V 37: iinc 4, 1 40: goto 18 43: aload_1 44: invokevirtual #6 // Method demo/jvm0104/MovingAverage.getAvg:()D 47: dstore_2 48: return LineNumberTable: line 8: 0 line 9: 8 line 10: 30 line 9: 37 line 12: 43 line 13: 48 StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 18 locals = [ class \u0026#34;[Ljava/lang/String;\u0026#34;, class demo/jvm0104/MovingAverage, class \u0026#34;[I\u0026#34;, int, int ] stack = [] frame_type = 248 /* chop */ offset_delta = 24 static {}; descriptor: ()V flags: (0x0008) ACC_STATIC Code: stack=4, locals=0, args_size=0 0: iconst_3 1: newarray int 3: dup 4: iconst_0 5: iconst_1 6: iastore 7: dup 8: iconst_1 9: bipush 6 11: iastore 12: dup 13: iconst_2 14: bipush 8 16: iastore 17: putstatic #4 // Field NUMBERS:[I 20: return LineNumberTable: line 5: 0 } SourceFile: \u0026#34;ForLoopTest.java\u0026#34; 字节码含义说明（程序流程控制指令）：\n21: if_icmpge 43：\n21：指令索引，表示当前指令的索引从21开始 if：if，判断语句 cmp：比较 ge：great，比较大小，一个值是否大于或等于另外一个值 43：指令索引，表示如果满足 if 条件，则跳转到索引为43的指令继续执行 37: iinc 4, 1：\n37：指令索引，表示当前指令的索引从37开始 inc：自增，参数 4, 1中的1表示自增步长为1，即每次自增1 i：int类型值 40: goto 18\n40：指令索引，表示当前指令的索引从40开始 goto：跳转指令 18：指令索引，goto 18 表示跳转到索引为18的指令继续执行 字节码执行流程 \u0026ndash;\u0026gt; 待补充\n补充：方法调用指令\ninvokestatic：用于调用某个类的静态方法，这是方法调用指令中最快的一个 invokespecial：用于调用构造函数，但也可以用于调用同一个类中的 private 方法，以及可见的超类方法。（常用） invokevirtual：如果是具体类型的目标对象，invokevirtual 用于调用公共、受保护和 package 级别的私有方法。（常用，调用实例方法，基于类进行分派） invokeinterface：当通过接口引用来调用方法时，将会编译为 invokeinterface指令。 invokedynamic：jdk7 新增加的指令，是实现“动态类型语言”（Dynamically Typed Language）支持而进行的升级改造，同时也是 jdk8以后支持 Lambda 表达式的实现基础。 关于 Java 源码、字节码、JVM 之间的关系的看法（仅做参考）：\n源码：是具体业务和逻辑的体现，是抽象的\n字节码：是经由源码编译后的，确定的执行指令集合\nJVM：执行平台，字节码的执行器，字节码的出现使得 JVM 较为无脑（少计算、直接反应、直接执行，比如类型转换），当然在 JVM 中也是存在计算的，比如热点探测、实时编译等\nJVM类加载器 类的生命周期（借鉴自极客时间Java进阶训练营） 一个类在 JVM 里的生命周期有7个阶段，分别是加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）、卸载（Unloading）。其中的前五个部分（加载、验证、准备、解析、初始化）统称为类加载，而验证、准备、解析三个部分统称为链接（Linking），也就是说类加载完整的包含了链接过程。下面我们就分别来说一下类加载的五个过程。\n加载 加载阶段也可以称为装载阶段。这个阶段主要的操作就是 \u0026ndash;\u0026gt; 根据明确知道的 class 完全限定名，来获取二进制 classfile 格式的字节流（即根据地址找到 class 文件，可以是本地文件，也可以是网络文件）。如果找不到二进制表示形式，则会抛出 NoClassDefFound 错误。\n在装载阶段并不会检查 classfile 的语法和格式。类加载的整个过程主要由 JVM 和 Java 的类加载系统共同完成，当然具体到 loading 阶段则是由 JVM 与某一个类加载器（java.lang.classLoader）协作完成。\n校验（验证） 链接过程的第一个阶段是校验，确保 class 文件里的字节流信息符合当前虚拟机的要求，不会危害虚拟机的安全。 校验过程检查 classfile 的语义，判断常量池中的符号，并执行类型检查，主要目的是判断字节码的合法性，比如 magic number，对版本号进行验证。这些检查过程中可能会抛出 VerifyError，ClassFormatError 或 UnsupportedClassVersionError。\n因为 classfile 的验证是链接阶段的一部分，所以这个过程中可能需要加载其他类，在某个类的加载过程中，JVM 必须加载其所有的超类和接口。如果类层次结构有问题（例如，该类是自己的超类或接口，死循环了），则 JVM 将抛出 ClassCircularityError。而如果实现的接口并不是一个 interface，或者声明的超类是一个 interface，也会抛出 IncompatibleClassChangeError。\n准备 由上而下，现在进入准备阶段，这个阶段将会创建静态字段，并将其初始化为标准默认值（比如 null 或者 0），并分配方法表，即在方法区中分配这些变量所使用的内存空间。请注意，准备阶段并未执行任何 Java 代码。\n例如： public static int i = 1;\n在准备阶段 i 的值会被初始化为 0，在后面的类初始化阶段才会执行赋值为 1。\n但是如果像下面这样使用 final 修饰静态常量，某些 JVM 的行为就不一样了：\npublic static final int i = 1;\n对于常量 i，在准备阶段就会被赋值为 1，其实这样还是比较 pizzle，例如其他语言（C#）有直接的关键字 const，让告诉编 译器在编译阶段就替换成常量，类似宏指令，更简单。\n解析 然后进入可选的解析符号引用阶段。也就是解析常量池，主要有以下四种：\n类或接口的解析 字段解析 类方法解析 接口方法解析 简单来说就是我们编写的代码中，当一个变量引用某个对象的时候，这个引用在 .class 文件中是以符号引用的方式来存储的（相当于做了一个索引记录）。在解析阶段就需要将其解析并链接为直接引用（相当于指向实际对象）。如果有了直接引用，那引用的目标必定在堆中存在。\n加载一个 class 时，需要加载其所有的 super 类和 super 接口\n初始化 JVM 规范明确规定，必须在类的首次“主动使用”时才能执行类的初始化。被初始化的过程包括执行：\n类构造器方法 static 静态变量赋值语句 static 静态代码块 如果一个子类进行初始化会先对其父类进行初始化，以保证父类在子类之前进行初始化。所以必然先初始化过 java.lang.Object 类，因为所有的 Java 类都继承自 java.lang.Object。\n只要我们尊重语言的语义，在执行下一步操作之前完成装载，那么链接和初始化这些步骤，如果出错就按照规定抛出相应的错误，类加载系统完成可以根据自己的策略，灵活地进行符号解析等链接过程。\n为了提高性能，HotSpot JVM 通常要等到类初始化时才会去装载和链接类。因此如果 A 类引用了 B 类，那么加载 A 类并不一定会去加载 B 类（除非需要进行验证）。主动对 B 类执行第一条指令时才会导致 B 类的初始化，这就需要先完成对 B 类的装载和链接。\n示例说明（类加载顺序） 类的加载时机 类加载器实现 三类加载器 继承关系 自定义类加载器 JVM内存模型 JVM内存模型图 程序计数器（Program Counter Register） 虚拟机栈（VM Stack） 本地方法栈（Native Stack） 堆（Heap） 非堆（Non-Heap） 直接内存 JVM启动参数 基本介绍与分类 系统属性参数 运行模式参数 堆内存设置参数 GC设置参数 分析诊断参数 JavaAgent参数 参考 深入理解Java虚拟机（第二版）\nJava进阶训练营（极客时间）\n深入拆解Java虚拟机\nJava核心技术面试精讲\n说明 我不是在卖课 本文大部分内容来源于极客时间以及网络博文节选，如有冒犯，我先向您道歉，另还请告知我进行处理，谢谢 邮箱：thread_zhou@126.com ","permalink":"https://fuyi-atlas.github.io/posts/program/java/jvm/001/","summary":"前言 老生长谈，Java是一个什么事物，都有些什么样的特性呢？\nJava：Java 是一种广泛使用的计算机编程语言，拥有跨平台、面向对象、泛型编程的特性，广泛应用于企业级Web应用开发和移动应用开发。\nJava 编程语言的风格十分接近C++语言。继承了C++语言面向对象技术的核心，舍弃了容易引起错误的指针，以引用取代；移除了C++中的运算符重载和多重继承特性，用接口取代；增加垃圾回收器功能。在Java SE 1.5版本中引入了泛型编程、类型安全的枚举、不定长参数和自动装/拆箱特性。Sun微系统对 Java语言 的解释是：“Java 编程语言是个简单、面向对象、分布式、解释性、健壮、安全与系统无关、可移植、高性能、多线程和动态的语言”。\nJava 不同于一般的编译语言或解释型语言。它首先将源代码编译成字节码，再依赖各种不同平台上的虚拟机来解释执行字节码，从而具有“一次编写，到处运行”的跨平台特性。在早期 JVM 中，这在一定程度上降低了 Java 程序的运行效率。但在J2SE1.4.2发布后，Java 的运行速度有了大幅提升。\n以上内容来自于维基百科，给定了 Java 一个明确的定义，但是对于Java平台以及特性并没有很好的归结，下面是我截取自极客时间Java核心技术面试精讲的部分内容：\nJava 本身是一种面向对象的语言，最显著的特性有两个方面，一是所谓的“书写一次，到处运行”（Write once, run anywhere），能够非常容易地获得跨平台能力；另外就是垃圾收集（GC, Garbage Collection），Java 通过垃圾收集器（Garbage Collector）回收分配内存，大部分情况下，程序员不需要自己操心内存的分配和回收。\n我们日常会接触到 JRE（Java Runtime Environment）或者 JDK（Java Development Kit）。 JRE，也就是 Java 运行环境，包含了 JVM 和 Java 类库，以及一些模块等。而 JDK 可以看作是 JRE 的一个超集，提供了更多工具，比如编译器、各种诊断工具等。\n对于“Java 是解释执行”这句话，这个说法不太准确。我们开发的 Java 的源代码，首先通过 Javac 编译成为字节码（bytecode），然后，在运行时，通过 Java 虚拟机（JVM）内嵌的解释器将字节码转换成为最终的机器码。但是常见的 JVM，比如我们大多数情况使用的 Oracle JDK 提供的 Hotspot JVM，都提供了 JIT（Just-In-Time）编译器，也就是通常所说的动态编译器，JIT 能够在运行时将热点代码编译成机器码，这种情况下部分热点代码就属于编译执行，而不是解释执行了。\n在维基百科给出的定义中，提到 Java 编程语言是解释性的语言，而在Java核心技术面试精讲中提出该说法不太准确，我比较认可后者的看法。类似 JIT、AOT（Ahead-of-Time Compilation，在执行前直接将字节码编译成机器代码，静态编译） 技术的出现以及实践，使得 Java 的解释性变得不那么纯粹，此时从不同的角度可能会得到不同的结果，比如从占比上考虑，那么 Java 还是解释性的；如果从 JIT 的角度看，此刻的 Java 是编译性的；而从总体的角度来看，则是混合性的（解释和编译的混合）。","title":"jvm 基础入门"},{"content":"前言 在我们日常的划水中，常常听到持续集成、持续部署、持续交付、DevOps，那么这些名词到底是什么意思？对我们的日常工作有什么作用呢？能够提高划水的效率呢？其实对于名词的解释始终还是千人千面，不同环境下必然存在不同的产物，我今天想说一说我自己的理解，希望不会对你有误导。\n什么是持续交付 持续交付：（英语：Continuous delivery，缩写为 CD），是一种软件工程手法，让软件产品的产出过程在一个短周期内完成，以保证软件可以稳定、持续的保持在随时可以释出的状况。它的目标在于让软件的建置、测试与释出变得更快以及更频繁。这种方式可以减少软件开发的成本与时间，减少风险。\n由上可知，持续交付的关注点在于以下几个方面：\n短周期 随时可释出 频繁构建 持续交付也可以拆开分析，所谓持续便是一直的频繁的做某件事，有一种开始也是结束，结束也是新的开始的感觉，需要闭环反馈来支持下一个阶段的持续行为；而交付则表示针对当前交付目标释出可行的产物。上面提到的随时可释出和频繁构建很好的体现了持续交付的行为特性，而短周期则是表示每一个阶段的交付过程应该在一个短的周期之内完成，因为短周期意味着快速交付、快速反馈、快速反应并快速的循环反复，而长周期必然会加大一次交付流程的耗时（加大试错成本，无法快速反应），所以这样才能够达到持续交付的目的（随时可交付）。至于什么才是合适的短周期，这个需要结合企业或团队的具体情况（比如项目当前所处阶段、研发人员素质、研发上线流程等）进行考虑。\n说了这么多，那么持续交付是以什么样的形式。如何落地到我们日常的工作中的呢？结合美团外卖持续交付分享（美团外卖持续交付的前世今生）和极客时间课程持续交付36讲，我总结了如下的一种描述。（希望不会将你带偏）\n关于持续交付，不同的企业、不同的团队站在不同的角度会存在不同的定义，我们可以把持续交付定义为一个产品价值的开发框架（站在企业的角度）、一套软件工程方法论以及许许多多最佳实践的集合。持续交付的落地便是开发框架或软件工程方法论与实际情况的结合的实践（也可以说是最佳实践的排列组合），更详细的实践情况还请接着往后看。\n持续集成、持续部署、DevOps和持续交付的关系 持续集成 持续集成：（英语：Continuous integration，缩写CI），是一种软件工程流程，是将所有软件工程师的软件工作副本持续集成到共享主线（mainline）的一种举措。该名称最早由葛来迪.布区（Grady Booch）在他的布区方法中提出，在测试开发驱动（TDD）的实践中，通常还会搭配自动单元测试。持续集成的提出主要是为了解决软件进行系统集成时面临的各种问题，极限编程称这些问题为集成地狱（integration hell）。\n持续集成的关注点在于：\n频繁集成 有效协作 自动化测试 我们通常会将软件研发工作拆解，拆分成不同模块或不同团队进行编码，编码完成后，进行集成构建和测试。这个从编码到构建再到测试的反复持续过程，就叫做持续集成。由持续集成的定义可联想到版本控制，如 Git，进而可以关联到分支策略（如：Git Flow、GitHub FLow、GitLab Flow），可以说分支策略是持续集成的前置条件。一般情况下，会有一个分支作为集成使用，加上多个特性分支（这里和具体的分支策略强相关，并不是所有团队的都是一样的），当特性分支开发完毕，通过 PR 请求合并到集成分支，此时会触发持续集成工作流程，保证待合并的分支内容是有效的，只有通过持续集成流程验证，才能合并到集成分支。\n持续部署 持续部署：（英语：Continuous deployment，缩写为CD），是一种软件工程方法，意指在软件开发流程中，以自动化、频繁而且持续性的，将软件部署到生产环境（production environment）中，使软件产品能够快速的发展。持续部署可以被整合到持续集成与持续交付的流程之中。\n持续部署的关注点在于：\n自动化的 频繁的 持续性 生产环境 在一些企业的软件部署工作中，仍然存在全人工操作的方式。如果只是单纯的安装部署目的还较为容易（不过繁琐的配置，人工的不规范、不确定性，部署的耗时等都是成本）；而如果是已上线的服务的部署工作（服务更新），此时的涉及面以及受影响的范围就比较大了。就算释出的产物已经可以达到可交付的标准，但是要使得用户真正可用，还需要跨越安全、快速以及稳定部署的障碍。那么是否可以将部署的场景和过程进行抽象，使用统一的、规范的、自动化的方法论和流程来约束和实现部署的过程。而持续部署便是这样的一套方法论及实践工具集，旨在规范部署行为，通过自动化提高部署效率，达到持续部署目的。\n持续集成、持续部署与持续交付的关系 当提到持续交付的时候，总能关联到持续集成与持续部署，我也一直傻傻分不清这三者之间有什么区别，是什么样的关系。不过从前面给出的定义和关注点可以知道，持续集成侧重于编码阶段内多人协作产物集成的有效性，持续部署侧重于将产物部署到生产环境，而持续交付则侧重于需要随时可释出；而相同点在于他们三者都推崇持续性，即存在一个反馈的过程，且反馈的结果作为下一阶段持续的支持。\n再说到持续集成、持续部署和持续交付之间的关系，这里还有一个比较有趣的地方，那就是在不同的视角下，三者的关系并不一样，这里借鉴美团外卖分享的内容（美团外卖持续交付的前世今生）举例，从研发和产品的角度来分析这三者的关系。\n研发视角：我们可以看到大部分研发团队，会从软件研发的角度进行定义，他们将软件的开发步骤拆解为持续集成、持续交付、持续部署，其中持续集成指开发人员从编码到构建的过程；持续交付则作为持续集成的自然延续，指将已经集成构建完成的代码，交给测试团队进行测试的过程；持续部署指将测试通过的软件交付给用户的过程。在研发的视角下，持续交付就是一个承上启下的过程，与持续集成形成了闭环，而又为将来达到持续部署做下了准备。此时持续集成 + 持续交付 + 持续部署便是一条完整的发布流水线。\n产品视角：产品团队会站在产品的角度来看，他们认为持续交付，是从需求的 PRD 文档提出来，到用户能够感受这个需求的周期。也就是说，此时持续交付是完整的包含了持续集成与持续部署，但是持续交付涵盖范围是大于持续集成 + 持续部署。并且，此时的持续交付流程本身就包含了一条完整的发布流水线。\n借用持续交付36讲中的 发布流水线 示意图\n说到这里已经不难看出，影响三者之间相互关系的因素主要在于对于持续交付的定位。在这里先说说我自己的看法，我认为持续交付的核心要义在于：短周期、时刻可释出、持续构建，即在于持续的产出与持续验证，由产出与验证形成闭环，进而相互推动，以达到快速反应，快速实现，持续优化。而短周期便是一次交付过程耗时的预定义，也是对于效率的要求，但一定不是对用人的压榨（去你xxx）。在持续交付中，交付对象不一定就是最终用户，所以千万不要认为一定要做到端到端完整才是持续交付。持续交付是一个周期性、可持续的行为，可以只是研发到测试的闭环，此时于研发而言交付对象是测试团队，交付物为通过持续集成验证的代码；也可以如产品视角一般，从需求的 PRD 提出来到用户能够感受到这个需求的周期，此时交付对象为用户，交付物为可用的产品。\n所以持续交付可以只是一套方法论，可以是产品价值开发框架，也可以是一部分流程实践。于我个人而言，我更认同持续交付是一套方法论（兼产品价值开发框架），由此指导持续交付体系的建设。如果你问我持续交付体系实践中使用到的技术是不是持续交付，我会说，在其中使用到的技术或工具只是当前持续交付体系建设的一个组成部分。所以，关于持续集成、持续部署、持续交付三者之间的关系，我认同为：三者相互渗透（可能这个词不是很恰当），并没有绝对的独立。\nDevOps DevOps：（是 Development 和 Operations 的组合词），是一种重视“软件开发人员（Dev）”和“IT 运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。通过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。\n传统的软件组织将开发、IT 运维和质量保障设为各自分离的部门，再这样的环境下如何采用的新的开发方法（例如敏捷软件开发），是一个重要的课题。按照从前的工作方式，开发和部署，不需要 IT 或者 QA 支持（跨部门的持之），而现在却需要极其紧密的多部门协作。而 DevOps 考虑的不仅仅是软件部署，它是一套针对这几个部门间沟通与协作问题的流程和方法。\n以上关于 DevOps 的定义摘取子维基百科，我们可以从中提取到如下几个关注点：\n是一种文化 是一种运动 沟通协作 自动化 持续性 频繁 低风险、可靠 通过上述定义和简单分析，我们对 DevOps 有了一个基本的认识，但还不够了解，不足够在我们的脑海里构建一个与之对应的场景，下面我会借鉴极客时间课程 DevOps 实战笔记的描述和我自己的理解来加深理解。\n提到 DevOps 是什么之前，我们可以先了解一下 DevOps 想要解决的问题究竟是个啥。由维基百科中给定的定义可知，DevOps 最初关注软件开发团队和IT运维团队之间的沟通协作问题。而谈到开发与运维的相爱相杀，又可以先说一下当前存在的开发模式：\n瀑布式开发模式：该模式将软件交付过程划分成几个阶段，从需求到开发、测试及运维。在软件开发的规模日益扩大时，通过分阶段、重流程、重规范、重管控来保证每一步都在计划之内，并以交付物作为每一个阶段的释出，只有达到交付标准才允许进入下一个阶段。这样的开发模式需要一开始就确定的知道项目的目标、范围以及实现方式，而现实中大部分时候处于了解的信息并不十分充足的阶段，如此情景下的决策总是缺少着完整数据的支持的。如果后续变化较多、较大，都会导致项目交付周期持续增长。\n敏捷式开发模式：瀑布式开发模式不适合需求不明确，时间周期较为紧张的场景，但是基本上很少有工程在一开始需求就是十分清晰且稳定的。基于这样的问题，敏捷的思路开始盛行。在我自一次听说敏捷开发的时候，我一直误认为使用了敏捷就能加速产出的速度，但其实这么说并不合适。敏捷的核心理念是，既然无法充分了解用户的真实需求，那么就拥抱变化吧，将一个大的目标拆解为一个个小的可交付的目标，然后不断迭代，以小步快跑的方式持续开发（分治思想，每一个小的迭代产生的变化是小的，带来的影响也是小的，可以平滑的进行生长）。与此同时，将测试工作注入到整个开发活动中，对开发交付的内容进行持验证，保证每次可交付的都是一个可用的功能集合，并且把质量把控内健在研发环节中，交付功能的质量也是有保障的。敏捷之所以敏捷，根本原因在于持续迭代和持续验证节省了大量不必要的浪费和返工。\nDevOps模式\n在瀑布模式中，开发、测试、发布是由三个各自独立的部门协作完成；到了敏捷式开发模式中，测试已经成功的被拉拢到了开发阶段中，剩下独自美丽的运维。但是，为了工作的效率，大家的快乐，只能想办法让所有人一块美丽。就是在这样的环境下，DevOps 应运而生。也就是说，DevOps 最开始想要做的，就是打破开发与运维之间的对立与隔阂。\n通过上面的分析和描述，我们可以知道，DevOps 最初的用意只是打破开发和运维之间的隔阂，建立友好、互助、共赢的价值观。但是在经过一段时间的实践之后发现，在整个软件交付过程中，不仅仅只有开发和运维，还会有如业务团队、安全团队等。也就是说，需要拉拢的其他对象出现了。此刻，我们相继拉拢了业务团队和安全团队，DevOps 涵盖的团队有开发、运维、业务和安全，而且业务团队认为可以使用 BizDevSecOps 来代替 DevOps 的称谓了，因为 BizDevSecOps 才是业务团队眼中的 DevOps。那么到底什么才是 DevOps呢？是否能够给出一个明确的定义？在这里先给出极客时间课程 DevOps 实战笔记作者石雪峰老师的见解：DevOps 是通过平台（Platform）、流程（Process）和人（People）的有机整合，以C（协作）A（精益）M（度量）S（共享）文化为指引，旨在建立一种可以快速交付价值并且具有持续改进能力的现代化IT组织。\n突然想起了 GIS 的组成部分，请自动过滤这句废话。老话说得好，不以结婚为目的的谈恋爱都是耍流氓。而 DevOps 也应该先谈目的，我认为，DevOps 的目的就是实现快速、稳定、可持续优化的价值交付，任何阻挡其实现的障碍都会通过一些手段（不择手段的手段）将其优化掉（比如：自动化代替绝大部分人工，繁琐或不正确流程重定义、协作共赢的价值理念宣传/洗脑 等）。所以，我更倾向于认为 DevOps 就是指引如何实现其目的的一类文化，而可以快速交付价值并且具有持续改进能力的现代化IT组织便是达到目的手段的整合，也是 DevOps 的落地。\n其实，DevOps 只是一个代号，甚至是不是叫做 DevOps 我认为并没有那么重要（也不太正确，如果名字没取好，可能就流行不起来了），比如对于安全团队来说，他们眼中的 DevOps 可能是 DevSecOps。但是我们知道，DevOps 代表这一种文化，当你心里想着如何才能更好地和前端（xx端）小哥哥小姐姐沟通以快速实现目标时，你就很“DevOps”了。\nDevOps与持续交付的关系 从前面所有的描述里面可以很明显的看出，其实 DevOps 和持续交付想要达到的最终目的是一致的（快速、稳定的交付产物），但是相较于持续交付来说，DevOps 涵盖的范围更加的广阔。由给出的定义分析，我认为 DevOps 和 持续交付是一种包含关系， DevOps 包含了持续交付。本质而言，他们都是同一类理念、文化（高效、精益、持续）的不同实践，只不过 DevOps 关注的范围更大，视角更高，所以从同理念不同实践的层面上分析，DevOps 包含了持续交付。\n持续交付有什么价值 都在讨论持续交付，那么持续交付能给我们带来什么样的价值呢？这个问题其实很好解答，或许不全面，但是能引起共鸣就好。\n我们完全可以从自己所在的环境进行分析，就比如我所在的环境中，我这里基本上都是 To G 的项目，暂时没有形成自己的核心产品，所以都是以项目的形式从无到有的推进。我们采用的开发模式是瀑布式开发模式，发布部署的方式仍然还是完全人工的方式。通过对一个完整项目交付结束后的分析，在交付过程中我们主要存在以下几个方面的问题：\n需求的不确定性，朝令夕改（夸张的修辞手法），发布时间延迟 瀑布开发模式下，测试介入比较晚，且不存在QA 完全人工部署，一次部署总是会出现环境或配置上的问题，需要通过维护一份友好的部署文档来提高效率 而持续交付能够带来的是什么，通过变更开发模式使得拥抱变化、通过持续集成可以保证特性功能集成的有效性、将测试工作提前到开发阶段保证功能的稳定性、通过持续部署保证部署的正确性、通过快速交付迭代保证快速的响应，快速变化，降低试错成本，同时时刻为交付做好准备。\n所以持续交付的价值在于：\n提高生产效率，支持快速响应 规范化、标准化发布流程 为什么需要引入持续交付 这个问题看上去和“持续交付有什么价值”有一定的重叠，确实如此，在上一个问题中，我列出了我所在环境遇到的问题，也简单提到了持续交付能够带来的价值。之所以需要引入持续交付，是因为需要变化来驱动我当前环境进行改变，以达到提高研发效能的目的。\n持续交付能带给我什么呢？\n执行编码标准、API设计标准等 制定统一的、规范的发布流程 持续集成保证每一次的集成中，代码是有效的 提前接入的测试，可以保障每一次的交付迭代的质量稳定 快速的迭代可以低成本的应对合理的需求变化 持续部署可以去掉大量的人工操作，保证了部署的正确性，且规范了部署的行为（比如目录的定义，产物存储位置） 所以我需要引入持续部署。（其实上面提到 DevOps 和持续交付是同理论不同实践的产物，为什么我在这里引入的是持续交付而不是 DevOps呢？主要是因为我目前只想快速的进行实践，通过持续交付的方法论，结合持续交付工具集快速的搭建符合我所在环境的持续交付流程。我当然不会告诉你我先买了持续交付课程，看完了才买的DevOps课程，再说了该文讲的是持续交付好吧。俗话说得好，不管黑猫白猫，抓到老鼠就是好猫。DevOps 等我！）\n总结 在这里，我结合多方资料对持续交付、继续继承、持续部署、DevOps 进行描述，并分析了他们之间的关系，希望不会给你带来误导（如果有你也别找我）。\n其实，不论是持续交付、DevOps 还是 AIOps等等，本质上都是通过一定的手段达到提高效能、精益求精的目的，所以没有最好的，只有最合适的。\n参考文献 持续交付 持续集成 持续部署 DevOps 美团外卖持续交付的前世今生 持续交付36讲 DevOps实战笔记 说明 我不是在卖课 本文大部分内容来源于极客时间以及网络博文节选，如有冒犯，我先向您道歉，另还请告知我进行处理，谢谢 邮箱：thread_zhou@126.com ","permalink":"https://fuyi-atlas.github.io/posts/program/continuous-delivery/001/","summary":"前言 在我们日常的划水中，常常听到持续集成、持续部署、持续交付、DevOps，那么这些名词到底是什么意思？对我们的日常工作有什么作用呢？能够提高划水的效率呢？其实对于名词的解释始终还是千人千面，不同环境下必然存在不同的产物，我今天想说一说我自己的理解，希望不会对你有误导。\n什么是持续交付 持续交付：（英语：Continuous delivery，缩写为 CD），是一种软件工程手法，让软件产品的产出过程在一个短周期内完成，以保证软件可以稳定、持续的保持在随时可以释出的状况。它的目标在于让软件的建置、测试与释出变得更快以及更频繁。这种方式可以减少软件开发的成本与时间，减少风险。\n由上可知，持续交付的关注点在于以下几个方面：\n短周期 随时可释出 频繁构建 持续交付也可以拆开分析，所谓持续便是一直的频繁的做某件事，有一种开始也是结束，结束也是新的开始的感觉，需要闭环反馈来支持下一个阶段的持续行为；而交付则表示针对当前交付目标释出可行的产物。上面提到的随时可释出和频繁构建很好的体现了持续交付的行为特性，而短周期则是表示每一个阶段的交付过程应该在一个短的周期之内完成，因为短周期意味着快速交付、快速反馈、快速反应并快速的循环反复，而长周期必然会加大一次交付流程的耗时（加大试错成本，无法快速反应），所以这样才能够达到持续交付的目的（随时可交付）。至于什么才是合适的短周期，这个需要结合企业或团队的具体情况（比如项目当前所处阶段、研发人员素质、研发上线流程等）进行考虑。\n说了这么多，那么持续交付是以什么样的形式。如何落地到我们日常的工作中的呢？结合美团外卖持续交付分享（美团外卖持续交付的前世今生）和极客时间课程持续交付36讲，我总结了如下的一种描述。（希望不会将你带偏）\n关于持续交付，不同的企业、不同的团队站在不同的角度会存在不同的定义，我们可以把持续交付定义为一个产品价值的开发框架（站在企业的角度）、一套软件工程方法论以及许许多多最佳实践的集合。持续交付的落地便是开发框架或软件工程方法论与实际情况的结合的实践（也可以说是最佳实践的排列组合），更详细的实践情况还请接着往后看。\n持续集成、持续部署、DevOps和持续交付的关系 持续集成 持续集成：（英语：Continuous integration，缩写CI），是一种软件工程流程，是将所有软件工程师的软件工作副本持续集成到共享主线（mainline）的一种举措。该名称最早由葛来迪.布区（Grady Booch）在他的布区方法中提出，在测试开发驱动（TDD）的实践中，通常还会搭配自动单元测试。持续集成的提出主要是为了解决软件进行系统集成时面临的各种问题，极限编程称这些问题为集成地狱（integration hell）。\n持续集成的关注点在于：\n频繁集成 有效协作 自动化测试 我们通常会将软件研发工作拆解，拆分成不同模块或不同团队进行编码，编码完成后，进行集成构建和测试。这个从编码到构建再到测试的反复持续过程，就叫做持续集成。由持续集成的定义可联想到版本控制，如 Git，进而可以关联到分支策略（如：Git Flow、GitHub FLow、GitLab Flow），可以说分支策略是持续集成的前置条件。一般情况下，会有一个分支作为集成使用，加上多个特性分支（这里和具体的分支策略强相关，并不是所有团队的都是一样的），当特性分支开发完毕，通过 PR 请求合并到集成分支，此时会触发持续集成工作流程，保证待合并的分支内容是有效的，只有通过持续集成流程验证，才能合并到集成分支。\n持续部署 持续部署：（英语：Continuous deployment，缩写为CD），是一种软件工程方法，意指在软件开发流程中，以自动化、频繁而且持续性的，将软件部署到生产环境（production environment）中，使软件产品能够快速的发展。持续部署可以被整合到持续集成与持续交付的流程之中。\n持续部署的关注点在于：\n自动化的 频繁的 持续性 生产环境 在一些企业的软件部署工作中，仍然存在全人工操作的方式。如果只是单纯的安装部署目的还较为容易（不过繁琐的配置，人工的不规范、不确定性，部署的耗时等都是成本）；而如果是已上线的服务的部署工作（服务更新），此时的涉及面以及受影响的范围就比较大了。就算释出的产物已经可以达到可交付的标准，但是要使得用户真正可用，还需要跨越安全、快速以及稳定部署的障碍。那么是否可以将部署的场景和过程进行抽象，使用统一的、规范的、自动化的方法论和流程来约束和实现部署的过程。而持续部署便是这样的一套方法论及实践工具集，旨在规范部署行为，通过自动化提高部署效率，达到持续部署目的。\n持续集成、持续部署与持续交付的关系 当提到持续交付的时候，总能关联到持续集成与持续部署，我也一直傻傻分不清这三者之间有什么区别，是什么样的关系。不过从前面给出的定义和关注点可以知道，持续集成侧重于编码阶段内多人协作产物集成的有效性，持续部署侧重于将产物部署到生产环境，而持续交付则侧重于需要随时可释出；而相同点在于他们三者都推崇持续性，即存在一个反馈的过程，且反馈的结果作为下一阶段持续的支持。\n再说到持续集成、持续部署和持续交付之间的关系，这里还有一个比较有趣的地方，那就是在不同的视角下，三者的关系并不一样，这里借鉴美团外卖分享的内容（美团外卖持续交付的前世今生）举例，从研发和产品的角度来分析这三者的关系。\n研发视角：我们可以看到大部分研发团队，会从软件研发的角度进行定义，他们将软件的开发步骤拆解为持续集成、持续交付、持续部署，其中持续集成指开发人员从编码到构建的过程；持续交付则作为持续集成的自然延续，指将已经集成构建完成的代码，交给测试团队进行测试的过程；持续部署指将测试通过的软件交付给用户的过程。在研发的视角下，持续交付就是一个承上启下的过程，与持续集成形成了闭环，而又为将来达到持续部署做下了准备。此时持续集成 + 持续交付 + 持续部署便是一条完整的发布流水线。\n产品视角：产品团队会站在产品的角度来看，他们认为持续交付，是从需求的 PRD 文档提出来，到用户能够感受这个需求的周期。也就是说，此时持续交付是完整的包含了持续集成与持续部署，但是持续交付涵盖范围是大于持续集成 + 持续部署。并且，此时的持续交付流程本身就包含了一条完整的发布流水线。\n借用持续交付36讲中的 发布流水线 示意图\n说到这里已经不难看出，影响三者之间相互关系的因素主要在于对于持续交付的定位。在这里先说说我自己的看法，我认为持续交付的核心要义在于：短周期、时刻可释出、持续构建，即在于持续的产出与持续验证，由产出与验证形成闭环，进而相互推动，以达到快速反应，快速实现，持续优化。而短周期便是一次交付过程耗时的预定义，也是对于效率的要求，但一定不是对用人的压榨（去你xxx）。在持续交付中，交付对象不一定就是最终用户，所以千万不要认为一定要做到端到端完整才是持续交付。持续交付是一个周期性、可持续的行为，可以只是研发到测试的闭环，此时于研发而言交付对象是测试团队，交付物为通过持续集成验证的代码；也可以如产品视角一般，从需求的 PRD 提出来到用户能够感受到这个需求的周期，此时交付对象为用户，交付物为可用的产品。\n所以持续交付可以只是一套方法论，可以是产品价值开发框架，也可以是一部分流程实践。于我个人而言，我更认同持续交付是一套方法论（兼产品价值开发框架），由此指导持续交付体系的建设。如果你问我持续交付体系实践中使用到的技术是不是持续交付，我会说，在其中使用到的技术或工具只是当前持续交付体系建设的一个组成部分。所以，关于持续集成、持续部署、持续交付三者之间的关系，我认同为：三者相互渗透（可能这个词不是很恰当），并没有绝对的独立。\nDevOps DevOps：（是 Development 和 Operations 的组合词），是一种重视“软件开发人员（Dev）”和“IT 运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。通过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。\n传统的软件组织将开发、IT 运维和质量保障设为各自分离的部门，再这样的环境下如何采用的新的开发方法（例如敏捷软件开发），是一个重要的课题。按照从前的工作方式，开发和部署，不需要 IT 或者 QA 支持（跨部门的持之），而现在却需要极其紧密的多部门协作。而 DevOps 考虑的不仅仅是软件部署，它是一套针对这几个部门间沟通与协作问题的流程和方法。","title":"持续交付（一）—— 持续交付是什么，和DevOps有什么关系"},{"content":"前言 在工作中，还没有仔细的去研究过一些算法实现，直到最近面试才知道，自己的数据结构与算法的功底这么差。 知道总是比不知道的强，那么就一个一个的来吧，我也会通过文字的方式记录自己的算法道路，下一个目标就是LeetCode。\n如题（某次面试题） 给定⼀个按开始时间从⼩到大排序的时间区间集合，请将重叠的区间合并。时间区间集合⽤用一个二维数组表示， 二维数组的每⼀行表示⼀个时间区间（闭区间），其中 0 位置元素表示时间区间开始，1 位置元素表示时间区间结束。 例 1：输入：[ [1, 3], [2, 6], [8, 10], [15, 18] ]返回： [ [1, 6], [8, 10], [15, 18]] 解释：时间区间 [1, 3] 和 [2, 6] 有部分重叠，合并之后为 [1, 6] 例 2：输入：[[1, 4], [4, 5]]返回：[[1, 5]]解释：时间区间[1，4] 和 [4，5]重叠了了⼀一个时间点，合并之后为 [1，5] 需要实现的⽅法原型：int[][] merge(int[][] intervals)\n二维数组中的每一行表示一个时间区间（闭区间），其中0位置表示开始时间，1位置表示结束时间 给定的时间区间集合按照开始时间从小到大排序，即为有序 这里给出的题还是比较简单的，因为给定的时间区间集合已经是一个有序的集合了，需要实现的内容只剩下区间合并了。 由题中可知，需要合并具有重叠部分的区间，只要确定了重叠的条件，便基本完成了解答。\n在已完成时间区间开始时间从小到大的排序后，那么此时的时间区间集合在时间横轴上回呈现类似上图的情况， 此时可以从小到大遍历时间区间集合，进行合并或收集。这里需要一个容器存放目标结果，也就是没有任何交集的时间区间。 具体实现代码如下所示：\n/** * 时间区间合并方法 * * @param intervals * @return */ public static int [][] merge(int [][] intervals) { // 二维数组行数 int rows = intervals.length; if (rows \u0026lt;= 1) { return intervals; } // 使用同样大小的二维数组用于存放计算结果 int [][] resultIntervals = new int[rows][2]; // 指向上一次计算出的结果区间 int resultIntervalsPosition = 0; resultIntervals[resultIntervalsPosition] = intervals[0]; /** * 如果给定区间集合中当前位置区间的左节点数值大于上一次计算出的结果区间的右节点， * 那么此时必定不存在交集，将该区间放入结果集合中 * 反之，则重新计算结果并更新对应的结果区间（区间右节点取最大值） */ for (int i = 1; i \u0026lt; rows; i++) { int[] currentResultInterval = resultIntervals[resultIntervalsPosition]; if (intervals[i][0] \u0026gt; currentResultInterval[1]) { resultIntervalsPosition ++; resultIntervals[resultIntervalsPosition] = intervals[i]; } else { currentResultInterval[1] = Math.max(intervals[i][1], currentResultInterval[1]); resultIntervals[resultIntervalsPosition] = currentResultInterval; } } // 使用相等大小的数组作为结果集时，可能会出现结果集中存在空值情况，所以在末尾进行一次拷贝 return Arrays.copyOf(resultIntervals, resultIntervalsPosition + 1, resultIntervals.getClass()); } LeetCode类似题 合并区间\n具体实现思路可参见题解，这里给出的条件相对上面的内容来说，少了有序，需要自己进行排序 具体实现如下：\npackage com.zhoujian.algorithm; import java.util.ArrayList; import java.util.Arrays; import java.util.List; /** * 合并时间区间集合 * * 给定条件: * * 1、给定的时间区间集合按照开始时间从小到大排序，即为有序 * * 2、其中每一行表示一个时间区间（闭区间），0表示时间区间的开始，1表示时间区间结束 * * @author zhoujian */ public class MergeTimeIntervals { public static void main(String[] args) { int [][] timeIntervals = {{1, 3}, {2, 6}, {8, 10}, {15, 18}}; // int [][] timeIntervals = {{4,5}, {1,4}, {0,1}}; // int [][] timeIntervals = {{1, 4}, {4, 5}}; int [][] result = merge(timeIntervals); System.out.println(Arrays.deepToString(result)); } /** * 时间区间合并方法 * * @param intervals * @return */ public static int [][] merge(int [][] intervals) { // 二维数组行数 int rows = intervals.length; if (rows \u0026lt;= 1) { return intervals; } // 排序 sort(intervals, rows); // 使用同样大小的二维数组用于存放计算结果 int [][] resultIntervals = new int[rows][2]; // 指向上一次计算出的结果区间 int resultIntervalsPosition = 0; resultIntervals[resultIntervalsPosition] = intervals[0]; /** * 如果给定区间集合中当前位置区间的左节点数值大于上一次计算出的结果区间的右节点， * 那么此时必定不存在交集，将该区间放入结果集合中 * 反之，则重新计算结果并更新对应的结果区间（区间右节点取最大值） */ for (int i = 1; i \u0026lt; rows; i++) { int[] currentResultInterval = resultIntervals[resultIntervalsPosition]; if (intervals[i][0] \u0026gt; currentResultInterval[1]) { resultIntervalsPosition ++; resultIntervals[resultIntervalsPosition] = intervals[i]; } else { currentResultInterval[1] = Math.max(intervals[i][1], currentResultInterval[1]); resultIntervals[resultIntervalsPosition] = currentResultInterval; } } // 使用相等大小的数组作为结果集时，可能会出现结果集中存在空值情况，所以在末尾进行一次拷贝 return Arrays.copyOf(resultIntervals, resultIntervalsPosition + 1, resultIntervals.getClass()); } /** * 冒泡排序, 对二维数组按照开始时间节点进行排序 * @param intervals * @param size * @return */ private static int [][] sort(int [][] intervals, int size) { if (size \u0026lt;= 1) { return intervals; } for (int i = 0; i \u0026lt; size; ++i) { boolean flag = false; for (int j = 0; j \u0026lt; size - i - 1; ++j) { if (intervals[j][0] \u0026gt; intervals[j + 1][0]) { // 开始交换 int [] temp = intervals[j]; intervals[j] = intervals[j + 1]; intervals[j + 1] = temp; // 表示存在数据交换 flag = true; } } if (!flag){ // 没有数据交换，提前退出 break; } } return intervals; } } 冒泡算法理解 冒泡排序，使用两个循环嵌套完成，外层循环为冒泡次数，控制完整的排序行为， 内层循环则是从有限的数据集合中查找最大值或最小值，并将该值传递到最右端或者是最左端。 每一次的内层循环都是在有限的数据集合中寻找，每进行一次冒泡，便相应的从有限的数据集中找到最大或最小值， 该有限数据集合在逻辑层面便少了一个需要排序的值，即下一次冒泡总是比前一次少计算一个值。 内层循环旨在寻找最大或最小，如果数据还未完全有序，那么该次冒泡必定会发生数据交换。 若是冒泡计算中未发生数据交换，则意味着本次排序已经完成。数据交换作为优化条件， 表示并不是每一次排序都需要N次冒泡（假设数据集合长度为N）。\n参考 LeetCode-合并区间 数据结构与算法之美\u0026ndash;排序（上） ","permalink":"https://fuyi-atlas.github.io/posts/program/algorithm/merge-interval-001/","summary":"前言 在工作中，还没有仔细的去研究过一些算法实现，直到最近面试才知道，自己的数据结构与算法的功底这么差。 知道总是比不知道的强，那么就一个一个的来吧，我也会通过文字的方式记录自己的算法道路，下一个目标就是LeetCode。\n如题（某次面试题） 给定⼀个按开始时间从⼩到大排序的时间区间集合，请将重叠的区间合并。时间区间集合⽤用一个二维数组表示， 二维数组的每⼀行表示⼀个时间区间（闭区间），其中 0 位置元素表示时间区间开始，1 位置元素表示时间区间结束。 例 1：输入：[ [1, 3], [2, 6], [8, 10], [15, 18] ]返回： [ [1, 6], [8, 10], [15, 18]] 解释：时间区间 [1, 3] 和 [2, 6] 有部分重叠，合并之后为 [1, 6] 例 2：输入：[[1, 4], [4, 5]]返回：[[1, 5]]解释：时间区间[1，4] 和 [4，5]重叠了了⼀一个时间点，合并之后为 [1，5] 需要实现的⽅法原型：int[][] merge(int[][] intervals)\n二维数组中的每一行表示一个时间区间（闭区间），其中0位置表示开始时间，1位置表示结束时间 给定的时间区间集合按照开始时间从小到大排序，即为有序 这里给出的题还是比较简单的，因为给定的时间区间集合已经是一个有序的集合了，需要实现的内容只剩下区间合并了。 由题中可知，需要合并具有重叠部分的区间，只要确定了重叠的条件，便基本完成了解答。\n在已完成时间区间开始时间从小到大的排序后，那么此时的时间区间集合在时间横轴上回呈现类似上图的情况， 此时可以从小到大遍历时间区间集合，进行合并或收集。这里需要一个容器存放目标结果，也就是没有任何交集的时间区间。 具体实现代码如下所示：\n/** * 时间区间合并方法 * * @param intervals * @return */ public static int [][] merge(int [][] intervals) { // 二维数组行数 int rows = intervals.","title":"合并区间"},{"content":"前言 终于还是吃了自己的狗粮\u0026hellip;\u0026hellip;\n关于 客户端-服务端 网络模型 常规情况下，网络应用都会存在客户端和服务器端，比如平时外卖应用一样，我们在外卖应用上的操作，都对应着客户端应用向服务器发起请求，并收到响应的过程。服务器为客户端提供业务数据支持，客户端则为用户提供交互界面。\n在网络编程中，具体到客户端 - 服务器模型时，我们经常会考虑是使用TCP还是UDP，其实它们二者的区别也很简单：在TCP中连接是谁发起的，在UDP中报文是谁发送的。在TCP中，建立连接是一个非常重要的环节。区别出客户端和服务器，本质上是因为二者编程模型是的不同的。\n服务器端需要在一开始就监听在一个确定的端口上，等待客户端发送请求，一旦有客户端建立连接，服务器端则会消耗一定的计算机资源为它服务。\n客户端相对简单，它向服务器的监听端口发起请求，连接建立之后，通过连接通路和服务器端进行通信。\n还有一点要强调的是，无论是客户端还是服务器端，它们运行的基本单位都是进程（Process），而不是机器。一个客户端，可以同时建立多个到不同服务器的连接；而服务器更是可能在一台机器上部署运行多个服务。\n什么是 socket socket是一种操作系统提供的进程间通信机制。这里并不局限于本地，可以是本地进程间的通信，也可以是远端进程间的通信。在操作系统中，通常会为应用程序提供一组应用程序接口（API），称为套接字接口（socket API）。应用程序可以通过套接字接口来使用套接字（socket），已进行数据交换。\n这里要注意一下，我们常说的TCP和UDP只是传输层协议，是一种约定。TCP三次握手则是基于TCP协议创建网络通路，该通路的具体创建与实现还是socket完成。socket是我们用来建立连接、传输数据的唯一途径。\n如何使用 socket 建立连接 通过前面的客户端 - 服务器模型，我们知道至少需要一对套接字才能进行网络连接的建立，它们分别是服务端套接字和客户端套接字，这里我们先从服务端说起。\n服务端准备连接过程 创建套接字（我们这里会使用 TCP的实现） 绑定监听地址：即为绑定需要监听的 IP地址以及 端口号，这里也可以使用本机 IP，但是考虑到部署环境 IP可能会发生变化，所以这里需要进行 IP地址的绑定（比如进行通配地址指定，或者主机存在多张网卡时指定具体的 IP）。如果不显式的指定端口号，就意味着把端口的选择权交给操作系统内核来处理，操作系统内核会根据一定的算法选择一个空闲的端口，完成套接字的绑定。 开启套接字监听模式：bind函数只是实现套接字与地址的关联，如同登记了电话号码，如果要让别人打通带年华，还需要我们把电话设备接入电话线，让服务器真正处于可接听的状态，这个过程需要依赖 listen函数。这里可以这么理解，socket存在 主动和 被动模式，比如服务器就是处于 被动模式下，它需要等待客户端套接字的 主动连接。而 listen函数便是可以将套接字设置为 被动模式，即告诉内核：“我这个套接字是用来等待用户请求的”。 建立连接（accept阻塞）：在客户端连接请求到达时，服务端应答成功，便完成连接建立。 package com.zhoujian.socket; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; /** * 线程池工具类 * @author zhoujian */ public class ExecutorServicePool { /** * 初始化线程池 */ public static ExecutorService executorService = Executors.newFixedThreadPool(10); } package com.zhoujian.socket.server; import com.zhoujian.socket.ExecutorServicePool; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.net.ServerSocket; import java.net.Socket; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; /** * Socket服务端示例 * @author zhoujian */ public class SocketServer { /** * 监听端口 */ private static int PORT = 8088; private ServerSocket serverSocket; private static Logger logger = LoggerFactory.getLogger(SocketServer.class); public static void main(String[] args) { try { new SocketServer().startUp(PORT); } catch (IOException e) { e.printStackTrace(); } } private void startUp(int port) throws IOException { /** * 在初始化的过程中先后完成了监听地址绑定和 listen 函数调用 */ serverSocket = new ServerSocket(port); logger.info(\u0026#34;Socket Server is online, listening at port {}\u0026#34;, PORT); while (true){ /** * 此处阻塞，等待客户端连接，在三次握手成功完成后，释放阻塞 */ Socket socket = serverSocket.accept(); logger.info(\u0026#34;socket port is {} connect successful\u0026#34;, socket.getPort()); ExecutorServicePool.executorService.execute(new AnswerThread(socket)); } } /** * 应答线程 */ static class AnswerThread implements Runnable { private Socket socket; public AnswerThread(Socket socket){ this.socket = socket; } @Override public void run() { String content = null; BufferedReader bufferedReader = null; try { bufferedReader = new BufferedReader(new InputStreamReader(socket.getInputStream(), \u0026#34;UTF-8\u0026#34;)); /** * 这里的判断条件就是数据发送完毕的标识 */ while ((content = bufferedReader.readLine()) != null){ logger.info(\u0026#34;form client: {}\u0026#34;, content); socket.getOutputStream().write(content.getBytes()); socket.getOutputStream().write(\u0026#34;\\n\u0026#34;.getBytes()); socket.getOutputStream().flush(); } } catch (IOException e) { e.printStackTrace(); } } } } 客户端发起连接过程 创建套接字 使用 connect发起连接：调用 connect函数向服务端发起连接请求，这里传入的是服务端套接字的地址，connect函数可以看做是将套接字转换为 主动模式。这里值得注意的是，客户端在调用 connect函数前不是非得调用 bind函数，因为如果需要（TCP|UDP|本地 套接字）的话，操作系统内核会确定源IP地址，并按照一定的算法选择一个临时端口作为源端口（这里是客户端，服务端应当先完成地址绑定，因为需要一个稳定的地址进行标记，客户端大可不必，还可以减小端口冲突的可能）。在这里我们使用的是 TCP套接字，在调用 connect函数时将激发 TCP的三次握手，贴图如下（图片来自于 极客时间），这里务必注意阻塞状态的改变情况: 注意：Read方法也是阻塞的，当调用Read方法是，它就会一直阻塞在那里，直到另一方告诉它数据已经发送完毕（一般情况下，都会使用长度进行控制，这里是采用`\\n`来作为数据完结发送的标识） package com.zhoujian.socket.client; import com.zhoujian.socket.ExecutorServicePool; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.*; import java.net.Socket; /** * Socket客户端示例 * @author zhoujian */ public class SocketClient { /** * 服务端套接字IP地址 */ private static String HOST = \u0026#34;127.0.0.1\u0026#34;; /** * 服务端套接字监听端口 */ private static int PORT = 8088; private static Logger logger = LoggerFactory.getLogger(SocketClient.class); /** * 于主线程中初始化客户端套接字，并完成与服务端套接字的连接 * @param args * @throws IOException */ public static void main(String[] args) throws IOException { Socket client = new Socket(HOST, PORT); ExecutorServicePool.executorService.execute(new ReceiveThread(client)); BufferedReader reader = new BufferedReader(new InputStreamReader(System.in, \u0026#34;UTF-8\u0026#34;)); BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(client.getOutputStream())); while (true){ String msg = reader.readLine(); writer.write(msg); writer.write(\u0026#34;\\n\u0026#34;); writer.flush(); } } /** * 用于接收服务端套接字的应答 */ static class ReceiveThread implements Runnable{ private Socket socket; public ReceiveThread(Socket socket){ this.socket = socket; } @Override public void run() { String receive = null; BufferedReader bufferedReader = null; try { bufferedReader = new BufferedReader(new InputStreamReader(socket.getInputStream(), \u0026#34;UTF-8\u0026#34;)); while ((receive = bufferedReader.readLine()) != null){ logger.info(\u0026#34;from server: {}\u0026#34;, receive); } } catch (IOException e) { e.printStackTrace(); } } } } 运行截图 服务端套接字启动 客户端A套接字启动 客户端B套接字启动 客户端A套接字与服务端套接字通信 客户端B套接字与服务端套接字通信 拓展 待更新\u0026hellip;\u0026hellip;\n引用 网络编程模型：认识客户端-服务器网络模型的基本概念\n套接字和地址：像电话和电话号码一样理解它们\nTCP三次握手：怎么使用套接字格式建立连接\n使用套接字进行读写：开始交流吧\nJava socket详解，看这一篇就够了\n说明 本节内容涉及的完整代码地址：socket-example\n本文内容大部分源自极客时间以及网络博客图文内容节选，如有冒犯，还请告知我进行处理 邮箱：thread_zhou@126.com ","permalink":"https://fuyi-atlas.github.io/posts/program/network-program/simple-case-001/","summary":"前言 终于还是吃了自己的狗粮\u0026hellip;\u0026hellip;\n关于 客户端-服务端 网络模型 常规情况下，网络应用都会存在客户端和服务器端，比如平时外卖应用一样，我们在外卖应用上的操作，都对应着客户端应用向服务器发起请求，并收到响应的过程。服务器为客户端提供业务数据支持，客户端则为用户提供交互界面。\n在网络编程中，具体到客户端 - 服务器模型时，我们经常会考虑是使用TCP还是UDP，其实它们二者的区别也很简单：在TCP中连接是谁发起的，在UDP中报文是谁发送的。在TCP中，建立连接是一个非常重要的环节。区别出客户端和服务器，本质上是因为二者编程模型是的不同的。\n服务器端需要在一开始就监听在一个确定的端口上，等待客户端发送请求，一旦有客户端建立连接，服务器端则会消耗一定的计算机资源为它服务。\n客户端相对简单，它向服务器的监听端口发起请求，连接建立之后，通过连接通路和服务器端进行通信。\n还有一点要强调的是，无论是客户端还是服务器端，它们运行的基本单位都是进程（Process），而不是机器。一个客户端，可以同时建立多个到不同服务器的连接；而服务器更是可能在一台机器上部署运行多个服务。\n什么是 socket socket是一种操作系统提供的进程间通信机制。这里并不局限于本地，可以是本地进程间的通信，也可以是远端进程间的通信。在操作系统中，通常会为应用程序提供一组应用程序接口（API），称为套接字接口（socket API）。应用程序可以通过套接字接口来使用套接字（socket），已进行数据交换。\n这里要注意一下，我们常说的TCP和UDP只是传输层协议，是一种约定。TCP三次握手则是基于TCP协议创建网络通路，该通路的具体创建与实现还是socket完成。socket是我们用来建立连接、传输数据的唯一途径。\n如何使用 socket 建立连接 通过前面的客户端 - 服务器模型，我们知道至少需要一对套接字才能进行网络连接的建立，它们分别是服务端套接字和客户端套接字，这里我们先从服务端说起。\n服务端准备连接过程 创建套接字（我们这里会使用 TCP的实现） 绑定监听地址：即为绑定需要监听的 IP地址以及 端口号，这里也可以使用本机 IP，但是考虑到部署环境 IP可能会发生变化，所以这里需要进行 IP地址的绑定（比如进行通配地址指定，或者主机存在多张网卡时指定具体的 IP）。如果不显式的指定端口号，就意味着把端口的选择权交给操作系统内核来处理，操作系统内核会根据一定的算法选择一个空闲的端口，完成套接字的绑定。 开启套接字监听模式：bind函数只是实现套接字与地址的关联，如同登记了电话号码，如果要让别人打通带年华，还需要我们把电话设备接入电话线，让服务器真正处于可接听的状态，这个过程需要依赖 listen函数。这里可以这么理解，socket存在 主动和 被动模式，比如服务器就是处于 被动模式下，它需要等待客户端套接字的 主动连接。而 listen函数便是可以将套接字设置为 被动模式，即告诉内核：“我这个套接字是用来等待用户请求的”。 建立连接（accept阻塞）：在客户端连接请求到达时，服务端应答成功，便完成连接建立。 package com.zhoujian.socket; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; /** * 线程池工具类 * @author zhoujian */ public class ExecutorServicePool { /** * 初始化线程池 */ public static ExecutorService executorService = Executors.newFixedThreadPool(10); } package com.","title":"Socket简单案例实现"},{"content":"前言 记录一下PostgreSQL11.2的数据恢复，场景是这样的，我们在A主机（windows server 2008）上安装一个PostgreSQL11.2，突然有一天A主机坏掉了，只能将磁盘卸载下来挂载到新的主机上，但是如何才能快速有效的将之前磁盘中的数据恢复呢？\n重新注册为windows服务（失败） 这里我使用主机B（windows server 2012 R2）进行重新注册 参考重新注册PostgreSQL服务\n找到并PostgreSQL的安装路径，在此启动命令行工具 使用pg_ctl命令将PostgreSQL11.2重新注册为服务，该命令的具体用法和写法可以查看pg_ctl帮助文档（pg_ctl \u0026ndash;help） 结果很现实，没有能够成功，具体的原因未知，返回提示消息为：无法注册服务，错误码1783\n此路不通，那我只能换一种方式。\n替换PostgreSQL的数据目录（成功） PostgreSQL的数据是存储到data目录下，具体的目录是服务安装的时候指定的，默认是在服务的安装目录下。只要你还拥有完整的data目录，那么你是完全可以从这一份原始物理文件进行数据恢复的。\n在这里我使用主机C（windows10）作为测试机 先在C主机上安装PostgreSQL11.2 将原始data目录拷贝到C主机PostgreSQL数据存在目录（即data）的同级目录，并重命名为data-bak（命名随意，符合规则且不冲突就行） 然后参照Change the default PGDATA directory on Windows完成剩余替换步骤 停止C主机的PostgreSQl11.2服务 修改注册表，用意为重新指定服务使用的data目录，替换为我拷贝的data-bak目录，注册表路径：HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\pgsql-some version，修改ImagePath项的value值中 -D 参数 到windows服务管理中查看PostgreSQL服务属性，检验可执行文件路径中是否已经指向了data-bak路径 重启PostgreSQL服务 这里有坑，请注意！ 如果出现服务启动后停止，请进行以下检查： 1. 检查 data-bak 目录权限，确保该目录和同级的 data 目录的权限一致 2. 检查 是否存在 postmaster.pid 文件，如果存在将其删除（这个未测试，听说可以哦） 3. 检查 postgresql.conf 文件，保持端口号、语言环境（lc_messages、lc_monetary、lc_numeric、lc_time）等基础配置和当前的服务一致，避免配置异常导致启动失败 4. 检查 pg_hba.conf 文件 我按照步骤完成了以上操作，但是发现还是无法在服务管理中启动服务，我通过事件查看器查看日志，也只是看到超时导致启动失败，遂又google+baidu一把，获取链接一个postgresql 在等待服务器启动时超时\n在这里提到需要执行一个命令：pg_resetwal -f E:\\Server\\PostgreSQL\\11\\data-bak 执行成功会得到打印信息：Write-ahead log reset\n然后可以直接在命令行验证是否可以正常启动：pg_ctl -D E:\\Server\\PostgreSQL\\11\\data-bak\n登录密码应该是旧的密码，我这里是直接使用刚安装PostgreSQL产生的postgresql.conf 替换了data-bak中的postgresql.conf ，所以我的登录密码是新设置的，你实在不知道你就改嘛\n我到这里已经能够成功启动了，但是还有坑，还是因为语言环境导致的，如果你的语言环境都一致，那么应该没啥问题了，问题解决方案看后记，最下面\n以下部分为pg_resetwal命令的网络解释pg_resetwal\npg_resetwal — 重置一个PostgreSQL数据库集簇的预写式日志以及其他控制信息 pg_resetwal [-f] [-n] [option...] {[-D] datadir} pg_resetwal会清除预写式日志（WAL）并且有选择地重置存储在 pg_control文件中的一些其他控制信息。如果这些文件已经被损坏， 某些时候就需要这个功能。当服务器由于这样的损坏而无法启动时， 这只应该被用作最后的手段。在运行这个命令之后，就应该可以启动服务器， 但是记住数据库可能包含由于部分提交事务产生的不一致数据。 你应当立刻转储你的数据、运行initdb并且重新载入。重新载入后， 检查不一致并且根据需要修复之。这个工具只能被安装服务器的用户运行，因为它要求对数据目录的读写访问。 出于安全原因，你必须在命令行中指定数据目录。pg_resetwal 不使用环境变量PGDATA。如果pg_resetwal抱怨它无法为pg_control 决定合法数据，你可以通过指定-f（强制）选项强制它继续。 在这种情况下，丢失的数据将被替换为看似合理的值。可以期望大部分域是匹配的， 但是下一个 OID、下一个事务 ID 和纪元、下一个多事务 ID 和偏移以及 WAL 开始地址域可能还是需要人工协助。这些域可以使用下面讨论的选项设置。 如果你不能为所有这些域决定正确的值，-f还是可以被使用， 但是恢复的数据库还是值得怀疑：一次立即的转储和重新载入是势在必行的。 在你转储之前不要在该数据库中执行任何数据修改操作， 因为任何这样的动作都可能使破坏更严重。 后记 通过替换data目录的方式，我已经成功启动了PostgreSQL服务，但是我在登录的时候出现了错误，提示信息如下： 数据库集群是以 LC_COLLATE \u0026ldquo;Chinese (Simplified)_People\u0026rsquo;s Republic of China.936\u0026quot;来初始化的，这个排序规则无法由setlocale()识别\n没错，我之前确实是将postgresql.conf配置文件中的语言环境从Chinese (Simplified)_People\u0026rsquo;s Republic of China.936修改为Chinese (Simplified)_China.936，不然服务没有办法启动。\n我\u0026hellip;\u0026hellip;，就差这临门一脚，怎么办，先看看能不能在我的windows10笔记本上切换语言环境。\n然后 \u0026hellip;\u0026hellip; \u0026hellip;\u0026hellip; \u0026hellip;\u0026hellip; \u0026hellip;\u0026hellip;\n算了，这个肯定不是什么好办法，我不能设置环境，那我找一个支持的环境总行了吧，原始环境我是没治了，好像win7的语言环境和原始环境一致（Chinese (Simplified)_People\u0026rsquo;s Republic of China.936），好好好，看到了希望的曙光\n咳咳，搜索引擎诚不欺我也，哈哈哈！恢复完成。\n其实，一开始就可以先看看官方的文档的\u0026hellip;\u0026hellip;，Mark一下PostgreSQL 11.2 手册\n参考 Recover postgreSQL databases from raw physical files Change the default PGDATA directory on Windows postgresql 在等待服务器启动时超时 pg_resetwal postgresql - 从原始物理文件恢复PostgreSQL数据库 无效的语言环境名称 Chinese (Simplified)_People\u0026rsquo;\u0026rsquo;s Republic of China.936 重新注册PostgreSQL服务 ","permalink":"https://fuyi-atlas.github.io/posts/program/database/postgresql/11.2-data-recovery/","summary":"前言 记录一下PostgreSQL11.2的数据恢复，场景是这样的，我们在A主机（windows server 2008）上安装一个PostgreSQL11.2，突然有一天A主机坏掉了，只能将磁盘卸载下来挂载到新的主机上，但是如何才能快速有效的将之前磁盘中的数据恢复呢？\n重新注册为windows服务（失败） 这里我使用主机B（windows server 2012 R2）进行重新注册 参考重新注册PostgreSQL服务\n找到并PostgreSQL的安装路径，在此启动命令行工具 使用pg_ctl命令将PostgreSQL11.2重新注册为服务，该命令的具体用法和写法可以查看pg_ctl帮助文档（pg_ctl \u0026ndash;help） 结果很现实，没有能够成功，具体的原因未知，返回提示消息为：无法注册服务，错误码1783\n此路不通，那我只能换一种方式。\n替换PostgreSQL的数据目录（成功） PostgreSQL的数据是存储到data目录下，具体的目录是服务安装的时候指定的，默认是在服务的安装目录下。只要你还拥有完整的data目录，那么你是完全可以从这一份原始物理文件进行数据恢复的。\n在这里我使用主机C（windows10）作为测试机 先在C主机上安装PostgreSQL11.2 将原始data目录拷贝到C主机PostgreSQL数据存在目录（即data）的同级目录，并重命名为data-bak（命名随意，符合规则且不冲突就行） 然后参照Change the default PGDATA directory on Windows完成剩余替换步骤 停止C主机的PostgreSQl11.2服务 修改注册表，用意为重新指定服务使用的data目录，替换为我拷贝的data-bak目录，注册表路径：HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\pgsql-some version，修改ImagePath项的value值中 -D 参数 到windows服务管理中查看PostgreSQL服务属性，检验可执行文件路径中是否已经指向了data-bak路径 重启PostgreSQL服务 这里有坑，请注意！ 如果出现服务启动后停止，请进行以下检查： 1. 检查 data-bak 目录权限，确保该目录和同级的 data 目录的权限一致 2. 检查 是否存在 postmaster.pid 文件，如果存在将其删除（这个未测试，听说可以哦） 3. 检查 postgresql.conf 文件，保持端口号、语言环境（lc_messages、lc_monetary、lc_numeric、lc_time）等基础配置和当前的服务一致，避免配置异常导致启动失败 4. 检查 pg_hba.conf 文件 我按照步骤完成了以上操作，但是发现还是无法在服务管理中启动服务，我通过事件查看器查看日志，也只是看到超时导致启动失败，遂又google+baidu一把，获取链接一个postgresql 在等待服务器启动时超时\n在这里提到需要执行一个命令：pg_resetwal -f E:\\Server\\PostgreSQL\\11\\data-bak 执行成功会得到打印信息：Write-ahead log reset\n然后可以直接在命令行验证是否可以正常启动：pg_ctl -D E:\\Server\\PostgreSQL\\11\\data-bak\n登录密码应该是旧的密码，我这里是直接使用刚安装PostgreSQL产生的postgresql.conf 替换了data-bak中的postgresql.conf ，所以我的登录密码是新设置的，你实在不知道你就改嘛\n我到这里已经能够成功启动了，但是还有坑，还是因为语言环境导致的，如果你的语言环境都一致，那么应该没啥问题了，问题解决方案看后记，最下面","title":"PostgreSQL11.2 数据恢复记录（From Physical Files）"},{"content":" 前言 我一直认为 Spring Data JPA 是一个好东西，有着自己独特的黑魔法。但是由于目前接触甚少，不知道该如何开启。所以想通过从无到有的过程，逐渐的去认识它，搞清楚它与Mybatis的关系，如果站在架构的角度看会怎么怎么样的情况。在这个过程中，会使用文字的方式将过程记录下来，也算是一点经历。\nSpring Data JPA 介绍 From spring.io：Spring Data JPA是Spring Data系列的一个组成部分，可以轻松快捷的实现数据访问层的增强支持，这使得基于Spring且使用了数据库访问技术的应用程序更加容易构建。Spring Data JPA 内置了简单数据库读写操作，包括分页查询，并提供接口以待增强。\nSpring Data JPA 是基于Hibernate（在3.2版本中便对JPA提供了完全的支持）、JPA规范的基础上封装的一套ORM框架，可以说就是JPA规范的一个实践落地的产品。Spring Data JPA的内置实现中提供了包括增删改查、分页、自定义SQL的常用功能，且提供接口以待拓展增强。基于Spring Data JPA可以简洁的代码，快速的实现对数据库的访问。\n使用示例 环境说明:\nwindows 10 专业版 IntelliJ IDEA 2019.3.1 JDK 1.8 maven 3.6.1 Spring Boot 2.2.5.RELEASE 引入依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 我这里使用mysql作为数据存储 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; 定义一个简单实体 import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.GenerationType; import javax.persistence.Id; /** * @author zhoujian */ @Entity public class Customer { @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String firstName; private String lastName; protected Customer() {} public Customer(String firstName, String lastName) { this.firstName = firstName; this.lastName = lastName; } public Long getId() { return id; } public void setId(Long id) { his.id = id; } public String getFirstName() { return firstName; } public String getLastName() { return lastName; } @Override public String toString() { return String.format(\u0026#34;Customer[id=%d, firstName=\u0026#39;%s\u0026#39;, lastName=\u0026#39;%s\u0026#39;]\u0026#34;, id, firstName, lastName); } } 创建查询接口 import org.springframework.data.repository.CrudRepository; import java.util.List; /** * {@link Customer}实体的仓库接口 * @author zhoujian */ public interface CustomerRepository extends CrudRepository\u0026lt;Customer, Long\u0026gt; { /** * 根据{@link Customer#lastName}查询数据 * @param lastName {@link Customer}的属性 * @return customer list */ List\u0026lt;Customer\u0026gt; findByLastName(String lastName);} 简单测试 这里只是方便测试，所以直接使用了 CommandLineRunner 进行业务测试，在这里也直接与 Repository 层耦合，也不是通过单元测试，并不推荐这样的做法。\nimport org.slf4j.Logger;import org.slf4j.LoggerFactory; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; /** * @author zhoujian */ @SpringBootApplication public class ForTestOrmApplication { public static void main(String[] args) { SpringApplication.run(ForTestOrmApplication.class, args); } private static final Logger LOG = LoggerFactory.getLogger(ForTestOrmApplication.class); @Bean public CommandLineRunner jpaTestRunner(CustomerRepository customerRepository){ return (args) -\u0026gt; { // 插入五条 customer 数据 customerRepository.save(new Customer(\u0026#34;Jack\u0026#34;, \u0026#34;Liu\u0026#34;)); customerRepository.save(new Customer(\u0026#34;Mary\u0026#34;, \u0026#34;Liu\u0026#34;)); customerRepository.save(new Customer(\u0026#34;Bob\u0026#34;, \u0026#34;Huang\u0026#34;)); customerRepository.save(new Customer(\u0026#34;Jim\u0026#34;, \u0026#34;Chen\u0026#34;)); customerRepository.save(new Customer(\u0026#34;David\u0026#34;, \u0026#34;Gao\u0026#34;)); // 获取所有的 customer 数据 LOG.info(\u0026#34;pull all customers with findAll():\u0026#34;); LOG.info(\u0026#34;---------------------------------\u0026#34;); customerRepository.findAll().forEach( customer -\u0026gt; LOG.info(\u0026#34;Customer is : {}\u0026#34;, customer.toString()) ); // 获取所有的 customer 数据 LOG.info(\u0026#34;pull all customers with findByLastName():\u0026#34;); LOG.info(\u0026#34;---------------------------------\u0026#34;); customerRepository.findByLastName(\u0026#34;Liu\u0026#34;).forEach( customer -\u0026gt; LOG.info(\u0026#34;Customer is : {}\u0026#34;, customer.toString()) ); }; } } 测试期望如下：\n2020-04-20 08:50:01.195 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : pull all customers with findAll(): 2020-04-20 08:50:01.195 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : --------------------------------- Hibernate: select customer0_.id as id1_0_, customer0_.first_name as first_na2_0_, customer0_.last_name as last_nam3_0_ from customer customer0_ 2020-04-20 08:50:01.293 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : Customer is : Customer[id=1, firstName=\u0026#39;Jack\u0026#39;, lastName=\u0026#39;Liu\u0026#39;] 2020-04-20 08:50:01.293 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : Customer is : Customer[id=2, firstName=\u0026#39;Mary\u0026#39;, lastName=\u0026#39;Liu\u0026#39;] 2020-04-20 08:50:01.293 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : Customer is : Customer[id=3, firstName=\u0026#39;Bob\u0026#39;, lastName=\u0026#39;Huang\u0026#39;] 2020-04-20 08:50:01.293 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : Customer is : Customer[id=4, firstName=\u0026#39;Jim\u0026#39;, lastName=\u0026#39;Chen\u0026#39;] 2020-04-20 08:50:01.293 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : Customer is : Customer[id=5, firstName=\u0026#39;David\u0026#39;, lastName=\u0026#39;Gao\u0026#39;] 2020-04-20 08:50:01.293 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : pull all customers with findByLastName(): 2020-04-20 08:50:01.293 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : --------------------------------- Hibernate: select customer0_.id as id1_0_, customer0_.first_name as first_na2_0_, customer0_.last_name as last_nam3_0_ from customer customer0_ where customer0_.last_name=? 2020-04-20 08:50:01.311 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : Customer is : Customer[id=1, firstName=\u0026#39;Jack\u0026#39;, lastName=\u0026#39;Liu\u0026#39;] 2020-04-20 08:50:01.312 INFO 12016 --- [ main] c.j.for_test_orm.ForTestOrmApplication : Customer is : Customer[id=2, firstName=\u0026#39;Mary\u0026#39;, lastName=\u0026#39;Liu\u0026#39;] 参考 guides/accessing-data-jpa ","permalink":"https://fuyi-atlas.github.io/posts/program/spring-data-jpa/quick-start/","summary":"前言 我一直认为 Spring Data JPA 是一个好东西，有着自己独特的黑魔法。但是由于目前接触甚少，不知道该如何开启。所以想通过从无到有的过程，逐渐的去认识它，搞清楚它与Mybatis的关系，如果站在架构的角度看会怎么怎么样的情况。在这个过程中，会使用文字的方式将过程记录下来，也算是一点经历。\nSpring Data JPA 介绍 From spring.io：Spring Data JPA是Spring Data系列的一个组成部分，可以轻松快捷的实现数据访问层的增强支持，这使得基于Spring且使用了数据库访问技术的应用程序更加容易构建。Spring Data JPA 内置了简单数据库读写操作，包括分页查询，并提供接口以待增强。\nSpring Data JPA 是基于Hibernate（在3.2版本中便对JPA提供了完全的支持）、JPA规范的基础上封装的一套ORM框架，可以说就是JPA规范的一个实践落地的产品。Spring Data JPA的内置实现中提供了包括增删改查、分页、自定义SQL的常用功能，且提供接口以待拓展增强。基于Spring Data JPA可以简洁的代码，快速的实现对数据库的访问。\n使用示例 环境说明:\nwindows 10 专业版 IntelliJ IDEA 2019.3.1 JDK 1.8 maven 3.6.1 Spring Boot 2.2.5.RELEASE 引入依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 我这里使用mysql作为数据存储 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; 定义一个简单实体 import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.GenerationType; import javax.persistence.Id; /** * @author zhoujian */ @Entity public class Customer { @Id @GeneratedValue(strategy= GenerationType.","title":"Spring Data JPA 使用入门"},{"content":" 前言 其实，手写jdbc也是蛮好玩的\u0026hellip;\n在企业开发过程中，除去一些特殊的要求外，基本上都会使用全自动或半自动的ORM框架代替原生JDBC进行数据库的访问。而在具体项目设计时，常常会根据项目业务情况进行技术选型。其中常用的ORM框架有：\nMybatis Hibernate Spring Data JPA JdbcTemplate 在这里我们主要讨论Mybatis与Spring Data JPA。\nSpring Data JPA 1、JPA是什么 维基百科：JPA（Java Persistence API，Java持久化API）是一个Java应用程序接口规范，描述了使用Java标准平台（Java SE）和Java企业版平台（Java EE）的应用中的关系数据的管理。\nSpring Data JPA 全面解析：JPA (Java Persistence API) 是 Sun 官方提出的 Java 持久化规范。它为 Java 开发人员提供了一种对象/关联映射工具来管理 Java 应用中的关系数据。他的出现主要是为了简化现有的持久化开发工作和整合 ORM 技术，结束现在 Hibernate，TopLink，JDO 等 ORM 框架各自为营的局面。值得注意的是，JPA 是在充分吸收了现有 Hibernate，TopLink，JDO 等ORM框架的基础上发展而来的，具有易于使用，伸缩性强等优点。从目前的开发社区的反应上看，JPA 受到了极大的支持和赞扬，其中就包括了 Spring 与 EJB3.0 的开发团队\nJPA所维护的核心是实体（Entity Bean），而它是通过一个持久化上下文（Persistence Context）来使用的。持久化上下文包含了以前3个部分：\n对象关系映射（Object Relational Mapping，简称ORM）描述，JPA支持注解或XML两种形式的描述。 实体操作API，内置通用CRUD操作，来完成对象的持久化与查询。 查询语言，约定了面向对象的查询语句JPQL。 简单来说，JPA就是一种接口规范，一种定义了对象关系映射（ORM）以及实体对象的持久化接口标准的规范，并不是一套直接可用的产品（如：Hibernate）。\n2、什么是Spring Data JPA spring-data-jpa：Spring Data JPA是Spring Data系列的一个组成部分，可以轻松快捷的实现数据访问层的增强支持，这使得基于Spring且使用了数据库访问技术的应用程序更加容易构建。 Spring Data JPA 内置了简单数据库读写操作，包括分页查询，并提供接口以待增强。\nSpring Data JPA 是基于Hibernate（在3.2版本中便对JPA提供了完全的支持）、JPA规范的基础上封装的一套ORM框架，可以说就是JPA规范的一个实践落地的产品。Spring Data JPA的内置实现中提供了包括增删改查、分页、自定义SQL的常用功能，且提供接口以待拓展增强。基于Spring Data JPA可以简洁的代码，快速的实现对数据库的访问。\nMybatis 1、Mybatis是什么 Mybatis：Mybatis是一款优秀的持久层框架，他支持自定义SQL、存储过程以及高级映射。Mybatis免除了几乎所有的JDBC代码以及设置参数和获取结果集的工作。Mybatis可以通过简单的XML或注解来配置和映射原始类型、接口和Java POJO（Plain Old Java Objects，普通老式Java对象）为数据库中的记录。\nMybatis的关注点在于：\nSQL与POJO的映射，或者说查询结果集与POJO的映射 数据的高效存储于读写 可定制化SQL，支持原生SQL 虽然一直都在使用Mybatis作为项目中的orm框架，但是始终没有深入的去了解。Mybatis就是一个ORM框架，是一个半自动的ORM框架，是一个面向查询结果集的ORM框架，一个关注实体与查询结果集之间的映射关系的ORM框架，且支持自定义SQL、存储过程、高级映射。在此基础上存在二次开发的开源产品：Mybatis-Plus，其在Mybatis的基础上进行了增强。\n对比分析 1、功能性对比 序号 功能项 Spring Data JPA Mybatis 1 启动配置 数据库连接+JPA相关配置 数据库连接+Mybatis相关配置 2 映射关系 注解方式实现POJO与数据表之间的映射 XML配置结合自动映射（或驼峰映射），实现POJO与SQL之间的映射 3 类型转换 Convert，多用于如枚举值的存取 TypeHandle，多用于枚举类型数据存储 4 读写接口 内置提供CRUD、分页、排序、 Example等基础接口，可自我拓展 默认不提供实现，由使用者实现，但是Mybatis-Plus提供通用mapper、分页 5 查询语言 提供约定的查询语言JPQL，同时支持原生SQL 支持原生SQL，支持动态SQL 6 其他 1、支持对象模型正向创建数据库模型；2、提供缓存机制；3、提供乐观锁指定 1、提供Interceptor，可以拦截SQL进行一定的处理；2、Mybatis-Plus提供代码生成器、支持数据库模型逆向生成对象模型 2、理论分析 Spring Data JPA与Mybatis的设计原理不一样，前置根据领域设计进行建模，提倡面向对象思想，但是为了提供兼容性，提供自定义查询方法，可接入其他ORM框架；后者根据数据设计进行建模，多数实现为贫血模型。 Spring Data JPA：先有对象关系，后有数据表关系；Mybatis：先有数据表关系，后有对象关系。 Spring Data JPA（面向对象设计）：强调对象，以及对象的特性（如：封装），考虑的是对象的整个生命周期（包括对象的创建、持久化、状态的变更和行为等），对象的持久化只是对象的一种状态，所有操作皆由对象来完成，是对象的行为引发状态的改变，从而引起持久化状态数据的变化；Mybatis（面向关系设计）：强调数据的高效存取，开放数据表的操作，并不受实体的限制，可以随意进行表的关联，持久化的数据类似于某个事物的快照，事物状态的变化后引发对于快照的修改，有点直接面向数据库开发的感觉。 Spring Data JPA：SQL为动态创建，创建后存在SQL缓存；Mybatis：SQL为静态文件。 面向对象设计试图为动态的世界建模，它要描述的是世界的过程和规律，进而适应发展和变化，面向对象总是在变化中处理各种各样的变化；关系型模型是为静态世界建模，它通过数据快照记录了世界在某一时刻的状态，它是静止的。 综合分析主要内容来自Spring Data JPA与Mybatis简单对比\n3、性能分析 待测试\n4、综合分析 待更新\n参考链接：\nSpring Data JPA与Mybatis简单对比 技术专题讨论第二期总结：如何对 JPA 或者 MyBatis 进行技术选型 ","permalink":"https://fuyi-atlas.github.io/posts/program/spring-data-jpa/spring-data-jpa-vs-mybatis/","summary":"前言 其实，手写jdbc也是蛮好玩的\u0026hellip;\n在企业开发过程中，除去一些特殊的要求外，基本上都会使用全自动或半自动的ORM框架代替原生JDBC进行数据库的访问。而在具体项目设计时，常常会根据项目业务情况进行技术选型。其中常用的ORM框架有：\nMybatis Hibernate Spring Data JPA JdbcTemplate 在这里我们主要讨论Mybatis与Spring Data JPA。\nSpring Data JPA 1、JPA是什么 维基百科：JPA（Java Persistence API，Java持久化API）是一个Java应用程序接口规范，描述了使用Java标准平台（Java SE）和Java企业版平台（Java EE）的应用中的关系数据的管理。\nSpring Data JPA 全面解析：JPA (Java Persistence API) 是 Sun 官方提出的 Java 持久化规范。它为 Java 开发人员提供了一种对象/关联映射工具来管理 Java 应用中的关系数据。他的出现主要是为了简化现有的持久化开发工作和整合 ORM 技术，结束现在 Hibernate，TopLink，JDO 等 ORM 框架各自为营的局面。值得注意的是，JPA 是在充分吸收了现有 Hibernate，TopLink，JDO 等ORM框架的基础上发展而来的，具有易于使用，伸缩性强等优点。从目前的开发社区的反应上看，JPA 受到了极大的支持和赞扬，其中就包括了 Spring 与 EJB3.0 的开发团队\nJPA所维护的核心是实体（Entity Bean），而它是通过一个持久化上下文（Persistence Context）来使用的。持久化上下文包含了以前3个部分：\n对象关系映射（Object Relational Mapping，简称ORM）描述，JPA支持注解或XML两种形式的描述。 实体操作API，内置通用CRUD操作，来完成对象的持久化与查询。 查询语言，约定了面向对象的查询语句JPQL。 简单来说，JPA就是一种接口规范，一种定义了对象关系映射（ORM）以及实体对象的持久化接口标准的规范，并不是一套直接可用的产品（如：Hibernate）。\n2、什么是Spring Data JPA spring-data-jpa：Spring Data JPA是Spring Data系列的一个组成部分，可以轻松快捷的实现数据访问层的增强支持，这使得基于Spring且使用了数据库访问技术的应用程序更加容易构建。 Spring Data JPA 内置了简单数据库读写操作，包括分页查询，并提供接口以待增强。","title":"Spring Data JPA 与 Mybatis 对比分析"},{"content":"伊始，见自己 写在前面 今天是2020年7月13日，从毕业现在，已经整整两年。一直生活在焦虑之中，浮躁之中，什么都想要，却不知道自己并没有获取的能力。深知静心是关键，但是始终无法做到，像只写代码的松鼠一样，屯了很多的技术书籍，技术课程，却也只是放着。或许每个人都会存在这样的一个阶段，或许只是我自己内心过于脆弱，这些于我已经过去了，只要你知道你想要的是什么。\n一知半解 一知半解是一件很危险的事情，回顾工作的两年时间，虽然已经成长了很多，但我仍然希望我的两年工作经验就是不折不扣的两年工作经验。现实情况下，工作内容多是面向搜索引擎，如果有一天没了网，我可能什么也不会做。很多东西都还是在知其然并不知其所以然的状态，对于计算机、网络、编程语言基础的掌握更是一塌糊涂。这并不是妄自菲薄，只是对自己的深入剖析。\n求法 我认为，不管你做什么事情，都需要先认识自己，明确自己的定位，才能更好的开始。我是一个普通人，一个很普通很普通的人，我没有想着我能做出多大的动静出来，我只想做好我想做的事情，不再是虎头蛇尾。\n坚持对于我来说还是有点困难的，我的第一篇博客是在2018年4月1日（这时间不是来搞笑的啊），最近的一篇博客发表却是在2019年7月4日（这是一篇水文），中间整整一年没有进行任何的动作，其实想写的东西很多，拖着拖着就成这样了，包括服务器和域名都是去年年末就已经弄好了。万恶的拖延症，万恶的懒惰，万恶的自己。\n写到这里，突然有点鄙视自己，这么菜了还这么懒。但我相信我还是可以改变了，就从现在开始，我将通过博客记录自己的生活、学习、工作以及其他杂谈内容（这些乱七八糟的东西只是想告诉我自己我都做了些什么，经历了什么），也希望通过这样的方式可以更好的提高自己。\n学习是一件逆人性的事情，长时间的学习更是，但是只有保持长时间的学习才能不断的进行自己升值。如何更有效的学习是一件值得不断优化的事情，光凭借记忆肯定是不行的，只有深入理解，并有着自己的见解，还能够友好的阐述才算是良好。纸上学来终觉浅，绝知此事要躬行，不管困难还是简单，一定动手尝试并理解。\n《易经》有云：取法其上，得乎法中；取法其中，得乎法下；取法其下，法不得也。目标很重要，方法很重要，坚持很重要，反馈很重要！\n","permalink":"https://fuyi-atlas.github.io/posts/other/perception-001/","summary":"伊始，见自己 写在前面 今天是2020年7月13日，从毕业现在，已经整整两年。一直生活在焦虑之中，浮躁之中，什么都想要，却不知道自己并没有获取的能力。深知静心是关键，但是始终无法做到，像只写代码的松鼠一样，屯了很多的技术书籍，技术课程，却也只是放着。或许每个人都会存在这样的一个阶段，或许只是我自己内心过于脆弱，这些于我已经过去了，只要你知道你想要的是什么。\n一知半解 一知半解是一件很危险的事情，回顾工作的两年时间，虽然已经成长了很多，但我仍然希望我的两年工作经验就是不折不扣的两年工作经验。现实情况下，工作内容多是面向搜索引擎，如果有一天没了网，我可能什么也不会做。很多东西都还是在知其然并不知其所以然的状态，对于计算机、网络、编程语言基础的掌握更是一塌糊涂。这并不是妄自菲薄，只是对自己的深入剖析。\n求法 我认为，不管你做什么事情，都需要先认识自己，明确自己的定位，才能更好的开始。我是一个普通人，一个很普通很普通的人，我没有想着我能做出多大的动静出来，我只想做好我想做的事情，不再是虎头蛇尾。\n坚持对于我来说还是有点困难的，我的第一篇博客是在2018年4月1日（这时间不是来搞笑的啊），最近的一篇博客发表却是在2019年7月4日（这是一篇水文），中间整整一年没有进行任何的动作，其实想写的东西很多，拖着拖着就成这样了，包括服务器和域名都是去年年末就已经弄好了。万恶的拖延症，万恶的懒惰，万恶的自己。\n写到这里，突然有点鄙视自己，这么菜了还这么懒。但我相信我还是可以改变了，就从现在开始，我将通过博客记录自己的生活、学习、工作以及其他杂谈内容（这些乱七八糟的东西只是想告诉我自己我都做了些什么，经历了什么），也希望通过这样的方式可以更好的提高自己。\n学习是一件逆人性的事情，长时间的学习更是，但是只有保持长时间的学习才能不断的进行自己升值。如何更有效的学习是一件值得不断优化的事情，光凭借记忆肯定是不行的，只有深入理解，并有着自己的见解，还能够友好的阐述才算是良好。纸上学来终觉浅，绝知此事要躬行，不管困难还是简单，一定动手尝试并理解。\n《易经》有云：取法其上，得乎法中；取法其中，得乎法下；取法其下，法不得也。目标很重要，方法很重要，坚持很重要，反馈很重要！","title":"伊始，见自己"}]